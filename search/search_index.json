{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"cs/curriculum/networking/ipv6/","title":"IPv6","text":""},{"location":"cs/curriculum/networking/ipv6/#why-ipv6","title":"Why IPv6?","text":"<p>The Internet Engineering Task Force (IETF) created IPv4 (RFC 791) with an address space of 4.3 billion, initially believed to be sufficient for all devices connecting to the Internet. However, as early as the late 1980s, when the Internet began to flourish, experts foresaw the eventual exhaustion of IPv4 addresses.</p>"},{"location":"cs/curriculum/networking/ipv6/#what-is-ipv6","title":"What is IPv6?","text":"<p>IPv6 is newer and better version of Internet Protocol bringing about 3.4*10^38 total addresses and many provides other technical benefits such as hierarchical address allocation methods &amp; multicast addressing is expanded and simplified. Do note that IPv4 and IPv6 are not interoperable.</p>"},{"location":"cs/curriculum/networking/ipv6/#the-ipv6-address","title":"The IPv6 Address","text":"<p>Remember IPv4 address looks like this:</p> <p>The bits refer to the binary digits of the address.</p> <p>$$ \\underbrace{192.168.32.152}_{\\text{32-bit in length}} $$</p> <p>is made up four sections called octets, each octet can contain any number between 0-255:</p>  $$ \\underbrace{192}_{\\text{octet}}.\\underbrace{168}_{\\text{octet}}.\\underbrace{32}_{\\text{octet}}.\\underbrace{152}_{\\text{octet}} $$  <p>Let's look at IPv6 address:</p> <p>$$ \\text{2001:0db8:0000:0000:a111:b222:c333:abcd} $$</p> <p>Things to note with IPv6:</p> <ol> <li>In comparison to IPv4 which is 32 bits long, IPv6 address is 128 bits long.</li> <li>IPv4 has 4 sections calles octets, but IPv6 has 8 sections called hextets.</li> <li>IPv4 uses <code>.</code> to separate octets. IPv6 uses <code>:</code> to separate hextets.</li> </ol>"},{"location":"cs/curriculum/networking/ipv6/#ipv6-in-binary","title":"IPv6 in Binary","text":"<p>IPv4 is usually represented in decimal format, but IPv6 is hexadecimal that is it is generally represented using any character between <code>0-9</code> and <code>a-f</code>.</p> <p>Each hexadecimal character is made up of four binary bits which can be 0 or 1. Each bit represents a value of 1, 2, 4 or 8. It doubles with each column from right to left.</p>  $$ \\def\\arraystretch{1.5}    \\begin{array}{c:c:c:c}    8 &amp; 4 &amp; 2 &amp; 1 \\\\ \\hline    0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} $$  <p>Wherever we see a <code>1</code>, that toggles that column on, and wherever we see a <code>0</code> that toggles that column off.</p> <p>We then add up all of the columns that have a 1 and this gives us our number or letter.</p> <ul> <li>To make the number <code>2</code> from our binary bits, we switch on the column for 2. Giving us the binary value for our first number: <code>0010</code>.</li> <li>To make the second number <code>0</code> from our binary bits, we keep all bits off.</li> </ul>  $$ \\underbrace{   \\textcolor{#07fc03}{2} }_{\\text{0010}} \\text{001:0db8:0000:0000:a111:b222:c333:abcd}  \\newline  \\def\\arraystretch{1.5} \\begin{array}{c:c:c:c} 8 &amp; 4 &amp; 2 &amp; 1 \\\\ \\hline 0 &amp; 0 &amp; \\textcolor{#f56c42}{1} &amp; 0 \\\\ \\end{array} $$   $$ \\text{2} \\underbrace{   \\textcolor{#07fc03}{0} }_{\\text{0000}} \\text{01:0db8:0000:0000:a111:b222:c333:abcd}  \\newline  \\def\\arraystretch{1.5} \\begin{array}{c:c:c:c} 8 &amp; 4 &amp; 2 &amp; 1 \\\\ \\hline 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} $$  <p>For <code>d</code> (or any alphabet), it gets interesting. The hexadecimal digits ends at <code>f</code> like this:</p>  $$  \\def\\arraystretch{1.5} \\begin{array}{} 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 &amp; a &amp; b &amp; c &amp; d &amp; e &amp; f \\\\ 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 &amp; 10 &amp; 11 &amp; 12 &amp; 13 &amp; 14 &amp; 15 \\\\ \\end{array}  $$  <p>We should always start with the highest number you can in our original table in order to achieve our desired digit and in this case that number is <code>8</code>.</p> <p>So, to achieve <code>13</code> or <code>d</code>, we'll try representing <code>13</code> in binary like this:</p>  $$  \\text{2001:0000:0} \\underbrace{ \\textcolor{#07fc03}{d} }_{\\text{1101}} \\text{b8:0000:0000:a111:b222:c333:abcd}  \\newline  \\def\\arraystretch{1.5} \\begin{array}{c:c:c:c} 8 &amp; 4 &amp; 2 &amp; 1 \\\\ \\hline 1 &amp; 1 &amp; 0 &amp; 1 \\\\ \\end{array}  $$  <p>Continuing this we'll achieve this binary digits:</p> <p>$$ 10000000000001:110110111000:000000000000:000000000000:1010000100010001:1011001000100010:1100001100110011:1010101111001101 $$</p>"},{"location":"cs/curriculum/networking/ipv6/#network-section","title":"Network Section","text":"<p>Just like IPv4, IPv6 has network section.</p>  $$ \\underbrace{\\textcolor{#ecf549}{2001}:\\textcolor{#ecf549}{0db8}:\\textcolor{#ecf549}{0000}:\\textcolor{#ecf549}{0000}}_\\text{Network section}: \\underbrace{a111:b222:c333:abcd}_\\text{Host section}\\textcolor{#ecf549}{/64} $$  <p>IPv4 uses subnet mask to define this network section, however IPv6 gets rid of subnet mask and  instead it just uses a <code>/</code> and number of network bits.</p> <p>In above example $\\textcolor{#ecf549}{64}$ is half of our 128 bit address. <code>/64</code> addresses are common when it comes to IPv6.</p>"},{"location":"cs/curriculum/networking/ipv6/#compressing-the-address","title":"Compressing the Address","text":"<ol> <li> <p>The first trick is to remove continuous zeroes and replace it with double colon, computer can then figure out there are two missing hextets and then it fill it up with zeroes.</p> <ul> <li>The rule to this is it can only appear once.</li> </ul> <p>$$ \\text{2001:0db8:\\underline{0000:0000}:a111:b222:c333:abcd} \\newline \\text{2001:0db8\\textcolor{#ecf549}{::}a111:b222:c333:abcd} $$</p> <p>So, this we cannot do!</p> <p>$$ \\text{2001:0db8:\\underline{0000:0000}:a111:b222:0000:abcd} \\newline \\text{2001:0db8\\textcolor{#ecf549}{::}a111:b222\\textcolor{#ecf549}{::}abcd} $$</p> </li> <li> <p>Second trick, we can all leading zeroes from each hextets and remove them:</p> <p>$$ \\text{2001:\\textcolor{#ecf549}{0}db8\\textcolor{#ecf549}{::}a111:b222:\\textcolor{#ecf549}{000}0:abcd} \\newline \\text{2001:db8\\textcolor{#ecf549}{::}a111:b222:0:abcd} $$</p> <ul> <li>We need to leave last <code>0</code> is second last hextet since we already placed <code>::</code>.</li> </ul> </li> </ol>"},{"location":"cs/curriculum/networking/ipv6/#global-unicast-address","title":"Global Unicast address","text":"<p>Just like IPv4 there are different type of IPv6 addresses and they serve different purposes.</p> <p>This one is Global unicast address, unlike IPv4, we don't need to rely on private addresses. Every device can have it's own public IP addresses.</p> <p>$$ \\underbrace{   \\textcolor{#ecf549}{2001:0db8:0000} }_{\\text{Global Prefix}} \\text{:0000:a111:b222:c333:abcd} $$</p> <p>The first part of IPv6 Global prefix is called  atleast 48 bits long Global Prefix which is provided by your ISP (Internet service provider).</p> <p>After Global prefix, we use the next 16-bits for subnet IDs.</p> <ul> <li>16 bits give us over 65,000 subnets to play with</li> </ul>  $$ \\underbrace{   \\textcolor{#ecf549}{2001:0db8:0000} }_{\\text{Global Prefix}}: \\overbrace{   \\textcolor{#07fc03}{0000} }^{\\text{Subnet ID}} \\text{:a111:b222:c333:abcd} $$  <p>The remaining 48-bits can then be used for host or interface ID</p>  $$ \\underbrace{   \\textcolor{#ecf549}{2001:0db8:0000} }_{\\text{Global Prefix}}: \\overbrace{   \\textcolor{#07fc03}{0000} }^{\\text{Subnet ID}}: \\underbrace{   \\text{a111:b222:c333:abcd} }_{\\text{Interface ID}}: $$"},{"location":"cs/curriculum/networking/ipv6/#other-address-type","title":"Other address type","text":"Address type Global unicast <code>2000::/3</code> Publicly routable Unique local <code>fc00::/7</code> Routable in the LAN Link local <code>fe80::/10</code> Not routable Multicast <code>ff00::/8</code> Addresses for groups Anycast <code>2000::/3</code> Shared address <ol> <li> <p>Global Unicast (<code>2000::/3</code>): The prefix to identify these address is like <code>2000::/3</code> which means, the <code>/3</code> first three bits identify a global unicast address are fixed. It will start with either a 2 or a 3</p> <ul> <li>i.e. the address will lie in range <code>2000::/3</code> to <code>3fff::/3</code>. And in binary notation, the first 3 bits will remain contant</li> <li>addresses in <code>2000::</code> to <code>3fff:ffff:fff1f:ffff:ffff:ffff:ffff:ffff</code></li> <li> <p>So in binary addresses starting with</p> <p>$$ \\textcolor{#07fc03}{0010}{000000000000}:0000000000000000:0000000000000000:0000000000000000:0000000000000000:0000000000000000:0000000000000000:0000000000000000 $$</p> <p>to address:</p> <p>$$ \\textcolor{#07fc03}{0011}{111111111111}:1111111111111111:1111111111111111:1111111111111111:1111111111111111:1111111111111111:1111111111111111:1111111111111111 $$</p> </li> <li> <p>Globally unique and routable, similar to public IPv4 address.</p> </li> </ul> </li> <li> <p>Unique local (<code>fc00::/7</code>): These are like IPv4 private address, not gloabally routable but routable only within the scope of private networks.</p> <ul> <li>Unique local address will always start with <code>f</code> followed by either <code>c</code> or a <code>d</code></li> <li>Unique local addresses may be used freely, without centralized registration, inside a signle site or organization.</li> </ul> </li> <li> <p>Link local (<code>fe80::/10</code>): Link-local addresses allow communication between neighboring nodes without needing a globally unique address. IPv6 routers do not forward data with link-local addresses beyond the local network. All IPv6-enabled interfaces automatically have a link-local unicast address.</p> </li> <li> <p>Think of this as similar to <code>169.254.xxx.xxx</code> of IPv4.</p> </li> <li> <p>Are crucial for tasks like automatic address configuration and for the Neighbor Discovery Protocl (NDP), which help devices on the same link find and communicate with each other.</p> </li> <li> <p>Mulicast (<code>ff00::/8</code>): These addresses are sent to a group of nodes listening for that particular multicast address.</p> </li> <li> <p>IPv6 abolishes IPv4 broadcast communication completely, replacing it with IPv6 multicast, implemented using Protocol Independent Multicast (PIM) routing.</p> </li> <li> <p>Anycast (<code>2000::/3</code>): IPv6 allows us to assign the same IP address to multiple devices. The data is then sent to the closest device with that address. There isn't a specific range for anycast address, they use the same address space as global unicast address.</p> </li> </ol>"},{"location":"cs/curriculum/sql/common-queries/","title":"Common SQL queries","text":"<pre><code>CREATE ROLE &lt;user_name&gt; WITH CREATEDB LOGIN ENCRYPTED PASSWORD '&lt;your_pasword&gt;';\nCREATE DATABASE &lt;db_name&gt; WITH OWNER '&lt;user_name&gt;' ENCODING 'utf8';\n\npsql -h localhost -p &lt;port_number&gt; -U &lt;user_name&gt; &lt;db_name&gt;\n\nSHOW search_path; \nSET search_path= translate;\n</code></pre> <pre><code>-- Create Table\nCREATE TABLE my_table (\n    col_name1 datatype,\n    col_name2 datatype\n);\n</code></pre> <pre><code>-- Create Table with integrity constraints \n-- applied to individual columns\nCREATE TABLE my_table (\n    col_name1 datatype NOT NULL,\n    col_name2 datatype UNIQUE,\n    col_name3 datatype NOT NULL PRIMARY,\n    col_name4 datatype DEFAULT=' ',\n    col_name5 datatype CHECK(condition)\n); \n</code></pre> <pre><code>(colname datatype CHECK(colname IN(' ', ' '));\n(colname datatype CHECK(colname BETWEEN 25.00 AND 770.00));\n(colname datatype CHECK(colname LIKE '        '));\n</code></pre>"},{"location":"cs/curriculum/sql/common-queries/#performing-select-queries","title":"Performing Select queries","text":"<pre><code>-- selecting specific columns\nSELECT colname1, colname2\nFROM my_table\n\n-- selecting every column in table\nSELECT * FROM my_table;\n\n-- avoid duplicate rows\nSELECT DISTINCT colname \nFROM my_table;\n</code></pre>"},{"location":"cs/curriculum/sql/common-queries/#select-queries-with-where-clause","title":"<code>SELECT</code> queries with <code>WHERE</code> clause","text":"<pre><code>-- select rows based on condition\nSELECT colname, colname1, colname2\nFROM my_table\nWHERE &lt;condition&gt;;\n\n-- using relational operator\n-- &lt;&gt; not equal to \n-- =, &gt;, &lt;, &gt;=, &lt;=, &lt;&gt;\nSELECT * FROM my_table\nWHERE city &lt;&gt; 'Delhi'\n</code></pre> <p><code>&gt;</code> later in alphabet and <code>&lt;</code> earlier in alphabet</p> <pre><code>-- either one\nWHERE (grade='E2' OR grade='E3'); \n\n-- both condition should meet\nWHERE (grade-'E4' AND gross&lt;9000);\n\nWHERE (NOT grade='G1');\n</code></pre> <p>Operator precedence: <code>NOT</code> &gt; <code>AND</code> &gt; <code>OR</code></p> <pre><code>WHERE colname BETWEEN 30 AND 50;\nWHERE colname NOT BETWEEN 30 AND 50;\n</code></pre> <pre><code>-- Display list of member from ' ' or ' '\nSELECT * FROM my_table\nWHERE colname IN('  ', '  ');\n\n-- Display list of members not from ' ' or ' '\nSELECT * FROM my_table\nWHERE colname NOT IN (' ', ' ');\n</code></pre> <pre><code>SELECT * FROM my_table\nWHERE colaname LIKE \"13%\";\n\n-- 13% - any string starting with '13'\n-- %13 - any string ending with '13'\n-- %13% - any string with '13'\nWHERE colname NOT LIKE \"13%\"\n\n-- String with any 3 characters ending with\nWHERE colname LIKE \"___0\"; \n-- LIKE \"wx\\%yz%\" ESCAPE \"\\\"  matches all string starting \"wx%yz\"\n\n-- searching for rows with or without null values\nWHERE colname IS NULL;\nWHERE colname IS NOT NULL;\n</code></pre> <pre><code>-- Order by\nORDER BY colnam;\nORDER BY colname DESC;\nORDER BY colname ASC;\n</code></pre> <pre><code>-- Group By\nGROUP BY colname;\n\n-- Group By with having clause appling condition per group\nGROUP BY colname\nHAVING condition;\n</code></pre> <p>Scalar expressions</p> <pre><code>SELECT colname, gross*100, '%'\nFROM my_table;\n</code></pre>"},{"location":"cs/curriculum/sql/common-queries/#creating-table-from-existing-table","title":"Creating table from existing table","text":"<pre><code>CREATE TABLE my_table2 AS (\n    SELECT col1, col2\n    FROM my_table\n);\n</code></pre>"},{"location":"cs/curriculum/sql/common-queries/#inserting-into-table","title":"Inserting into table","text":"<pre><code>-- insert into all rows\nINSERT INTO my_table\nVALUES (, ' ',  ,' ');\n\n-- inset into specific rows\n-- columns not listed will have default value \n-- if defiend otherwise NULL value\nINSERT INTO my_table(ecode, ename, sex)\nVALUES (2018, 'MAX', 'M');\n</code></pre> <pre><code>-- insert result from a where query\nINSET INTO my_table2\nSELECT * FROM  my_table\nWHERE gross &gt; 700.00\n</code></pre>"},{"location":"cs/curriculum/sql/common-queries/#delete-from-table","title":"Delete from table","text":"<pre><code>DELETE FROM my_table\nWHERE &lt;condition&gt;;\n</code></pre>"},{"location":"cs/curriculum/sql/common-queries/#update-table","title":"Update table","text":"<pre><code>UPDATE my_table\nSET colname = 250\nWHERE colname = 14;\n</code></pre>"},{"location":"cs/curriculum/sql/common-queries/#create-views","title":"Create views","text":"<pre><code>CREATE VIEW vir_tab AS\nSELECT * FROM my_table\nWHERE gross &gt; 8000;\n</code></pre> <pre><code>CREATE VIEW vir_tab(col, col1, col2) AS\nSELECT * FROM my_table\nWHERE gross &gt; 8000;\n</code></pre> <pre><code>-- create views with sacalar expressions\nCREATE VIEW vir_tab(col1, col2, col3) AS\nSELECT col1, col2, col3 * 0.1\nFROM my_table\nWHERE gross &gt; 8000;\n</code></pre>"},{"location":"cs/curriculum/sql/common-queries/#joins","title":"Joins","text":""},{"location":"cs/curriculum/sql/common-queries/#inner-join","title":"Inner join","text":"<pre><code>SELECT\n       emp.first_name,\n       dep.department_id,\n       loc.city,\n       loc.state_province\nfrom\n     employees emp\nINNER JOIN\n    departments dep on dep.department_id = emp.department_id\nINNER JOIN\n    locations loc on dep.location_id = loc.location_id\nWHERE\n    lower(emp.first_name) like '%s%'\n</code></pre>"},{"location":"cs/curriculum/sql/common-queries/#right-join","title":"Right join","text":"<pre><code>SELECT\n    emp.first_name,\n    dep.department_name\nFROM\n    employees emp\nRIGHT JOIN departments dep on dep.department_id = emp.department_id\n</code></pre>"},{"location":"cs/curriculum/sql/common-queries/#useful-commands-for-psql","title":"Useful commands for psql","text":"<ul> <li><code>\\dn schema</code> : Show list of schema</li> <li><code>\\q</code> : Quit psql</li> <li><code>\\i &lt;drive_letter&gt;_:///&lt;sql_file&gt;.sql</code> : Run sql from file</li> <li><code>\\du</code>: show list of roles</li> <li><code>\\d &lt;Table_name&gt;</code>: Show schema</li> <li><code>\\l</code> : List of databases</li> </ul>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/","title":"Terraform","text":"<p>Notes from freeCodeCamp course<sup>1</sup>.</p>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#what-is-terraform","title":"What is Terraform?","text":"<p>Terraform is an infrastructure as code tool that lets you build, change and version cloud and on-premise resources.</p> <p>It's built by HashiCorp.</p> <p>Info</p> <p>The Terraform file will have different structure depending on the version, Terraform <code>0.12</code> and earlier, &amp; <code>0.13</code> and later.</p> <p>The following code will use <code>0.13</code> and later syntax and was tested with Terraform <code>v1.7.0</code>. The code demonstrated in the video and this wiki might not be same due to version updates!</p>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#installation","title":"Installation","text":"<ul> <li>You can follow the instructions mentioned on their developers page: Install Terraform.</li> <li> <p>Installation on Windows:</p> <ul> <li>Download the <code>.exe</code> file from <code>terraform.io/downloads.html</code> with appropriate architecture. Paste this file in <code>C:\\Terraform\\terraform.exe</code></li> <li>Update the Path in Environment Variables, by adding a New entry to the path: <code>C:\\Terraform</code>.</li> <li>Check the Terraform version from cmd: <code>terraform -v</code>. </li> </ul> </li> <li> <p>Installation on Mac:</p> <ul> <li>Using Homebrew by running <code>brew install terraform</code>.</li> <li>Check the version using: <code>terraform -v</code></li> </ul> </li> <li> <p>Installation on Linux:</p> <p>Run following commands (Fedora): <pre><code>sudo dnf install -y dnf-plugins-core\nsudo dnf config-manager --add-repo https://rpm.releases.hashicorp.com/fedora/hashicorp.repo\nsudo dnf -y install terraform\n</code></pre></p> <p>Check the version using: <code>terraform -v</code></p> </li> </ul>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#install-terraform-vscode-extension","title":"Install Terraform VSCode Extension","text":"<p>Go to VSCode extensions tab and install Terraform extension for all autocomplete features and syntax highlighting. Make sure it's from HashiCorp.</p>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#terraform-overview","title":"Terraform overview","text":"<p>Let's create a new project and open in VSCode. All Terraform code is going to be stored in <code>.tf</code> extension file.</p>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#setup-provider","title":"Setup provider","text":"<p>Create a new file, naming it whatever you want, but make sure it's extension is <code>.tf</code>.</p> <ul> <li> <p>Terraform provides many providers to interact with various cloud service providers, SaaS providers, and other APIs.</p> </li> <li> <p>On a per-user basis, per-project basis, Terraform will figure out which plugins need to be installed based off of the provider configuration in your Terraform file.</p> </li> </ul> <p>We'll try creating an AWS infrastructure in Terraform file. For that we can check the AWS provider in Terraform registry.</p> <pre><code>terraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n    }\n  }\n}\n\n# Configure the AWS Provider &amp; region\nprovider \"aws\" {\n  region = \"us-east-1\"\n}\n</code></pre>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#setup-authentication","title":"Setup Authentication","text":"<p>We'll try setting up bare minimum authentication, by hard coding credentials into the file.</p> <p>Warning</p> <p>Hard-coding credentials into any file that is meant to versioned by Version Control System and is going to be available for everybody to read is a bad practice. </p> <p>This might leak the credentials.</p> <p>We can set up credentials by adding it to <code>aws</code> provider block:</p> <pre><code># Configure the AWS Provider\nprovider \"aws\" {\n  region = \"us-east-1\"\n  access_key = \"my-access-key\"\n  secret_key = \"my-secret-key\"\n}\n</code></pre> <p>You can access them by clicking the profile name on top-right corner, and visiting Security Credentials and Create access key.</p> <p></p>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#provision-a-resource-within-aws","title":"Provision a resource within AWS","text":"<p>Regardless of what provider you're using, whether it's to create a resource on GCP/AWS/Azure (etc.), It's going to use the same exact syntax from a Terraform side.</p> <p>The basic syntax is like this: <pre><code># resource \"&lt;provider&gt;_&lt;resource_type&gt;\" \"name\" {  (1)\n#   config options....  \n#   key = \"value\"\n#   key2 = \"value2\"\n# }\n</code></pre></p> <ol> <li>Comments in Terraform files beings with a <code>#</code></li> </ol> <p>We'll walk through on how to deploy an EC2 (Elastic Compute Cloud) instance withing AWS, which is basically a compute instance/virtual machine withing AWS.</p> <p>Create EC2 instance via AWS console</p> <p>The video go through creating AWS Instance via AWS console.</p> <p>We'll use aws_instance (aws is provider, instance is resource type) as the resource to configure:</p> <pre><code>resource \"aws_instance\" \"my-test-server\" {\n  ami           = \"ami-0c7217cdde317cfec\" # (1)!\n  instance_type = \"t2.micro\"\n}\n</code></pre> <ol> <li>The AMI (Amazon Machine Image) might change in the future, so do check the AMI in the console or in Docs. Stick to free-tier eligible for the duration of this tutorial.</li> </ol> <p>Open a terminal tab in your editor and run <code>terraform init</code>.</p> <ul> <li>This will make Terraform to look at our config (<code>.tf</code> file(s)) and is going to look for all providers that we have defined.</li> <li>Right now, we have only specified one provider <code>aws</code>, so it's going to download the necessary plugins to interact with the AWS API. </li> </ul> <pre><code>terraform init\n</code></pre> <p>You should see output something like this:</p> <p>Terraform has been successfully initialized!</p> <p>The next command <code>terraform plan</code> command does sort for dry run of your code. It'll kind of show you, it's going to delete any instances, if it's going to create new instances, if it's going to modify instances. This make sure you don't accidantly break your production environment or anything else.</p> <pre><code>terraform plan\n</code></pre> <ul> <li>The output is color coded so, green means new resource, red means deletion and orange implies modification to existing resource.</li> </ul> <p>Lastly, we run </p> <pre><code>terraform apply\n</code></pre> <p>Which will actually run our code, after you verify it, it will start provisioning the resources.</p>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#modify-resources","title":"Modify resources","text":"<p>Running the <code>terraform apply</code> again won't create another instance, the reason being Terraform is written in declarative manner, which means we're not actually giving Terraform a bunch of steps to carry out. Instead, we're telling Terraform, what we want our infrastructure to look like.</p> <p>Terraform files are basically a blueprint of the infrastructure we want to have at the end.</p> <p>Try running <code>terraform plan</code> for a sanity check, you'll see output something like this:</p> <ul> <li><code>terraform plan</code> also talks to AWS and checks the state and gather information about it's state, if it's up or not.</li> </ul> <pre><code>aws_instance.my-test-server: Refreshing state... [id=i-0eae7fc4ff618a32a]\n\nNo changes. Your infrastructure matches the configuration.\n\nTerraform has compared your real infrastructure against your configuration and found no differences, so no changes are needed.\n</code></pre> <p>Running <code>terraform apply</code> will add this to the above output:</p> <pre><code>Apply complete! Resources: 0 added, 0 changed, 0 destroyed.\n</code></pre>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#adding-a-tag","title":"Adding a tag","text":"<pre><code>resource \"aws_instance\" \"my-test-server\" {\n  ami           = \"ami-0c7217cdde317cfec\" \n  instance_type = \"t2.micro\"\n\n  tags = {\n    Name = \"fedora\"\n  }\n}\n</code></pre> <p>And run <code>terraform plan</code> which should show you modification with tilde (~) marked with orange color about the tags.</p> <pre><code>aws_instance.my-test-server: Refreshing state... [id=i-0eae7fc4ff618a32a]\n\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n  ~ update in-place\n\nTerraform will perform the following actions:\n\n  # aws_instance.my-test-server will be updated in-place\n  ~ resource \"aws_instance\" \"my-test-server\" {\n        id                                   = \"i-0eae7fc4ff618a32a\"\n      ~ tags                                 = {\n          + \"Name\" = \"fedora\"\n        }\n      ~ tags_all                             = {\n          + \"Name\" = \"fedora\"\n        }\n        # (30 unchanged attributes hidden)\n\n        # (8 unchanged blocks hidden)\n    }\n\nPlan: 0 to add, 1 to change, 0 to destroy.\n</code></pre> <p>And finally run <code>terraform apply</code> to apply the changes.</p>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#deleting-resources","title":"Deleting Resources","text":"<p>To delete the resources we can run:</p> <pre><code>terraform destroy\n</code></pre> <p>With <code>terraform destroy</code> it's going to destroy every single resource (whole infrastructure) that was created by Terraform. If you want to destroy a single resource there are other parameters that you need to pass in to make it work.</p> <p>Instead, you can remove (or comment) the specific resource you want to be destroyed.</p> <pre><code># resource \"aws_instance\" \"my-test-server\" {\n#   ami           = \"ami-0c7217cdde317cfec\" \n#   instance_type = \"t2.micro\"\n\n#   tags = {\n#     Name = \"fedora\"\n#   }\n# }\n</code></pre> <p>And hit <code>terraform apply</code> which will compare the state on AWS and state declared in your Terraform file and destroy the resource.</p>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#referencing-resources","title":"Referencing Resources","text":"<p>Delete the resource you declared previously in the Terraform file, <code>my-test-server</code> and run <code>terraform apply</code>.</p> <p>Let's explore how to create a VPC (Virtual Private Network) and subnet within that VPC. A VPC is a private, isolate network, within your AWS environment. Each one of the VPC by default is going to be isolated from one another. </p> <p>We'll use aws_vpc resource.</p> <pre><code>resource \"aws_vpc\" \"my-test-vpc\" {\n  cidr_block = \"10.0.0.0/16\"\n  tags = {\n    Name = \"production\"\n  }\n}\n</code></pre> <ul> <li>The above block implies, for our VPC, the <code>10.0.0.0/16</code> is going to be the network that's going to be the network that's going to be used for that VPC.</li> </ul> <p>We'll also want to create a subnet withing that VPC. For that we'll use aws_subnet resource.</p> <pre><code>resource \"aws_subnet\" \"subnet-1\" {\n  vpc_id     = aws_vpc.my-test-vpc.id\n  cidr_block = \"10.0.1.0/24\" # (1)!\n\n  tags = {\n    Name = \"prod-subnet\"\n  }\n}\n</code></pre> <ol> <li> <p>Make sure <code>cidr_block</code> of subnet falls within VPC's <code>cidr_block</code></p> </li> <li> <p>To reference the <code>vpc_id</code> we use the resource <code>id</code> property which is defined for every resource, of the VPC we just created <code>my-test-vpc</code>.</p> </li> </ol> <p>Hit <code>terraform apply</code> and deploy the changes.</p> <p>!!!note Terraform does not care about the order you define.</p> <pre><code>Due to declarative nature of Terraform VPC **doesn't need** to declared before the subnet. Terraform will take care of what needs to get created first.\n\nAlthough, there are certain instances where it can't. In those case you can look at documentation to figure out a workaround.\n</code></pre>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#terraform-files","title":"Terraform files","text":"<p>Let's talk about various files Terraform generates:</p> <pre><code>.\n\u251c\u2500\u2500 .terraform/\n\u251c\u2500\u2500 .terraform.loc.hcl\n\u251c\u2500\u2500 main.tf\n\u251c\u2500\u2500 terraform.tfstate\n\u2514\u2500\u2500 terraform.tfstate.backup\n</code></pre> <ul> <li>When we did <code>terraform init</code> to initialize any plugins, it creates the <code>.terraform/</code> directory and install all the required plugins in this directory. If you delete this dir, you can get it back just by running <code>terraform init</code>.</li> <li>The <code>terraform.tfstate</code> represents whole state of Terraform. Anytime we create a resource withing any of the cloud provider, we need a way for Terraform to keep a track of what is created. This file is very important, if this gets deleted, you'll break Terraform, causing a mismatched state between what's deployed and what's declared..</li> </ul>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#practice-project","title":"Practice Project","text":"<p>In this section we'll create a new EC2 instance, assign it a public IP address, so that we can SSH into it a</p> <ol> <li>Create a VPC</li> <li> <p>Create Internet Gateway</p> <p>So that we can send traffic out to the internet, 'cause we want to be able to assign a public IP address to this server so that anybody in the world can reach to it.</p> </li> <li> <p>Create custom route table</p> </li> <li>Create a subnet</li> <li>Associate subnet with Route table</li> <li>Create security group to allow port <code>22</code>, <code>80</code> &amp; <code>443</code>.</li> <li>Create a network interface with an IP in the subnet that was created in step 4.</li> <li>Assign an elastic IP to the network interface created in step 7.</li> <li>Create Fedora server and install/enable apache2</li> </ol> <p>The first thing we need to do is to create a key pair within AWS. A key pair, consisting of a public key and a private key, is a set of security credentials that you use to prove your identity when connecting to an EC2 instance.EC2 stores the public key on your instance, and you store private key. </p> <p>Checkout this documentation on how to create a key-pair.</p>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#step-1-create-a-vpc","title":"Step 1: Create a VPC","text":"<pre><code>resource \"aws_vpc\" \"prod-vpc\" {\n  cidr_block = \"10.0.0.0/16\"\n  tags = {\n    Name = \"production\"\n  }\n}\n</code></pre>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#step-2-create-a-gateway","title":"Step 2: Create a gateway","text":"<pre><code>resource \"aws_internet_gateway\" \"gw\" {\n  vpc_id = aws_vpc.prod-vpc.id\n}\n</code></pre>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#step-3-create-custom-route-table","title":"Step 3: Create custom route table","text":"<pre><code>resource \"aws_route_table\" \"prod-route-table\" {\n  vpc_id = aws_vpc.prod-vpc.id\n\n  route {\n    # cidr_block = \"10.0.1.0/24\"  (1)\n    cidr_block   = \"0.0.0.0/0\" # (2)!\n    gateway_id   = aws_internet_gateway.gw.id\n  }\n\n  route {\n    ipv6_cidr_block        = \"::/0\"\n    gateway_id             = aws_internet_gateway.gw.id\n  }\n\n  tags = {\n    Name = \"prod\"\n  }\n}\n</code></pre> <ol> <li>For the subnet <code>10.0.1.0/24</code> we're going to send it to the internet gateway.</li> <li>We set up a default route, i.e., all traffic is going to get sent to the internet gateway.</li> </ol>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#step-4-create-a-subnet","title":"Step 4: Create a subnet","text":"<pre><code>resource \"aws_subnet\" \"subnet-1\" {\n  vpc_id            =  aws_vpc.prod-vpc.id\n  cidr_block        = \"10.0.1.0/24\"\n\n  availability_zone = \"us-east-1a\"\n\n  tags = {\n    Name = \"prod-subnet\"\n  }\n}\n</code></pre>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#step-5-associate-subnet-with-route-table","title":"Step 5: Associate subnet with Route table","text":"<p>For this we'll use another resource type within Terraform called route table association</p> <pre><code>resource \"aws_route_table_association\" \"a\" {\n  subnet_id      = aws_subnet.subnet-1.id\n  route_table_id = aws_route_table.prod-route-table.id\n}\n</code></pre>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#step-6-create-a-security-group","title":"Step 6: Create a security group","text":"<pre><code>resource \"aws_security_group\" \"allow_web\" {\n  name        = \"allow_web_traffic\"\n  description = \"Allow TLS inbound traffic and all outbound traffic\"\n  vpc_id      = aws_vpc.prod-vpc.id\n\n  ingress {\n    description      = \"HTTPS\"\n    from_port        = 443             # (1)!\n    to_port          = 443\n    protocol         = \"tcp\"\n    cidr_blocks      = [\"0.0.0.0/0\"]   # (2)!\n  }\n  ingress {\n    description      = \"HTTP\"\n    from_port        = 80             \n    to_port          = 80\n    protocol         = \"tcp\"\n    cidr_blocks      = [\"0.0.0.0/0\"]  \n  }\n  ingress {\n    description      = \"SSH\"\n    from_port        = 22         \n    to_port          = 22\n    protocol         = \"tcp\"\n    cidr_blocks      = [\"0.0.0.0/0\"]  \n  }\n\n  egress {\n    from_port        = 0               # (3)!\n    to_port          = 0\n    protocol         = \"-1\"            # (4)!\n    cidr_blocks      = [\"0.0.0.0/0\"]\n  }\n\n  tags = {\n    Name = \"allow_web\"\n  }\n}\n</code></pre> <ol> <li><code>from_port</code> and <code>to_port</code> allows us to specify port in specific range.</li> <li>You can even specify a specific IP address, say you're work-computer so that only that can access it.</li> <li>We're allowing all ports in the egress direction.</li> <li><code>-1</code> means any protocol</li> </ol>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#step-7-create-a-network-interface","title":"Step 7: Create a Network Interface","text":"<pre><code>resource \"aws_network_interface\" \"web-server-nic\" {\n  subnet_id       = aws_subnet.subnet-1.id\n  private_ips     = [\"10.0.1.50\"]                     # (1)!\n  security_groups = [aws_security_group.allow_web.id] # (2)!\n}\n</code></pre> <ol> <li>What IP we need to give the server. We can choose any IP from the subnet except for those that AWS reserves a couple of addresses.</li> <li>Pass in a list of security group.</li> </ol>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#step-8-assign-elastic-ip-to-nic","title":"Step 8: Assign Elastic IP to NIC","text":"<pre><code>resource \"aws_eip\" \"one\" {\n  domain                    = \"vpc\"\n  network_interface         = aws_network_interface.web-server-nic.id\n  associate_with_private_ip = \"10.0.1.50\"  # (1)!\n  depends_on = [aws_internet_gateway.gw]  # (2)!\n}\n</code></pre> <ol> <li>Reference the <code>private_ips</code> we assigned to our NIC, the IP we gave to the server.</li> <li>Take a look at documentation for <code>aws_eip</code>. We can use <code>depends_on</code> to set an explicit dependency on the Internet Gateway. We want to reference the whole object, so no <code>id</code> here.</li> </ol> <p>Deploying an elastic IP requires the Internet Gateway to be deployed first before the actual elastic IP gets deployed.</p>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#step-9-create-fedora-server-and-install-apache2","title":"Step 9: Create Fedora Server and install apache2","text":"<pre><code>resource \"aws_instance\" \"web-server-instance\" {\n  ami                = \"ami-081f29ca9a2a16cec\"\n  instance_type      = \"t2.micro\"\n  availability_zone  = \"us-east-1a\"\n  key_name           = \"main-key\"\n\n  # another block\n  network_interface {\n    device_index = 0\n    network_interface_id = aws_network_interface.web-server-nic.id\n  }\n\n  user_data = &lt;&lt;-EOF\n              #!/bin/bash\n              sudo dnf update -y\n              sudo dnf install httpd -y\n              sudo systemctl enable httpd.service\n              sudo systemctl start httpd.service\n              sudo bash -c 'echo you very first web server &gt; /var/www/html/index.html'\n              EOF   \n\n  tags = {\n    Name = \"fedora-server\"\n  }\n}\n</code></pre>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#terraform-commands","title":"Terraform Commands","text":"<p>You can just hit <code>terraform</code>, and it will list all terraform command available to execute.</p> <p>We'll take a look at <code>terraform state</code> command.</p> <pre><code># Will list all resources that we have state for\nterraform state list\n</code></pre> <p>If you want to take a detailed look use <code>show</code> sub-command passing it the resource <code>id</code> shown in <code>state list</code> output:</p> <pre><code>terraform state show &lt;resource_id&gt;\n</code></pre>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#terraform-output","title":"Terraform Output","text":"<p>What we could get Terraform to automatically print resources properties out when we run a <code>terraform apply</code>, i.e, when the resource is created?</p> <p>We can try this let's say when we want to extract the public IP, that's going to get assigned to an elastic IP or an EC2 instance that gets created.</p> <pre><code>output \"server_public_ip\" {\n  value = aws_eip.one.public_ip  # (1)!\n}\n</code></pre> <ol> <li>You can get the property you want to show from <code>terraform state show</code> sub-command.</li> </ol> <p>Next time, when you'll run <code>terraform apply</code> it will print the property for you, instead of you having to manually go and check it either via AWS Console or via Terraform <code>state</code> command.</p> <p>use <code>--auto-approve</code> with <code>terraform apply</code> to automatically approve the changes</p> <p>Use carefully though!</p> <p>If you do add <code>output</code> but don't want to run <code>terraform apply</code>, because it can potentially make changes to your network. In production environment, you don't want to accidentally deploy or delete something, but just to see what the output is.</p> <p>In that case, you should use:</p> <pre><code>terraform refresh\n</code></pre> <p>Which refreshes all of your state, and it'll run the outputs. So, you can verify them without actually having to apply anything.</p>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#target-resources","title":"Target Resources","text":"<p>Maybe you just wanted to delete an individual resource or if you're trying to roll out with a deployment to do staged deployments, where only certain resources are to be deployed one day. And then the next day, another set of resources.</p> <p>We can individually target the resources in our config by passing a <code>-target</code> flag.</p> <p>Let's say we want to destroy the web server, we can run:</p> <pre><code>terraform destroy -target aws_instance.web-server-instance\n</code></pre> <p>and to redeploy it:</p> <pre><code>terraform apply -target aws_instance.web-server-instance\n</code></pre>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#variables","title":"Variables","text":"<p>Terraform allows us to use variables so that we can reuse values throughout our code without having to repeat ourselves.</p> <p>Let's say we want to take <code>cidr_block</code> definition for <code>subnet-1</code> and store it in a variable.</p> <pre><code>resource \"aws_subnet\" \"subnet-1\" {\n  vpc_id = aws_vpc.prod-vpc.id\n  cidr_block = \"10.0.1.0/24\"\n\n  availability_zone = \"us-east-1a\"\n\n  tags = {\n    Name = \"prod-subnet\"\n  }\n}\n</code></pre> <p>You can do it like this:</p> <pre><code>variable \"subnet_prefix\" {   # (1)!\n  description = \"cidr block for the subnet\"\n  default     = \"10.0.66.0/24\" # (2)!\n  type        = string # (3)!\n}\n</code></pre> <ol> <li>Either we can leave this block empty or provide optional values, which are <code>description</code>, <code>default</code> &amp; <code>type</code>.</li> <li>If user doesn't specify a default value, use the default one.</li> <li>We can type constrain it, so when a user enters in a value, we can make sure that they enter the proper type for this. Take a look at Types and Values</li> </ol> <p>To reference it we use it something like this:</p> <pre><code>resource \"aws_subnet\" \"subnet-1\" {\n  vpc_id = aws_vpc.prod-vpc.id\n  cidr_block = var.subnet_prefix\n\n  availability_zone = \"us-east-1a\"\n\n  tags = {\n    Name = \"prod-subnet\"\n  }\n}\n</code></pre> <p>When we hit <code>terraform apply</code>, Terraform will ask you to enter the value, since the var has not been assigned a value.</p> <ul> <li>Another way we can assign it a value is using command line arguments.</li> </ul> <pre><code>terraform apply -var \"subnet_prefix=10.0.100.0/24\"\n</code></pre> <ul> <li>The best way would be to use separate file to assign a variable.</li> </ul> <p>Terraform looks for <code>terraform.tfvars</code> for variable assignments. We can keep our vars in this file as:</p> <pre><code>subnet_prefix=\"10.0.100.0/24\"\n</code></pre> <p>And when you run <code>terraform apply</code>, it won't ask you for the variable assignment and instead take it from this file.</p>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#multiple-tfvars-files","title":"Multiple <code>.tfvars</code> files?","text":"<p>We can pass in the filename to look into for vars like this:</p> <pre><code>terraform apply -var-file example.tfvars\n</code></pre>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#using-list-as-a-variable","title":"Using list as a variable.","text":"<p>Let's say our <code>terraform.tfvars</code> looks like this:</p> <pre><code>subnet_prefix=[\"10.0.1.0/24\", \"10.0.2.0/24]\n</code></pre> <p>and Terraform file like:</p> <pre><code># provider config...\n\nresource \"aws_vpc\" \"prod-vpc\" {\n  cidr_block = \"10.0.0.0/16\"\n  tags = {\n    Name = \"production\"\n  }\n}\n\nvariable \"subnet_prefix\" {  \n  description = \"cidr block for the subnet\"\n}\n\nresource \"aws_subnet\" \"subnet-1\" {\n  vpc_id = aws_vpc.prod-vpc.id\n  cidr_block = var.subnet_prefix[0]\n\n  availability_zone = \"us-east-1a\"\n\n  tags = {\n    Name = \"prod-subnet-1\"\n  }\n}\n\nresource \"aws_subnet\" \"subnet-2\" {\n  vpc_id = aws_vpc.prod-vpc.id\n  cidr_block = var.subnet_prefix[1]\n\n  availability_zone = \"us-east-1a\"\n\n  tags = {\n    Name = \"prod-subnet-2\"\n  }\n}\n</code></pre>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#using-objects-within-variables","title":"Using objects within variables","text":"<p>We'll change the way we assign the tag. Instead, of assigning the tag or hard coding it, we want to have a variable/object that has two properties, <code>cidr_block</code> and <code>name</code> property.</p> <p>In our <code>terraform.tfvars</code>:</p> <pre><code>subnet_prefix = [\n  { cidr_block = \"10.0.1.0/24\", name = \"prod-subnet\"}, \n  { cidr_block = \"10.0.2.0/24\", name = \"dev_subnet\"}\n]\n</code></pre> <p>And so our Terraform file will look something like:</p> <p>```terraform hl_lines=\"16 21 27 32\"</p>"},{"location":"cs/devops/terraform/terraform-freecodecamp-course/#provider-config","title":"provider config...","text":"<p>resource \"aws_vpc\" \"prod-vpc\" {   cidr_block = \"10.0.0.0/16\"   tags = {     Name = \"production\"   } }</p> <p>variable \"subnet_prefix\" {   description = \"cidr block for the subnet\" }</p> <p>resource \"aws_subnet\" \"subnet-1\" {   vpc_id = aws_vpc.prod-vpc.id   cidr_block = var.subnet_prefix[0].cidr_block</p> <p>availability_zone = \"us-east-1a\"</p> <p>tags = {     Name = var.subnet_prefix[0].name   } }</p> <p>resource \"aws_subnet\" \"subnet-2\" {   vpc_id = aws_vpc.prod-vpc.id   cidr_block = var.subnet_prefix[1].cidr_block</p> <p>availability_zone = \"us-east-1a\"</p> <p>tags = {     Name = var.subnet_prefix[1].name   } } ```AKIA3HFXV3NBTRFZRZOI</p> <ol> <li> <p>Created on January 21, 2024.\u00a0\u21a9</p> </li> </ol>"},{"location":"cs/rust/advanced/","title":"Advanced Rust","text":""},{"location":"cs/rust/advanced/rust_str_types/","title":"All Rust String Types explained","text":"<p>Unlike other languages, Rust str types comes in many flavours<sup>1</sup>:</p> <ol> <li><code>&amp;Str</code></li> <li><code>String</code></li> <li><code>&amp;[u8; N]</code></li> <li><code>Vec[u8]</code></li> <li><code>Cow&lt;'a, str&gt;</code></li> <li><code>CStr</code></li> <li><code>OsStr</code></li> <li><code>OSString</code></li> <li><code>Path</code> 10 <code>PartBuff</code></li> </ol>"},{"location":"cs/rust/advanced/rust_str_types/#string-fundamentals","title":"String Fundamentals","text":"<p>In programming, everything is fundamentally binary data represented via 1's and 0's. For a program to convert this binary data into a human readable string, it needs two things:</p> <ol> <li>The encoding used</li> <li>Length of the string</li> </ol>"},{"location":"cs/rust/advanced/rust_str_types/#encoding","title":"Encoding","text":"<p>Binary data is typically processed as a sequence of bytes, each byte containing 8 bits. Each byte can be represented as an integer using a binary number system. A character encoding is simply a standard for maping bytes to characters.</p> <p>There are two common encodings you probably already heard of:</p> <ol> <li> <p>ASCII: Stands for American Standard Code for Information Interchange very simple standard dating back to 90's.</p> <p>ASCII's simplicity is also it's limiting factor, and can only represent 256 characters. And as such, can only represent english alphabet symbols and control characters and has no support for other languages or emoji/emoticons.</p> <p> (source: Wikipedia) </p> </li> <li> <p>UTF-8: UTF-8 short for Unicode Transformation Format-8 is a variable length encoding where characters can go from 1 to 4 bytes, allowing us to encode multiple languages characters and complex characters like emojis. UTF-8 is widely adopted and is currently the standard for character encoding.</p> </li> </ol> <p>Now with this information, string is a sequence of bytes that lives within the memory.</p> <p>When you create a string in your program, you basically create a pointer to the first bytes of the string, but how do you know where the string end? You can device two methods:</p> <ol> <li>Traverse the string bytes-by-byte until you read a termination character, commonly called null byte. Incurrs a runtime cost for certain operations.</li> <li>Or store the length of string along with the pointer to the first byte of the string in a higher level data structure. This gives the benefit for certain operations, such as retrieving the string length will be done in constant time, trade off being, use of more memory.</li> </ol>"},{"location":"cs/rust/advanced/rust_str_types/#strings-in-c","title":"Strings in C","text":"<p>Let's understand why strings in C is really simple but also makes it dangerous.</p> <p>In C, string is represented as an array of character:</p> <pre><code>int main() {\n    char my_string[] = \"Hello, World!\";\n\n    return 0\n}\n</code></pre> <p>or a pointer pointing to the first character in the string:</p> <pre><code>int main() {\n    char my_string[] = \"Hello, World!\";\n\n    char* string_ptr = my_string;\n\n    return 0\n}\n</code></pre> <pre><code>int main() {\n    char my_string[] = \"Hello, World!\\0\";   // (1)\n\n    char* string_ptr = my_string;\n\n    return 0\n}\n</code></pre> <ol> <li>A null terminator is automatically added by the compiler at the end of the string.</li> </ol> <p>C doesn't enfore any encoding and developer is supposed to enforce it. If your program takes in an invalid input say, invalid utf-8 characters, or your validation isn't done properly, can lead to data corruption or security vulnerabilities.</p> <p>Take a look at this example, this can lead to data corruption, undefined behavior, security vulnerabilities or system crashes.</p> <pre><code>int main() {\n    char my_string[] = \"Hello, World\"; // 12 characters\n\n    char buffer[16];  // Buffer is one character (null terminator) short\n    strcpy(buffer, my_string);\n\n    printf(\"The copied string is: %s\\n\", buffer);\n    return 0;\n}\n</code></pre>"},{"location":"cs/rust/advanced/rust_str_types/#strings-in-rust","title":"Strings in Rust","text":"<p>Let's see how Rust handle string with safety in mind:</p> <ol> <li>No null terminator: Rust stores the string length as metadata instead of null terminator, leading to more efficient runtime operations and prevent vulnerabilities like buffer overflows.</li> <li>All strings are valid UTF-8: Rust guarantees string to be a valid UTF-8 encoded, ensuring string are intercompatible with languages and prevents issues like data corruption.</li> <li>Immutable by default: strings or more generally variables in rust are immutable by default, ensuring that the content of the strings are changed unexpectedly.</li> </ol>"},{"location":"cs/rust/advanced/rust_str_types/#strings-and-str","title":"Strings and &amp;str","text":"<p><code>String</code> and <code>&amp;str</code> (called string slices) are two most fundamental string types in Rust.</p> <p><code>String</code> type is heap allocated and hence growable, UTF-8 encoded string.</p> <ul> <li>It's an owned type, since it owns the underlying data</li> <li>responsible for deallocating the underlying data when the string variable goes out of scope.</li> </ul> <pre><code>let my_string: String = String::from(\"Hello!\");\n</code></pre> <p>This type consists of a pointer to the data on the heap, it's length and it's capcity.</p> <p>String slices</p> <p>String slices (<code>&amp;str</code>) on the other hand is a view into a string it represents. It represents a cotiguous sequence of UTF-8 encoded bytes.</p> <ul> <li>It's called a borrow type, as it doesn't own the underlying data. Instead it simply has access to it.</li> <li>It's only holds the pointer to the starting byte of the data and the length. <li>They also doesn't have a <code>capacity</code>, since they are not growable.</li> <li>They can reference data on the heap or in the data section of the compiled binary (which is the case for string literals) or strings stored on stack.</li> <pre><code>let my_string: String = String::from(\"Hello!\");\nlet my_str: &amp;str = &amp;my_string;\n</code></pre> <p>Use case: 1. <code>String</code> is useful when you want to create or modify string data dynamically at runtime. 2. <code>&amp;str</code> is useful when you want to read or analyze pre-existing string data without making changes to it.</p>"},{"location":"cs/rust/advanced/rust_str_types/#static-str","title":"<code>&amp;'static str</code>","text":"<p>Now let's see how Rust handle string with efficiency &amp; flexibility in mind:</p> <p>This is an example of string literal, which is a reference to string slice. String literals are stored in compiler's binary.</p> <p><pre><code>let hello: &amp;str = \"Hello, world!\";\n</code></pre> This is actually just a syntactic sugar for a reference with static lifetime.</p> <pre><code>let hello: &amp;'static str = \"Hello, world!\";\n</code></pre> <p>A static lifetime indicates that the data being pointed to is guaranteed to be available during the entirity of the program execution.</p> <p>You don't need to explicity write out the static lifetime, except in certain cases, for e.g.:</p> <ul> <li> <p>When storing string slices in structs or enums:</p> <p><pre><code>#[dervice(Debug]\nenum MyError {\n    IoErrror,\n    ParseError(&amp;'static str),\n}\n</code></pre> - when, returning a string slice from a function that has no other borrowed paramter.</p> </li> </ul> <pre><code>fn get_greeting(hour: u8) -&gt; &amp;'static str {\n    if hour &lt; 12 {\n        \"Good Morning\"\n    } else if hour &lt; 18 {\n        \"Good Afternnon!\"\n    } else {\n        \"Good evening!\"\n    }\n}\n</code></pre>"},{"location":"cs/rust/advanced/rust_str_types/#boxstr","title":"<code>Box&lt;str&gt;</code>","text":"<p>You may notice <code>&amp;str</code> is made up two parts: <code>str</code> represents a dynamically sized sequence of UTF-8 encoded bytes, in other words <code>str</code> describes a string slice. We can't use it directly as a standalone type, because it's size is not known at compile time. Instead we have to use <code>str</code> behind some type of pointer: <code>&amp;str</code>, creating a reference.</p> <p>There are other types of string slice as well:</p> <p>We can wrap the <code>str</code> type in <code>Box</code> smart pointer: <code>Box&lt;str&gt;</code>.</p> <p><code>Box&lt;str&gt;</code> represents a string slice which is: <ol> <li>Owned,</li> <li>Non-growable,</li> <li>Heap allocated</li> </ol> <p>Useful when you want to freeze a <code>String</code> to avoid extra modification or save memory by dropping the extra capacity information.</p> <pre><code>let my_string: String = String::from(\"This is a long string, needs no modification\");\n\n// convert the String to a Box&lt;str&gt;\nlet my_boxed_str: Box&lt;str&gt; = my_string.into_boxed_str();\n\nprintln!(\"My boxed str: {}\", my_boxed_str);\n</code></pre>"},{"location":"cs/rust/advanced/rust_str_types/#rcstr","title":"<code>Rc&lt;str&gt;</code>","text":"<p>We can also use <code>str</code> with <code>Rc</code> (reference counting) smart pointer: <code>Rc&lt;str&gt;</code>.</p> <p><code>Rc&lt;str&gt;</code> represents a string slice which is: <ol> <li>shared,, to allowing sharing between parts of program without cloning the actual data.</li> <li>Immutable</li> </ol> <pre><code>use str::rc::Rc;\n\nfn main() {\n    let some_large_text: &amp;'static str = \"This is some large text that we need to work with.\";\n\n    // Extract a subsection that multiple parts of the program will need to reference\n    let subsection: Rc&lt;str&gt; = Rc::from(&amp;some_large_text[5..24]);\n\n    // Simulate multiple owners by cloning the Rc pointer\n    let another_reference = Rc::clone(&amp;subsection);\n    let yet_another_reference = Rc::clone(&amp;subsection);\n\n    println!(\"First reference: {}\", subsection);\n    println!(\"Second refernce: {}\", another_reference);\n    println!(\"Third reference: {}\", yet_another_reference);\n}\n</code></pre>"},{"location":"cs/rust/advanced/rust_str_types/#arcstr","title":"<code>Arc&lt;str&gt;</code>","text":"<p>Atomic refernce counting smart pointer, unlike <code>Rc</code> smart pointer, <code>Arc</code> is thread safe.</p> <p><code>Arc&lt;str&gt;</code> represents a string slice which is: <ol> <li>shared,, to allowing sharing between parts of program without cloning the actual data.</li> <li>thread-safe,</li> <li>Immutable</li> </ol> <p>Allows us to share the <code>str</code> across threads, without having to clone the data.</p> <pre><code>fn main() {\n    let tet_string = String::from(\"This is some text taht multiple threads will read.\");\n    let text_slice = &amp;text_string[..];  // this also doesn't have 'static lifetime\n\n    // Convert it to an Arc&lt;str&gt;\n    for _ in 0..3 {\n        let text_ref = Arc::clone(&amp;shared_text);\n        let handle = thread::spawn(move || {\n            println!(\"Thread is reading: {}\", text_ref)\n        })\n    }\n\n    for handle in handles {\n        handle.join().unwrap();\n    }\n}\n</code></pre>"},{"location":"cs/rust/advanced/rust_str_types/#vecu8-u8","title":"<code>Vec[u8]</code> &amp; <code>&amp;[u8]</code>","text":"<p>The <code>String</code> type is basically a wrapper around <code>Vec[u8]</code> the difference being, the <code>String</code> type gurantees to be a valid UTF-8.</p> <p>This allows the <code>String</code> type to provide methods that make it convenient to work with unicode text and safe manipulation of underlying data.</p> <p>We can use Vector of bytes <code>Vec[u8]</code> or slice of bytes <code>&amp;[u8]</code> can be useful when creating string from binary data or with strings which uses encodings other than UTF-8.</p> <pre><code>fn latin1_to_string(latin1_data: &amp;[u8]) -&gt; String {\n    latin1_data.iter().map(|&amp;c| c as char).collect()\n}\n\nfn main() -&gt; io::Result&lt;()&gt; {\n    // Read Latin-1 data into a Vec&lt;u8&gt;\n    let latin1_data: Vec&lt;u8&gt; = read_latin1_string()?;\n\n    // Convert Latin-1 data to a UTF-8 Rust String\n    let utf8_string: String = latin1_to_string(&amp;latin1_data);\n\n    println!(\"Converted string: {}\", utf8_string);\n\n    Ok(());\n}\n</code></pre>"},{"location":"cs/rust/advanced/rust_str_types/#string-literal-representations-u8-n","title":"String literal representations (<code>&amp;[u8; N]</code>)","text":"<ol> <li> <p>Raw string literals: allows us to write special characters like back slashes without need of escape character. They are creating by starting the string with <code>r</code> and adding a <code>#</code> hash symbol on either side of string literal.</p> <pre><code>let text = \"He said \\\"goodbye\\\" and left\";\n\nlet text = r#\"He said \"goodbye\" and left\"#; // raw string literal (1)\nlet re = regex::Regex::new(r#\"\\b(word)\\b\"#).unwrap();\n</code></pre> <ol> <li>Raw string literals can be useful in cases like writing regular expressions or defining JSON objects as string literals.</li> </ol> </li> <li> <p>Byte string: are created by prefixing a string literal with a <code>b</code>. This creates a slice of bytes which is useful for dealing with network protocols that expects a byte sequence.</p> </li> </ol> <pre><code>let http_ok: &amp;[u8; 17] = b\"HTTP/1.1 200 ok\\r\\n\";\n</code></pre>"},{"location":"cs/rust/advanced/rust_str_types/#mut-str","title":"<code>&amp;mut str</code>","text":"<p>String slices are usually represented as <code>&amp;str</code>, i.e, immutable. But it is possible to create a mutable reference, i.e., <code>&amp;mut str</code>.</p> <p>This allows us to directly modify the contents of a string slice while ensuring memory safety and UTF-8 compliance.</p> <p>Useful for in-place string transformations without needing to create a allocate memory for separate string.</p> <pre><code>fn anonymize_emails(s: &amp;mut str) {\n    let re = regex::Regex::new(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\").unwrap();\n\n    let mut matches = Vec::new();\n\n    // ..\n\n    // Now that we have the matches, perform the replacements\n    let s_bytes: &amp;mut [u8] = unsafe { s.as_bytes_mut() };\n    for range in matches {\n        let replacement = vec![b'*'; range.end - range.start];\n        s_bytes[range].copy_from_slice(&amp;replacement);\n    }\n}\n</code></pre> <p>Warning</p> <p>Mutable slices are generally avoided in idiomatic Rust code due to complexities and potential of invalid UTF-8 string.</p>"},{"location":"cs/rust/advanced/rust_str_types/#cowa-str","title":"<code>Cow&lt;'a, str&gt;</code>","text":"<p><code>Cow</code> or Copy on write, is useful when you have a function that sometimes modifies a string and other times doesn't, and you want to avoid making new allocation in cases where no modifications are made.</p> <pre><code>use std::borrow::Cow;\n\nfn sanitize(input: &amp;str) -&gt; Cow(str) {\n    if input.contains(\"badword\") {\n        let sanitized: String = input.replace(\"badword\", \"****\");\n        return Cow::Owned(sanitized);\n    }\n    Cow::Borrowed(input)  // (1)\n}\n</code></pre> <ol> <li>Returns the string withuot allocating anything a new string</li> </ol> <p>Let's discuss string types that focuses on interoperability, which abstract away differences between operating systems and help Rust code connect with other languages.</p>"},{"location":"cs/rust/advanced/rust_str_types/#osstring-osstr","title":"<code>OsString</code> &amp; <code>OsStr</code>","text":"<p><code>OsString</code> &amp; <code>OsStr</code> can contain any sequence of bytes on unix-like systems or any sequence of 16-bit values on Windows. This is useful for interacting with system calls that don't require strings to be UTF-8 encoded.</p> <pre><code>fn main() -&gt; std::io::Result&lt;()&gt; {\n    let paths = fs::read_dir(\".\")?;\n\n    for path in paths {\n        match path {\n            Ok(entry) =&gt; {\n                let os_string: OsString = entry.file_name();\n                match os_string.into_string() {\n                    Ok(string) =&gt; println!(\"Found a file: {]\", string),\n                    Err(os_string) =&gt; println!(\"Found a non-UTF-8 filename: {:?}\", os_string),\n                }\n            }\n            Err(_) =&gt; println!(\"Couldn't read the path.\"),\n        }\n    }\n\n    Ok(())\n}\n</code></pre>"},{"location":"cs/rust/advanced/rust_str_types/#path-pathbuf","title":"<code>Path</code> &amp; <code>PathBuf</code>","text":"<p><code>Path</code> is an immutable view of a path, useful for reading or inspecting paths.</p> <p><code>PathBuf</code> is a mutable and owned version of <code>Path</code> similar to <code>String</code> type, used when you want to create or modify paths.</p> <p>These are useful for filesystem handling across various OSs, as they handle file paths differently.</p> <p>Let's see an example:</p> <pre><code>fn read_file(dir: &amp;Path, filename: &amp;str) -&gt; std::io::Result&lt;String&gt; {\n    let mut full_path = PathBuf::from(dir);\n    full_path.push(filename);\n\n    let mut file = File::open(full_path)?;\n    let mut content = String::new();\n    file.read_to_string(&amp;mut content)?;\n\n    Ok(content)\n}\n\nfn main() -&gt; std::io::Result&lt;()&gt; {\n    let dir = Path::new(\"./\");\n    let content = read_file(dir, \"example.txt\")?;\n    println!(\"File content: {}\", content);\n\n    Ok(())\n}\n</code></pre>"},{"location":"cs/rust/advanced/rust_str_types/#cstr-cstring","title":"<code>CStr</code> &amp; <code>CString</code>","text":"<p>Useful when interfacing rust code with C libraries that expects null terminated strings, providing a safe way to handle C compatible strings.</p> <p>Let's see an example of retrieving env variable from a C code:</p> <pre><code>extern \"C\" {\n    fn getenv(name: *const str::os::raw::c_char) -&gt; *const std::os::raw::c_char;\n}\n\nuse std::ffi::{CStr, CString};\n\nfn main() {\n    let key = CString::new(\"PATH\").expect(\"CString::new failed\");\n    unsafe {\n        let val = getenv(key.as_ptr());\n        if val.is_null() {\n            let c_str = CStr::from_ptr(val);\n            let str_slices = c_str.to_str().unwrap();\n            println!(\"Found: {}\", str_slice\n        } else {\n            println!(\"Not found\");\n        }\n    }\n}\n</code></pre> <ol> <li> <p>Content from Let's Get Rusty's Video: https://www.youtube.com/watch?v=CpvzeyzgQdw \u21a9</p> </li> </ol>"},{"location":"cs/rust/tutorial/","title":"Rust","text":"<p>Notes from Rust Lang Tutorial based on 'The Rust Book'</p>"},{"location":"cs/rust/tutorial/common-collections/","title":"Common Collections in Rust","text":"<p>Collections allow us to store multiple values, but unlike array or tuples, they are allocated on the heap. Meaning the size of the collection could grow or shrink as needed.</p> <p>Common Collections</p>"},{"location":"cs/rust/tutorial/common-collections/#vectors","title":"Vectors","text":"<p>Vectors is a type of collection that can store only one type of data.</p> <pre><code>fn main() {\n    let a = [1, 2, 3]; // array\n\n    let mut v: Vec&lt;i32&gt; = Vec::new(); // empty vector\n    // a vector can grow\n    v.push(1);\n    v.push(2);\n    v.push(3);\n\n    // create vector using macro with value\n    let v2 = vec![1, 2, 3];\n}\n</code></pre>"},{"location":"cs/rust/tutorial/common-collections/#access-elements","title":"Access elements","text":"<pre><code>fn main() {\n    let v = vec![1, 2, 3, 4, 5];\n\n    let third = &amp;v[2]; // a reference to vector\n    println!(\"The third element is {}\", third);\n}\n</code></pre> <p>If we try to access out-of-bound element, the program panics and we get a run time error.</p> <pre><code>let third = &amp;v[20];\n</code></pre> <pre><code>thread 'main' panicked at 'index out of bounds: the len is 5 but the index is 20', src/main.rs:4:18\n</code></pre> <p>If we had used array and tried to access index that was out of bounds, we'd get compile time error, program would'nt even run. This is because with the arrays the size is fixed and knows at compile time but not with vectors.</p> <p>Don't use index to access elements of vector</p> <p>Success</p> <p>Insead use the <code>get</code> method to gracefully handle the out-of-bound index which returns an <code>Option</code> Enum.</p> <pre><code>match v.get(2) {\n    Some(third) =&gt; println!(\"The third element is {}\", third),\n    None =&gt; println!(\"There is no third element.\")\n}\n</code></pre> <p>We know that an immutable or mutable reference to an item can't exist at the same time. So if we wanted to push an element <code>third</code> before printing it out:</p> <pre><code>let third = &amp;v[2];\n// error. Cannot borrow `v` as mutable because it is also borrows as immutable\nv.push(6);\nprintln!(\"The third element is {}\", third);\n</code></pre> <p>This is because when we need to add element to a vector, we might need to allocate more memory if the vector is full. In that case <code>third</code> will go invalid pointing to unknown memory address.</p>"},{"location":"cs/rust/tutorial/common-collections/#iterating-over-elements","title":"Iterating over elements","text":"<p>Let's iterate over all elements and print them.</p> <pre><code>fn main() {\n    let mut v: vec![1, 2, 3, 4, 5];\n\n    // take a immmutable reference of each element\n    for i in &amp;v {\n        println!(\"{}\", i);\n    }\n\n    // take a mutable reference\n    for i in &amp;mut v {\n        *i +=50; // dereference operator and add 50\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/common-collections/#storing-enum-varients-inside-of-a-vector","title":"Storing Enum varients inside of a vector","text":"<p>A <code>row</code> stores <code>SpreadsheetCell</code> type with different varients.</p> <pre><code>fn main() {\n    enum SpreadsheetCell {\n        Int(i32),\n        Float(f64),\n        Text(String),\n    }\n\n    // you are allowed to store an Enum type with different varients\n    let row: Vec&lt;SpreadsheetCell&gt; = vec![\n        SpreadsheetCell::Int(3),\n        SpreadsheetCell::Text(String::from(\"blue\")),\n        SpreadsheetCell::Float(10.12),\n    ]\n\n    // the catch is when you reference a specific element inside\n    // of vector we need to use a match expression to figure\n    // out which varient of Enum it is.\n    // since we're storing Enum inside\n    match &amp;row[1] {\n        SpreadsheetCell::Int(i) =&gt; println!(\"{}\", i),\n        _ =&gt; println!(\"Not a integer!\")\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/common-collections/#strings","title":"Strings","text":"<p>Strings are stored as a collection of UTF-8 encoded bytes.</p> <ul> <li>In memory a string is just a list or a collection 1s and 0s. Now a program needs to be able to interpret 1s and 0s and print out the correct characters, that's were encoding comes into play.</li> <li> <p>ASCII or American Standard code for Information Interchange is a character encoding and decoding (1s and 0s to string and back). Although it can only represent 128 characters which includes english alphabet, some special characters</p> </li> <li> <p>Since ASCII can't represent other language characters, others countries created their own encoding standards for their own languages.</p> </li> <li>This becomes problematic since how a program will know which encoding standards to use to parsing a collection of bytes</li> <li> <p>To solve this Unicode was created which represent a wider array of characters from all well-known languages, emojis and is also backwards compatible with ASCII.</p> </li> <li> <p>UTF-8 is a variable-width character encoding. Variable because it can be represented as one bytes, two bytes, three bytes or four bytes.UTF-8 is a very popular encoding of Unicode.</p> </li> </ul> <pre><code>fn main() {\n    let s1: String = String::new();    // empty String\n    let s2: &amp;str = \"initial contents\"; // creating string slices\n    let s3: String = s2.to_string;     // turn into owned String\n    let s4: String = String::from(\"initial commit\");\n}\n</code></pre> <p>Strings are UTF-8 encoded, so you can represent other languages as well.</p> <pre><code>fn main() {\n    let hello = String::from(\"\uc548\ub155\ud558\uc138\uc694\");\n    let hello = String::from(\"\u0928\u092e\u0938\u094d\u0924\u0947\");\n    let hello = String::from(\"\u3053\u3093\u306b\u3061\u306f\");\n    let hello = String::from(\"\u4f60\u597d\");\n    let hello = String::from(\"Hello\");\n    let hello = String::from(\"Dobr\u00fd den\");\n    let emoji = String::from(\"\ud83d\ude3c\");\n}\n</code></pre>"},{"location":"cs/rust/tutorial/common-collections/#appending-to-a-string","title":"Appending to a String","text":"<pre><code>fn main() {\n    let mut s = String::from(\"foo\");\n    s.push_str(\"bar\"); // takes a string slice to avoid owning it\n    s.push('!'); // append characers\n    // foobar!\n}\n</code></pre> <pre><code>fn main() {\n    let s1 = String::from(\"Hello, \");\n    let s2 = String::from(\"world!\");\n\n    // moving ownership of `s1` into `s3` and taking all\n    // characters of `s2` append them at end\n    let s3: String = s1 + &amp;s2;\n}\n</code></pre> <pre><code>fn main() {\n    let s1 = String::from(\"Hello, \");\n    let s2 = String::from(\"world!\");\n\n    // contatenate using format macro\n    let s3 = format!(\"{}{}\", s1, s2);\n}\n</code></pre>"},{"location":"cs/rust/tutorial/common-collections/#indexing-into-string","title":"Indexing into string","text":"<pre><code>fn main() {\n    let hello = String::from(\"hello\");\n    // error!\n    // the type `String` cannot be indexed by `{integer}`\n    // the trait `Index&lt;{integer}&gt;` is not implemented for `String`\n    let c: char = hello[0];\n}\n</code></pre> <p>String is a collection of bytes. What is the length of <code>hello</code> here? Well, 5. But what would be the length of <code>hello</code> in this case, hello in Hindi:.</p> <pre><code>let hello = String::from(\"\u0928\u092e\u0938\u094d\u0924\u0947\");\n</code></pre> <p>5? Nope, the length is 18.</p> <p>Or in this case:</p> <pre><code>let hello = String::from(\"\uc548\ub155\ud558\uc138\uc694\");\n</code></pre> <p>12? Nope, 15.</p> <p>In UTF-8 strings could be 1 to 4 bytes long. So getting the first character in the string using <code>[]</code> syntax would not work because <code>hello[0]</code> only specifies the first byte in our collection of bytes</p>"},{"location":"cs/rust/tutorial/common-collections/#representation-of-word-in-unicode","title":"Representation of word in Unicode","text":"<p>Let's understand the three Relevant ways a word in represented in unicode.</p> <p>Rust doens't know what we want to receive; Byes, scalar values or grpheme clusters. For that we need special methods.</p> <pre><code>fn main() {\n    let hello = String::from(\"\u0928\u092e\u0938\u094d\u0924\u0947\");\n    // Bytes\n    // [224, 164, 168, 224, 164, 174, 224, 164, 184, 224, 165, 141, 224, 164, 164, 224, 165, 135]\n\n    // Scalar values (`char` type): represent full or parts of characters\n    // ['\u0928', '\u092e', '\u0938', '\u094d', '\u0924', '\u0947']\n\n    // Grapheme clusters: what every hooman consider as character\n    // [\"\u0928\", \"\u092e\", \"\u0938\u094d\", \"\u0924\u0947\"]\n}\n</code></pre> <pre><code>use unicode_segmentation::UnicodeSegmentation;\n\nfn main() {\n    let hello = String::from(\"\u0928\u092e\u0938\u094d\u0924\u0947\");\n\n    // Ierating over bytes\n    for b in hello.bytes() {\n        println!(\"{}\", b);\n    }\n    // or\n    println!(\"{:?}\", String::from(\"\u0928\u092e\u0938\u094d\u0924\u0947\").as_bytes());\n\n    // Iterating over Scalar value\n    for c in hello.chars() {\n        println!(\"{}\", c);\n    }\n\n    // Iterating over Grapheme clusters is not present by default\n    // to keep the standard library clean and lean.\n    // this requires an external crate: `unicode-segmentation`\n    // `true` to get extended grapheme\n    for g in hello.graphemes(true) {\n        println!(\"{}\", g);\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/common-collections/#hashmaps","title":"Hashmaps","text":"<p>Stores keys, values pair.</p> <p>We need to bring in <code>HashMap</code> from Rust standard library</p>"},{"location":"cs/rust/tutorial/common-collections/#inserting-and-extracting-values-from-hashmap","title":"Inserting and extracting values from HashMap","text":"<pre><code>use std::collections::HashMap;\n\nfn main() {\n    let blue = String::from(\"Blue\");\n    let yellow = String::from(\"Yellow\");\n\n    let mut scores = HashMap::new();\n\n    // move value of strings; taking ownership\n    // If we didn't want the HashMap to take ownership of our String,\n    // We could pass reference to string, but that would have required lifetimes.\n    scores.insert(blue, 10);\n    scores.insert(yellow, 50);\n\n    // To get values back, pass reference to String (or string slice)\n    // this returns an Option enum,\n    // because we can't gurantee a value will be returned\n    let score = scores.get(&amp;String::from(\"Blue\"));\n    let score = scores.get(\"Blue\");\n}\n</code></pre>"},{"location":"cs/rust/tutorial/common-collections/#iterating","title":"Iterating","text":"<pre><code>use std::collections::HashMap;\n\nfn main() {\n    // defines `scores` and insert values\n    for (key, value) in &amp;scores {\n        println!(\"{}: {}\", key, value)\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/common-collections/#updating-hashmap","title":"Updating hashmap","text":"<pre><code>use std::collections::HashMap;\n\nfn main() {\n    let mut scores = HashMap::new();\n\n    scores.insert(String::from(\"Blue\"), 10);\n    scores.insert(String::from(\"Blue\"), 20); // override the original value\n\n\n    // if don't want to override\n    // `entry` gives an Entry enum representing value for given key\n    // If an entry `Yellow` doesn't exist then insert one with value 30; otherwise do nothing\n    scores.entry(String::from(\"Yellow\")).or_insert(30);\n    scores.entry(String::from(\"Yellow\")).or_insert(40); // won't override\n}\n</code></pre>"},{"location":"cs/rust/tutorial/common-collections/#updating-value-based-on-old-value","title":"Updating value based on old value","text":"<p>Here we're are populating <code>map</code> with word count</p> <pre><code>use std::collections::HashMap;\n\nfn main() {\n    let text = \"hello world wonderful world\";\n\n    let mut map = HashMap::new();\n\n    for word in text.split_whitespace() {\n        let count: &amp;mut i32 = map.entry(word).or_insert(0);\n        *count += 1;\n    }\n\n    println!(\"{:?}\", map);\n}\n</code></pre> <p>Output:</p> <pre><code>{\"world\": 2, \"hello\": 1, \"wonderful\": 1}\n</code></pre>"},{"location":"cs/rust/tutorial/common-programming-concepts/","title":"Common Programming Concepts in Rust","text":"<p>Common Programming Concepts</p>"},{"location":"cs/rust/tutorial/common-programming-concepts/#variables-and-mutability","title":"Variables and mutability","text":"<p>Variables are immutable by default. I said it.</p> <p><pre><code>fn main() {\n    let x = 5;\n    println!(\"The value of x is {}\", x);\n    x = 42;\n    println!(\"The value of x is {}\", x);\n}\n</code></pre> This will throw error, stating, cannot assign twice to immutable variable <code>x</code>.</p> <pre><code>error[E0384]: cannot assign twice to immutable variable `x`\n --&gt; src/main.rs:4:5\n  |\n2 |     let x = 5;\n  |         -\n  |         |\n  |         first assignment to `x`\n  |         help: consider making this binding mutable: `mut x`\n3 |     println!(\"The value of x is {}\", x);\n4 |     x = 42;\n  |     ^^^^^^ cannot assign twice to immutable variable\n\nFor more information about this error, try `rustc --explain E0384`.\nerror: could not compile `playground` due to previous error\n</code></pre> <p>Make it mutable as we learned:</p> <pre><code>let mut x = 5;\n</code></pre>"},{"location":"cs/rust/tutorial/common-programming-concepts/#constant","title":"Constant","text":"<p>Values that never changes, create it by using <code>const</code> keyword (make sure to provide a type as well, it's required):</p> <pre><code>const MINUTES_IN_YEAR: i32 = 5_25_600;\n</code></pre> <p>So, why do we require constants, when we already have immutable variables?</p> <p>Constants v/s variables</p> <ol> <li>We cannot mutate a constant. <code>mut</code> keyword is invalid with <code>const</code>.</li> <li><code>const</code>s must also be type annotated.</li> <li><code>const</code> can also be set to constant expressions. We cannot assign return value of functions to them.</li> </ol>"},{"location":"cs/rust/tutorial/common-programming-concepts/#shadowing","title":"Shadowing","text":"<p>Shadowing allows us to create new variables using an existing name.</p> <p>The below code is valid:</p> <p><pre><code>let x = 5; // This gets shadowed by second `x`\nlet x = 42;\n</code></pre> !!! \"Advantages of shadowing\"</p> <pre><code>1. We can preserve immutability.\n\n    Both `x` in above example remains immutable.\n\n2. We can change type.\n\n   ```rust\n   let x = 5;\n   let x = \"six\";\n   ```\n</code></pre>"},{"location":"cs/rust/tutorial/common-programming-concepts/#data-types","title":"Data Types","text":"<p>We have two types of Data Types: 1. Scalar data types: represent a single value 2. Compound data types: represent a group of values</p>"},{"location":"cs/rust/tutorial/common-programming-concepts/#scalar-data-types","title":"Scalar data types:","text":"<ol> <li>Integers: Numbers without a fractional component</li> </ol> Length Signed Unsigned 8-bit <code>i8</code> <code>u8</code> 16-bit <code>i16</code> <code>u16</code> 32-bit <code>i32</code> <code>u32</code> 64-bit <code>i64</code> <code>u64</code> 128-bit <code>i128</code> <code>u128</code> arch (architecture based) <code>isize</code> <code>usize</code> <p>Signed integers can be positive or negative, unsigned are positive only. By default rust uses signed 32 bit integer. </p> <pre><code>fn main() {\n    let a = 98_222; // Decimal\n    let b = 0xff;   // Hex\n    let c = 0o77;   // Octal\n    let d = 0b1111_0000; // Binary\n    let e = b'A';   // Byte (u8 only)\n\n    // Any value greater than what it can hold will\n    // in Debug builds will panic\n    // in Release builds will wrap around back to minimum \n    // i.e., perform 2s Complement wrapping\n    let f: u8 = 255;\n}\n</code></pre> <ol> <li>Floating point numbers</li> </ol> <pre><code>fn main() {\n    let f = 2.0;     // defaults to f64\n    let g: f32 = 3.0;\n}\n</code></pre> <ol> <li>Booleans</li> </ol> <pre><code>fn main() {\n    let t = true;\n    let f: bool = false;\n}\n</code></pre> <ol> <li>Characters: represent unicode characters</li> </ol> <pre><code>fn main() {\n    let c = 'z';\n    let z = 'Z';\n    let heart = '\u2665';\n}\n</code></pre>"},{"location":"cs/rust/tutorial/common-programming-concepts/#compound-data-types","title":"Compound Data Types","text":"<ol> <li>Tuples: A tuple is a collection of values of different types constructed using paranthesis.</li> </ol> <pre><code>fn main() {\n    let tup = (\"Hello, World!\", 100_000);\n\n    // To get values of tuple\n    // 1. Destructuring\n    let (str_var, int_var) = tup;\n\n    // 2. Dot notation\n    // tuples and array start at index 0\n    let int_var  = tup.1;\n}\n</code></pre> <ol> <li>Arrays: An array is a fixed length collection of objects of the same type T, stored in contiguous memory, constructed using bracked [].</li> </ol> <pre><code>fn main() {\n    let http_err_codes = [200, 404, 500];\n    let res_not_found = error_codes[1];\n\n    // create array of size 8, all set to 0\n    let byes = [0; 8];\n}\n</code></pre>"},{"location":"cs/rust/tutorial/common-programming-concepts/#functions","title":"Functions","text":"<p>Function naming scheme specifies that functions should named in snake case, all in lower case characters.</p> <pre><code>fn main() {\n    my_function(11, 22);\n}\n\nfn my_function(x:i32, y:i32) {\n    println!(\"Value of x: {}\", x);\n    println!(\"Value of y: {}\", y);\n}\n</code></pre> <p>In Rust, we can think of any piece of code as either a statement or an expression.</p> <ol> <li>Statements perform some action but do not return a value</li> <li>Expressions returns a value</li> </ol> <p>We can return values in two ways: 1. either using <code>return</code> keyword and specifying return types of function at the end of decalaration.</p> <pre><code>fn my_function(x:i32, y:i32) -&gt; i32 {\n    println!(\"Value of x: {}\", x);\n    println!(\"Value of y: {}\", y);\n    let sum = x + y;\n    return sum\n}\n</code></pre> <ol> <li>or inside a function the last expression is implicityly returned. So something like this is also ok:</li> </ol> <pre><code>fn my_function(x:i32, y:i32) -&gt; i32 {\n    println!(\"Value of x: {}\", x);\n    println!(\"Value of y: {}\", y);\n    x + y\n}\n</code></pre>"},{"location":"cs/rust/tutorial/common-programming-concepts/#control-flow","title":"Control Flow","text":""},{"location":"cs/rust/tutorial/common-programming-concepts/#if-else-statements","title":"if-else statements","text":"<pre><code>fn main() {\n    let number = 5;\n\n    // conditions needs to be explicitly stated\n    if number &lt; 10 {\n        println!(\"smaller than 10\");\n    } else if number &lt; 22 {\n        println!(\"smaller than 22\");\n    } else {\n        println!(\"Condition was false\");\n    }\n}\n</code></pre> <p>We can also use <code>if-else</code> statement inside of <code>let</code> statement.</p> <pre><code>fn main() {\n    let condition = true;\n    let number = if condition { 5 } else { 6 }; \n}\n</code></pre>"},{"location":"cs/rust/tutorial/common-programming-concepts/#loops","title":"Loops","text":"<p>This code below can loop infinitely unless we call <code>break</code>. <pre><code>fn main() {\n    loop {\n        println!(\"again\");\n        break;\n    }\n}\n</code></pre></p> <p>We can also return values from these loops: <pre><code>fn main() {\n    let mut conter = 0;\n    let result = loop {\n        counter += 1;\n        if counter == 10 {\n            break counter;\n        }\n    };\n\n    println!(\"The result is {}\", result);\n}\n</code></pre></p> <p>While loop <pre><code>fn main() {\n    let mut number = 3;\n    while number != 0 {\n        println!(\"{}\", number);\n        number -= 1;\n    }\n\n    println!(\"LIFTOFF!!!\");\n}\n</code></pre></p> <p>For loop <pre><code>fn main() {\n    let a = [10, 20, 30, 40, 50];\n\n    // for every element in `a` without giving \n    // ownership to for loop. \n    // we'll learn more about Ownership later on\n    for element in a.iter() {\n        println!(\"the value is: {}\", element);\n    }\n\n    // loop over range, exclusive\n    for number in 1..4 {\n        println!(\"Range element: {}\", number);\n    }\n}\n</code></pre></p>"},{"location":"cs/rust/tutorial/common-programming-concepts/#comments","title":"Comments","text":"<p>You usually comment in Rust in either of the below two ways: <pre><code>fn main() {\n    // Line comment\n\n    /*\n        Block comment\n    */\n}\n</code></pre></p>"},{"location":"cs/rust/tutorial/enums-and-pattern-matching/","title":"Enums and Pattern Matching in Rust\"","text":"<p>Enums and Pattern Matching in Rust</p>"},{"location":"cs/rust/tutorial/enums-and-pattern-matching/#defining-enums","title":"Defining Enums","text":"<p>Enums allow us to enumerate a list of variants.</p> <p>When is it appropriate to use Enums over Structs? Take an example of IP Addresses, whose all variants can be enumerated, which are just two v4 and v6. And we can express these variants in our code for IP addresses.</p> <pre><code>enum IPAddrKind {\n    V4,\n    V6,\n}\n\nfn main() {\n    let four = IPAddrKind::V4;\n    let six = IPAddrKInd::V6;\n}\n\n// a function that accepts variables of type `IPAddrKind`\nfn route(ip_kind: IPAddrKind) {}\n</code></pre>"},{"location":"cs/rust/tutorial/enums-and-pattern-matching/#enums-in-structs","title":"Enums in Structs","text":"<p>The Enums defined above are able to capture the version of IP address, but what if we wanter to capture the actual IP address as well?</p> <pre><code>enum IPAddrKind {\n   V4,\n   V6\n}\n\n// group version of IP address with actual IP address\nstruct IPAddr {\n    kind: IPAddrKind,\n    address: String\n}\n\nfn main() {\n    let four = IPAddrKind::V4;\n    let six = IPAddrKInd::V6;\n\n    let localhost = IPAddr {\n        kind: IPAddrKind::V4,\n        address: String::from(\"127.0.0.1\")\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/enums-and-pattern-matching/#values-inside-enums","title":"Values inside Enums","text":"<p>What if we can store the data directly inside the Enum variants? This is done by adding Paranthesis <code>()</code> and specifying what type of data does the variant stores.</p> <pre><code>enum IPAddrKind {\n   V4(String),\n   V6(String)\n}\n\nfn main() {\n    let localhost = IPAddrKind::V4(String::from(\"127.0.0.1\"));\n}\n</code></pre> <p>Enum variants can store wide variety of types. To demostrate let's write a <code>Message</code> Enum.</p> <pre><code>enum Message {\n    Quit,                      // stores no data\n    Move {x: i32, y: i32},     // stores anonymous struct\n    Write(String),             // stores a String\n    ChangeColor(i32, i32, i32) // stores three integers\n}\n</code></pre>"},{"location":"cs/rust/tutorial/enums-and-pattern-matching/#enum-methods","title":"Enum Methods","text":"<p>Just like Struct, Enum methods and associated functions can be written using <code>impl</code> block.</p> <pre><code>impl Message {\n    fn some_function() {\n        println!(\"Hello, World!\");\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/enums-and-pattern-matching/#the-option-enum","title":"The Option Enum","text":"<p>Many languages out there, suffers from the sin of <code>Null</code> values, which are useful to represent whether a value exist or is it null, i.e., there is no value. The problem is the type system can't guarantee if you use a value it's not null.</p> <p>Which leads to Runtime Exception, like <code>NullPointerException</code> or languages like Kotlin trying to solve the issue by introduing operator <code>?</code> to check if it's null or not.</p> <p>In Rust there are no NULL values. Instead we have <code>Option</code> enum, which looks something like this (You don't define it, Rust has already defined it for you) and are included in our program scope by default:</p> <pre><code>enum Option&lt;T&gt; {\n    Some(T),   // stores some value of generic type T\n    None       // No value\n}\n</code></pre> <p>Success</p> <p>So if you have any value that could potentially be null/not exist, then you would wrap it in <code>Option</code> Enum.</p> <p>When to use Option or Result type?</p> <ul> <li>Options (to be, or not to be)</li> </ul> <p>Briefly stated, an Option type can either be something or nothing. For example, the value <code>Some(10)</code> is definitely something: an integer wrapped in <code>Some</code>, whereas None is a whole lot of nothing.</p> <pre><code>enum Option&lt;T&gt; {\n    Some(T),\n    None\n}\n</code></pre> <ul> <li>Results (is everything ok?)</li> </ul> <p>This may hold something, or an <code>error</code>. Whereas the <code>Option</code> type uses either <code>Some</code> to wrap successful results or <code>None</code>, the Result type uses <code>Ok</code> to wrap successful results or <code>Err</code> to wrap error information for the situations when things have gone south, e.g. <code>Ok(3.14159)</code>, and <code>Err(\"This Bad Thing Happened\")</code>.</p> <pre><code>enum Result&lt;T, E&gt; {\n    Ok(T),\n    Err(E),\n}\n</code></pre> <p>This allows type system to enforce that we handle the <code>none</code> case when value doesn't exist and in <code>some</code> case the value is present</p> <pre><code>fn main() {\n    let some_number: Option&lt;i32&gt; = Some(5);\n    let some_string: Option&lt;&amp;str&gt; = Some(\"a string\");\n\n    // You don't need to annotate the above variables.\n    // except in below case where no value is passed in so we were\n    // required to annotate\n    let absent_number: Option&lt;i32&gt; = None;\n}\n</code></pre> <p>Danger</p> <p>You can't do something like this:</p> <pre><code>fn main() {\n    let msg = None;\n}\n</code></pre> <p>The compiler will complain:</p> <pre><code>error[E0282]: type annotations needed for `Option&lt;T&gt;`\n --&gt; src/main.rs:4:9\n  |\n4 |     let msg = None;\n  |         ^^^\n  |\nhelp: consider giving `msg` an explicit type, where the type for type parameter `T` is specified\n  |\n4 |     let msg: Option&lt;T&gt; = None;\n  |            +++++++++++\n\nFor more information about this error, try `rustc --explain E0282`.\nerror: could not compile `playground` due to previous error\n</code></pre> <p>Let's look at another example, where we try to add an integer and an optional integer:</p> <pre><code>fn main() {\n    let x: i8 = 5;\n    let y: Option&lt;i8&gt; = Some(5);\n\n    // well, you can't\n    // error[E0277]: cannot add `Option&lt;i8&gt;` to `i8`\n    let sum = x + y;\n}\n</code></pre> <p>For this code to work, we need to extract our integer out of the <code>Some</code> varient. In general to extract values out of <code>Some</code> varient, we'll be required to handle all possible varients, like if the variant is <code>Some</code> we are allowed to safely use the value, otherwise branch out.</p> <p><code>Option</code> Enum has some very useful set of method, for example, here we can use <code>unwrap_or()</code> method on <code>y</code> to use value if it exist, otherwise use the default value.</p> <pre><code>fn main() {\n    ...\n    let sum = x + y.unwrap_or(0);\n}\n</code></pre>"},{"location":"cs/rust/tutorial/enums-and-pattern-matching/#using-match-expressions","title":"Using Match Expressions","text":"<p>We already know that match allows us to compare a value against the set of patterns.</p> <p>This makes a <code>match</code> expression very useful for Enums, to match a variable of an Enum variant. <code>match</code> expressions are exhaustive meaning that we have to match all possible value.</p> <pre><code>enum Coin {\n    Penny,\n    Nickel,\n    Dime,\n    Quarter,\n}\n\nfn value_in_cents(coin: Coin) -&gt; u8 {\n    match coin {\n        Coin::Penny =&gt; 1,\n        Coin::Nickel =&gt; 5,\n        Coin::Dime =&gt; 10,\n        Coin::Quarter =&gt; 25\n    }\n}\n</code></pre> <p>These patterns can also bind to values.</p> <pre><code>// enum to represent the state minted on each quarter\n#[derive(Debug)]\nenum UsState {\n    Albama,\n    Alaska,\n    Arizona,\n    Arkansas,\n    California,\n    //...\n}\n\nenum Coin {\n    Penny,\n    Nickel,\n    Dime,\n    Quarter(UsState),\n}\n\nfn value_in_cents(coin: Coin) -&gt; u8 {\n    match coin {\n        Coin::Penny =&gt; 1,\n        Coin::Nickel =&gt; 5,\n        Coin::Dime =&gt; 10,\n        Coin::Quarter(state) =&gt; {\n            println!(\"State quarter from {:?}!\", state);\n            25\n        }\n    }\n}\n</code></pre> <p>Let write our main function and call this function:</p> <pre><code>fn main() {\n    value_in_cents(Coin::Quarter(UsState::Alaska));\n}\n</code></pre> <p>Which prints:</p> <pre><code>State quarter from Alaska!\n</code></pre> <p>Let's try to combine the <code>match</code> expression with <code>Option</code> enum.</p> <pre><code>fn main() {\n    let five = Some(5);\n    let six = plus_one(five);\n    let none = plus_one(None);\n}\nfn plus_one(x: Option&lt;i32&gt;) -&gt; Option&lt;i32&gt; {\n    match x {\n        None =&gt; None,\n        Some(i) =&gt; Some(i+1),\n    }\n}\n</code></pre> <p>Since <code>match</code> expressions are exhaustive, we need to match for all possible variants. If we can't write the match arm for all possible variants, then we can use <code>_</code> to match for everything else.</p> <pre><code>match coin {\n    Coin::Penny =&gt; 1,\n    Coin::Nickel =&gt; 5,\n    _ =&gt; {\n        println!(\"Some value.\");\n        0\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/enums-and-pattern-matching/#using-if-let-syntax","title":"Using if let Syntax","text":"<pre><code>fn main() {\n    let some_value = Some(3);\n    // we only care about one variant, otherwise do nothing\n    match some_value {\n        Some(3) =&gt; println!(\"three\"),\n        _ =&gt; (),\n    }\n}\n</code></pre> <p>This is a little verbose, we can make it more concise using <code>if-let</code> sytanx. With <code>if-let</code> sytanx we only specify the pattern we care about. <pre><code>fn main() {\n    let some_value = Some(3);\n\n    // start with `if-let` and rest is read backwards, i.e.,\n    // if `some_value` matches `Some(3)` then print \"three\"\n    if let Some(3) = some_value {\n        println!(\"three\");\n    }\n}\n</code></pre></p>"},{"location":"cs/rust/tutorial/error-handling/","title":"Error Handling in Rust","text":"<p>Error Handling</p>"},{"location":"cs/rust/tutorial/error-handling/#using-panic","title":"Using panic!","text":"<p>If your program fails in a way that's unrecoverable or there is no way to handle the error gracefully, then you can call the panic macro, which immediately quit the program.</p> <pre><code>fn main() {\n    panic!(\"crash and burn\");\n}\n</code></pre> <p>Which crashes: <pre><code>thread 'main' panicked at 'crash and burn', src/main.rs:2:5\n</code></pre></p> <p>If we wanted to get a full backtrace of where and in which function did the panic originated, like in this case, we can run with <code>RUST_BACKTRACE=full</code> environment variable set as:</p> <pre><code>RUST_BACKTRACE=full cargo run\n</code></pre> <pre><code>fn main() {\n    a();\n}\n\nfn a() {\n    b();\n}\n\nfn b() {\n    c(22);\n}\n\nfn c(num: i32) {\n    if num == 22 {\n        panic!(\"Don't pass in 22!\");\n    }\n}\n</code></pre> <p>which emits, <pre><code>thread 'main' panicked at 'Don't pass in 22!', src/main.rs:15:9\nstack backtrace:\n   0: rust_begin_unwind\n             at /rustc/a55dd71d5fb0ec5a6a3a9e8c27b2127ba491ce52/library/std/src/panicking.rs:584:5\n   1: core::panicking::panic_fmt\n             at /rustc/a55dd71d5fb0ec5a6a3a9e8c27b2127ba491ce52/library/core/src/panicking.rs:142:14\n   2: playground::c\n             at ./src/main.rs:15:9\n   3: playground::b\n             at ./src/main.rs:10:5\n   4: playground::a\n             at ./src/main.rs:6:5\n   5: playground::main\n             at ./src/main.rs:2:5\n   6: core::ops::function::FnOnce::call_once\n             at /rustc/a55dd71d5fb0ec5a6a3a9e8c27b2127ba491ce52/library/core/src/ops/function.rs:248:5\n</code></pre></p>"},{"location":"cs/rust/tutorial/error-handling/#the-result-enum","title":"The Result Enum","text":"<p>Errors that are recoverable and can be handled gracefully, in those cases the result is required to be wrapped inside <code>Result</code> enum:</p> <pre><code>enum Result&lt;T, E&gt; {\n    Ok(T),   // success case, contains result\n    Err(E),  // error case\n}\n</code></pre> <p>The <code>Result</code> enum represent success or failure</p> <p>For example:</p> <pre><code>use std::fs::File;\n\nfn main() {\n    // calling `File::open()` may fail incase file not existing\n    // and hence it returns a `Result` enum; annotated just for clarity\n    let f: Result&lt;File, Error&gt; = File::open(\"hello.txt\");\n\n    // We must handle both the cases of `Result` using `match`\n    // shadowing to declare f\n    let f = match f {\n        Ok(file) =&gt; file,\n        Err(error) =&gt; panic!(\"Problem opening the file: {:?}\",error)\n    };\n}\n</code></pre> <p>Let's handle it more gracefully by instead of panicking, create a new file:</p> <p>First bring <code>ErrorKind</code> struct from <code>io</code> module into scope, which let us match on the type of errror we get.</p> <pre><code>use std::fs::File;\nuse std::io::ErrorKind;\n</code></pre> <p>and then match on the error we get instead of calling <code>panic!</code> macro:</p> <pre><code>let f = match f {\n        Ok(file) =&gt; file,\n        Err(error) =&gt; match error.kind() {\n            // which returns an enum representing the kind of error\n            // creating a new file can also fail, so match expression for that\n            ErrorKind::NotFound =&gt; match File::create(\"hello.txt\") {\n                Ok(fc) =&gt; fc,\n                Err(e) =&gt; panic!(\"Problem creating the file: {:?}\", e)\n            },\n            other_error =&gt; {\n                panic!(\"Problem opening the file: {:?}\", other_error)\n            }\n        }\n    };\n</code></pre> <p>This looks not so pretty, so we can rewrite the above code using closures, which we'll learn more about later.</p> <pre><code>use std::fs::File;\nuse std::io::ErrorKind;\n\n// `unwrap_or_else()` gives use file or call the anonymous\n// function / closure passing in `error`\n// inside the closue we have if-else statement;\n// - first checking error is created for file not found\n//   - if yes, then try creating the file which itself can fail\n//   - in that case, we have another closure binding error and panicking\n// - otherwise, for any other error, we just panic\nfn main() {\n    let f = File::open(\"hello.txt\").unwrap_or_else( |error| {\n        if error.kind() == ErrorKind::NotFound {\n            File::create(\"hello.txt\").unwrap_or_else( |error| {\n                panic!(\"Problem creating the File: {:?}\", error);\n            })\n        } else {\n            panic!(\"Problem opening the file: {:?}\", error);\n        }\n    });\n}\n</code></pre>"},{"location":"cs/rust/tutorial/error-handling/#useful-functions-on-result-enum","title":"Useful functions on Result Enum","text":"<p>We can simplify our original code of <code>match</code> expression and <code>panic</code> by calling  <code>unwrap()</code> which panics or return the result:</p> <pre><code>use std::fs::File;\n\nfn main() {\n    // unwrap does the same thing\n    // - in success case, return item stored in `Ok` varient\n    // - otherwise, in `Err` case panic\n    let f = File::open(\"hello.txt\").unwrap();\n}\n</code></pre> <p>We can use <code>expect()</code> method which let us specify the error message or panics.</p> <pre><code>use std::fs::File;\n\nfn main() {\n    let f = File::open(\"hello.txt\").expect(\"Failed to open Hello.txt\");\n}\n</code></pre>"},{"location":"cs/rust/tutorial/error-handling/#error-propagation","title":"Error Propagation","text":"<p>Often times, we want to give more control to the caller function of what to do with the error, i.e., we want to return an error instead of handling it ourselves. This is known as Error propagation.</p> <pre><code>use std::fs::File;\nuse std::io;\nuse std::io::Read;\n\n// this function returns a `Result` type returning the file content\n// or an `io::Error`\nfn read_username_from_file() -&gt; Result&lt;String, io::Error&gt; {\n    let f = File::open(\"hello.txt\");\n\n    // opening file can fail, in which case we return an error\n    let mut f = match f {\n        Ok(file) =&gt; file,\n        Err(e) =&gt; return Err(e)\n    }\n\n    // otherwise we read content in String `s` and return `s`\n    let mut s = String::new();\n    match f.read_to_string(&amp;mut s) {\n        Ok(_) =&gt; Ok(s),\n        Err(e) =&gt; Err(e)\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/error-handling/#the-operator","title":"The ? Operator","text":"<p>We can simplify this further, by adding <code>?</code> to the call of the <code>open()</code> method, which gives the unwrapped item otherwise in case of error it returns early with the error:</p> <pre><code>use std::fs::File;\nuse std::io;\nuse std::io::Read;\n\nfn read_username_from_file() -&gt; Result&lt;String, io::Error&gt; {\n    let mut f = File::open(\"hello.txt\")?;\n\n    // opening file can fail, in which case we return an error\n    // let mut f = match f {\n    //     Ok(file) =&gt; file,\n    //     Err(e) =&gt; return Err(e)\n    // }\n\n    // let mut s = String::new();\n    // match f.read_to_string(&amp;mut s) {\n    //     Ok(_) =&gt; Ok(s),\n    //     Err(e) =&gt; Err(e)\n    // }\n    let mut s = String::new();\n    f.read_to_string(&amp;mut s)?;\n    Ok(s)\n}\n</code></pre> <p>We can simplify even further by chaining method calls</p> <pre><code>use std::fs::File;\nuse std::io;\nuse std::io::Read;\n\nfn read_username_from_file() -&gt; Result&lt;String, io::Error&gt; {\n    let mut s = String::new();\n    File::open(\"hello.txt\")?.read_to_string(&amp;mut s)?;\n    Ok(s)\n}\n</code></pre> <p>Or maybe even further,</p> <pre><code>use std::fs::{self, File};\nuse std::io;\nuse std::io::Read;\n\nfn read_username_from_file() -&gt; Result&lt;String, io::Error&gt; {\n    fs::read_to_string(\"hello.txt\")\n}\n</code></pre> <p>What happens if we try to use <code>?</code> in our main function?</p> <p>Info</p> <p>The <code>main()</code> can returning Nothing, or a <code>Result</code> type.</p> <pre><code>use std::error::Err;\nuse std::fs::File;\n\nfn main() {\n    // error!\n    // error[E0277]: the `?` operator can only be used in a\n    // function that returns `Result` or `Option` (or another\n    // type that implements `FromResidual`)\n    let f = File::open(\"hello.txt\")?;\n}\n</code></pre> <p>So we can change this to return a <code>Result</code> type:</p> <pre><code>use std::error::Err;\nuse std::fs::File;\n\n// In success case, we return `()` unit\n// in error case we return a trait object (any type of error)\nfn main() -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {\n    let f = File::open(\"hello.txt\")?;\n\n    Ok(())\n}\n</code></pre> <p>When to use panic or Result enum</p> <p>In general, by default you should be using <code>Result</code> enum and Error propagation. This prevents your program from crashing while the caller function to handle the error as per requirement.</p> <p>The <code>panic!</code> should be reserved for exceptional circumstances where recovering from error is not possible.</p> <ul> <li>Another place where <code>panic!</code> is in example code where there is not context on how to deal with error.</li> </ul> <p>You can also use <code>unwrap()</code> or <code>expect()</code> in prototype phase code, after which you might want to consider error handling for all such calls.</p> <ul> <li><code>unwrap()</code> and <code>expect()</code> could also be used in test code, because we want the test to fail if we expected the code to succeed fails to do so.</li> <li>or use <code>unwrap()</code> and <code>expect()</code> when you know your call to a function will succeed.</li> </ul> <pre><code>use std::net::IpAddr;\n\nfn main() {\n    let home: IpAddr = \"127.0.0.1\".parse().unwrap();\n}\n</code></pre>"},{"location":"cs/rust/tutorial/generic-types/","title":"Generic Types in Rust","text":"<p>Generic Data Types</p>"},{"location":"cs/rust/tutorial/generic-types/#extracting-functions","title":"Extracting Functions","text":"<p>Generic lifetimes anf traits are ways to reduce code duplication.</p> <p>But, first let's talk about code duplication. We are trying to find the largest number, this works but to again find the largest number from a vector we might need to write the logic again.</p> <pre><code>fn main() {\n    let number_list = vec![34, 50, 25, 100, 65];\n\n    let mut largest = number_list[0];\n\n    for number in number_list {\n        if number &gt; largest {\n            largest = number;\n        }\n    }\n\n    println!(\"The largest number is {}\", largest);\n}\n</code></pre> <p>The obvious step to extract the logic into function and call it twice:</p> <pre><code>fn main() {\n    let number_list = vec![34, 50, 25, 100, 65];\n    let largest = get_largest(number_list);\n    println!(\"The largest number is {}\", largest);\n\n    let number_list = vec![102, 52, 6000, 89, 54, 2, 43, 8];\n    let largest = get_largest(number_list);\n    println!(\"The largest number is {}\", largest);\n}\n\nfn get_largest(number_list: Vec&lt;i32&gt;) -&gt; i32 {\n    let mut largest = number_list[0];\n\n    for number in number_list {\n        if number &gt; largest {\n            largest = number;\n        }\n    }\n    largest\n}\n</code></pre> <p>This work but the logic of <code>get_largest()</code> is tied to a concrete type <code>i32</code>, but what if wanted to get largest character in a vector.</p> <pre><code>let char_list = vec!['y', 'm', 'c', 'k'];\n// error\n// mismatched types\n// expected struct `Vec&lt;i32&gt;` found struct `Vec&lt;char&gt;`\nlet largest = get_largest(char_list);\n</code></pre> <p>One way might be duplicate the function with different definition, accepting <code>char</code> vector.</p> <pre><code>fn get_largest_char(char_list: Vec&lt;char&gt;) -&gt; char {}\n</code></pre> <p>But we can do better. We'll modify our original function to take in both sets of arguments using generic:</p> <ul> <li>Generic types are specified inside angle bracked right after function name.</li> <li>We can also have multiple generic types:     <pre><code>fn get_largest&lt;T, U, V&gt;(number_list: Vec&lt;i32&gt;) -&gt; i32 {}\n</code></pre></li> </ul> <pre><code>// We do want to specify that the generic type could be anything\n// that could be compared, which requires knowledge of Traits.\n// but let's just quickly fix it up here\nfn get_largest&lt;T: PartialOrd + Copy&gt;(number_list: Vec&lt;T&gt;) -&gt; T {\n    let mut largest = number_list[0];\n\n    for number in number_list {\n        if number &gt; largest {\n            largest = number;\n        }\n    }\n    largest\n}\n</code></pre>"},{"location":"cs/rust/tutorial/generic-types/#generics-in-struct-definitions","title":"Generics in Struct Definitions","text":"<pre><code>struct Point {\n    x: i32,\n    y: i32\n}\n\nfn main() {\n    let p1 = Point { x:5, y: 10};\n\n    // error: mismatched types\n    // what if we wanted to created a `Point` with floating numbers\n    let p2 = Point { x: 5.0, y: 10.0};\n}\n</code></pre> <p>Using Generics:</p> <pre><code>struct Point&lt;T&gt; {\n    x: T,\n    y: T\n}\n\nfn main() {\n    let p1 = Point { x:5, y: 10};\n    let p2 = Point { x: 5.0, y: 10.0};\n}\n</code></pre> <p>This will work with <code>p1</code> with integers and <code>p2</code> with floating numbers, but if we wanted to mix the two, we'd be required to add another generic type:</p> <pre><code>struct Point&lt;T, U&gt; {\n    x: T,\n    y: U\n}\n\nfn main() {\n    let p1 = Point { x:5, y: 10};      // works\n    let p2 = Point { x: 5.0, y: 10.0}; // works\n    let p3 = Point { x: 5, y: 10.0};   // works\n}\n</code></pre>"},{"location":"cs/rust/tutorial/generic-types/#generics-in-enum-definitions","title":"Generics in Enum Definitions","text":"<pre><code>fn main() {\n    enum Option&lt;T&gt; {\n        Some(T),\n        None\n    }\n\n    enum Result&lt;T, E&gt; {\n        Ok(T),\n        Err(E)\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/generic-types/#generics-in-method-definitions","title":"Generics in Method Definitions","text":"<pre><code>struct Point&lt;T&gt; {\n    x: T,\n    y: T\n}\n\n// the generic `T` here is not tied\n// to `T` specified up here for struct `Point`\n// available to all `Point` instances\nimpl&lt;T&gt; Point&lt;T&gt; {\n    fn x(&amp;self) -&gt; &amp;T {\n        &amp;self.x\n    }\n}\n\n// this impl block is only for f64\n// only available to those `Point` instance where\n// x and y are both f64\nimpl Point&lt;f64&gt; {\n    fn y(&amp;self) -&gt; f64 {\n        self.y\n    }\n}\nfn main() {\n    let p = Point { x: 5, y: 10 };\n    p.x();  // available\n    p.y();  // not available\n\n    let p1 = Point { x: 5.0, y: 1.0};\n    p.x();  // available\n    p.y();  // available\n}\n</code></pre> <p>Let's go complex, introduce two generic types, <code>T</code> and <code>U</code>.</p> <pre><code>struct Point&lt;T, U&gt; {\n    x: T,\n    y: U\n}\n\nimpl&lt;T, U&gt; Point&lt;T, U&gt; {\n    // notice, mixup has it's own generic types, `V` and `W` scoped to this function\n    // Why? Because we want `other` to potentially have different types\n    // then the `Point` we're calling the function on\n    // And we return a `Point` of mixuped type `T` from `self` and `W` from passed in `other`.\n    fn mixup&lt;V, W&gt;(self, other: Point&lt;V, W&gt;) -&gt; Point&lt;T, W&gt; {\n        Point {\n            x: self.x,\n            y: other.y\n        }\n    }\n}\n\nfn main() {\n    let p1 = Point { x: 5, y: 10.4 };\n    let p2 = Point { x: \"Hello\", y: 'c'};\n\n    let p3 = p1.mixup(p2);\n    println!(\"p3.x = {}, p3.y = {}\", p3.x, p3.y);\n}\n</code></pre> <p>Running this program outputs:</p> <pre><code>p3.x = 5, p3.y = c\n</code></pre>"},{"location":"cs/rust/tutorial/generic-types/#performance-impact","title":"Performance Impact","text":"<p>We don't need to define two versions of the <code>Option</code> enum using generic, and this also doesn't cost us in performance because at compile time, Rust will convert the <code>Option</code> enum into two option enums:</p> <pre><code>enum Option&lt;T&gt; {\n    Some(T),\n    None,\n}\n\nfn main() {\n    let integer = Option::Some(5);\n    let float = Option::Some(5.0);\n}\n</code></pre> <pre><code>enum Option_i32 {\n    Some(i32),\n    None,\n}\n\nenum Option_f64 {\n    Some(f64),\n    None,\n}\n\nfn main() {\n    let integer = Option_i32::Some(5);\n    let float = Option_f64::Some(5.0);\n}\n</code></pre>"},{"location":"cs/rust/tutorial/getting-started/","title":"Getting Started with Rust","text":"<p>This tutorial series is minified version of The Rust Book: The Rust Programming Language</p> <p>and follow up of this tutorial series:</p> <p>So, if you like watching video tutorial, do check out the above series by Bogdan Pshonyak and follow up with the notes here.</p> <p>If you do give this quick read and test yourself while diving deep, then checkout this :</p> <p>The Rust Programming Language</p>"},{"location":"cs/rust/tutorial/getting-started/#installing-rust-on-linux-or-macos","title":"Installing Rust on Linux or macOS","text":"<pre><code>curl --proto '=https' --tlsv1.3 https://sh.rustup.rs -sSf | sh\n</code></pre> <p>Check Rust version with: <code>rustc --version</code></p> <p>Then go ahead and install official Plugin/extensions for Rust and Rust-analyzer for your faviourite IDE.</p>"},{"location":"cs/rust/tutorial/getting-started/#hello-world-in-rust","title":"Hello, World in Rust","text":"<pre><code>mkdir ~/projects\ncd ~/projects\nmkdir hello_world\ntouch main.rs\ncode .\n</code></pre> <p>And then write in <code>main.rs</code> <pre><code>fn main() {\n    println!(\"Hello World\");\n}\n</code></pre></p> <p>Let's compile this and run this: <pre><code>rustc main.rs\n./main.rs\n</code></pre></p> <p>In real world projects we're going to work with multiple files with dependencies and therefore we need a build system and package manager: Cargo.</p> <p>Check it's version as: <code>cargo --version</code></p> <p>Let's create a new package named <code>hello_cargo</code>:</p> <p>Note</p> <p>In Rust the package names are supposed to be in all smallcase.</p> <p><pre><code>cargo new hello_cargo\n</code></pre> which build a directory structure something like this: <pre><code>.\n\u251c\u2500\u2500 Cargo.toml\n\u251c\u2500\u2500 .gitignore\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 main.rs\n</code></pre></p> <p>The <code>Cargo.toml</code> contains information about the package, and list dependencies along with their aliases.</p> <pre><code>[package]\nname = \"hello_cargo\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html\n\n[dependencies]\n</code></pre> <p>To build using Cargo, let's cd into our newly created directory: <code>cd hello_cargo</code> and then: <pre><code>cargo build\ncargo run\n</code></pre> This creates new <code>cargo.lock</code> (don't temper with this) <code>target</code> dir which contains bunch of other stuff and main executable file.</p> <p>Run <code>cargo --help</code> to see more bunch of options.</p> <p>Note</p> <p>You can use <code>cargo check</code> to check program for errors without actually building an executable file, much faster.</p> <p>The default tools on the rustup toolchain are:</p> <ol> <li>Cargo: package manager and crate host for Rust</li> <li>Rustup: the rust toolchain installer/updater/release switcher</li> <li>Rustc: the rust compiler</li> </ol> <p>To update Rust, simply run:</p> <pre><code>rustup update\n</code></pre> <p>and to uninstall: <pre><code>rustup self uninstall\n</code></pre></p>"},{"location":"cs/rust/tutorial/guessing-game/","title":"Guessing game","text":"<p>Programming a guessing game in Rust</p> <p>Let's create a new project by using <code>Cargo</code> and open the dir in editor: Guessing Game</p> <pre><code>cargo new guessing-game\n</code></pre> <p>And edit <code>src/main.rs</code>:</p> <p><pre><code>fn main() {\n    println!(\"Guess the number!\");\n    println!(\"Please input your guess.\");\n    let guess = String::new();\n}\n</code></pre> <code>String is a UTF-8 encoded, growable string from standard library.</code>new()<code>is an associative function on</code>String` which returns an empty string.</p> <p>You IDE would probably infer the type of <code>guess</code> and annotate it with String. You can annotate your variable explicitly like this:</p> <pre><code>let guess: String = String::new();\n</code></pre> <p>One thing to note here is that unlike C++/C, in Rust variables are immutable by default. So our <code>guess</code> variable still ain't ready to mutate and store any input and change value from default string. Let's change that in the original code:</p> <pre><code>fn main() {\n    println!(\"Guess the number!\");\n    println!(\"Please input your guess.\");\n\n    // a mutable `guess`\n    let mut guess = String::new();\n}\n</code></pre>"},{"location":"cs/rust/tutorial/guessing-game/#io","title":"I/O","text":"<p>Standard I/O operations can be supported using by io module in <code>std</code> (standard) library. Let's use it by adding it to top of our code:</p> <p><pre><code>use std::io;\n\nfn main() { ...\n</code></pre> We can now create a handle to the standard input of the current process using <code>stdin</code> function. <code>read_line()</code> will take the input and store in passed <code>buf</code> variable which is supposed to be a mutable reference to String (in our case <code>guess</code>). This allows us to modify it without changing ownership of string. We'll talk about this in depth later:</p> <pre><code>    let mut guess = String::new();\n\n    io::stdin()\n        .readline(&amp;mut guess)\n}\n</code></pre> <p><code>readline()</code> returns an Enum <code>std::result::Result</code> type:</p> <pre><code>pub enum Result&lt;T, E&gt; {\n    Ok(T),\n    Err(E),\n}\n</code></pre> <p><code>Result</code> is a type that represent either variantsuccess (<code>Ok</code>) or variant failure (<code>Err</code>).</p> <p>So to handle the error, we can use the <code>expect()</code> to indicate any error msg otherwise print out the guess variable:</p> <pre><code>    io::stdin()\n        .read_line(&amp;mut guess)\n        .expect(\"Failed to read line\");\n\n    println!(\"You guessed: {}\", guess);\n</code></pre>"},{"location":"cs/rust/tutorial/guessing-game/#random-number-generation","title":"Random Number Generation","text":"<p>To generate random numbers we'll need to add a depedency to our <code>Cargo.toml</code> file since <code>std</code> library don't ship with one:</p> <p><pre><code>[dependencies]\nrand = \"0.5.5\"\n</code></pre> After that run <code>cargo build</code> command, which will compile and bring any new dependecies as described by <code>Cargo.toml</code> and also other libs that the new dependency depends on.</p> <p>Let \"import\" this dependency as we learned previously, or in Rust terms, bring into scope of current file. We'll be bringing <code>Rng</code> trait from <code>rand</code> library which defines methods that random number generators.</p> <pre><code>use std::io;\nuse rand::Rng;\n\nfn main() { ...\n</code></pre> <p>Let's create a new variable to store our randomly generated number and check it's value by running our program using command <code>cargo run</code>:</p> <pre><code>fn main() {\n    println!(\"Guess the number\");\n\n    // gen_range generates a number between 1 and 100\n    let secret_number = rand::thread_rng().gen_range(1..101);\n    println!(\"The secret number is: {}\", secret_number);\n    ...\n}\n</code></pre> <p>If we wanted to create inclusive range we could have done something like this:</p> <pre><code>let secret_number = rand::thread_rng().gen_range(1..=100);\n</code></pre>"},{"location":"cs/rust/tutorial/guessing-game/#compare-the-guess-and-secret_number","title":"Compare the guess and secret_number","text":"<p>To do that let's bring <code>Ordering</code> into scope. <code>Ordering</code> is an enum that is a result of two things being compared, viz: <code>Less</code>, <code>Equal</code>, <code>Greater</code>:</p> <pre><code>use std::io;\nuse std::cmp::Ordering;\n...\n</code></pre> <p>Continuing at the bottom, we'll use <code>cmp()</code> at match all possible variants of <code>Ordering</code> enum using <code>match</code> keyword:</p> <pre><code>    println!(\"You guessed: {}\", guess);\n\n    match guess.cmp(&amp;secret_number) {\n        Ordering::Less =&gt; println!(\"Too small!\"),\n        Ordering::Greater =&gt; println!(\"Too bug!\"),\n        Ordering::Equal =&gt; println!(\"You win!\"),\n    }\n}\n</code></pre> <p>But, this will give an error since <code>guess</code> is of type <code>String</code> whereas <code>secret_number</code> is an <code>int</code>. So we'll need to convert our <code>guess</code> to <code>int</code>.</p> <pre><code>    // perform this just after read_line()\n    // shadowing `guess` of type String\n    let guess: u32 = guess.trim().parse()\n        .expect(\"Please type a number\");\n</code></pre>"},{"location":"cs/rust/tutorial/guessing-game/#game-loop","title":"Game Loop","text":"<p>Let's add a loop, so that user can keep guessing until they get the right number.</p> <pre><code>fn main() {\n    loop {\n        println!(\"Guess the number!\");\n        // gen_range generates a number between 1 and 100\n        let secret_number = rand::thread_rng().gen_range(1..101);\n        println!(\"The secret number is: {}\", secret_number);\n\n        println!(\"Please input your guess.\");\n\n        // a mutable `guess`\n        let mut guess = String::new();\n\n        io::stdin()\n            .read_line(&amp;mut guess)\n            .expect(\"Failed to read line\");\n        let guess: u32 = guess.trim().parse()\n            .expect(\"Please type a number\");\n\n        println!(\"You guessed: {}\", guess);\n\n        match guess.cmp(&amp;secret_number) {\n            Ordering::Less =&gt; println!(\"Too small!\"),\n            Ordering::Greater =&gt; println!(\"Too bug!\"),\n            // exit loop if found the match\n            Ordering::Equal =&gt; {\n                println!(\"You win!\");\n                brak;\n            },\n        }\n    }\n}\n</code></pre> <p>Ok, that's almost done!</p>"},{"location":"cs/rust/tutorial/guessing-game/#handling-invalid-inputs","title":"Handling invalid inputs","text":"<p>Currently, our program panics if we user gives String as input. We want user to keep prompting to input a number instead of panicking.</p> <p>We can modify our <code>parse()</code> when parsing <code>guess</code> from <code>String</code> to <code>u32</code> as it returns a <code>Result</code> enum which can be either <code>Ok</code> or <code>Err</code> with <code>_</code> to catch any err value and continue no matter what.</p> <pre><code>    let guess: u32 = match guess.trim().parse() {\n        Ok(num) =&gt; num,\n        Err(_) =&gt; continue,\n    }\n</code></pre>"},{"location":"cs/rust/tutorial/lifetimes/","title":"Rust Lifetimes","text":"<p>Validating References with Lifetimes</p>"},{"location":"cs/rust/tutorial/lifetimes/#the-borrow-checker","title":"The Borrow Checker","text":"<p>Dangling References</p> <p>A reference that points to invalid data.</p> <pre><code>fn main() {\n    let r;\n    {\n        let x = 5;\n        r = &amp;x;  // `x` does not live long enough\n    }\n    // `r` has become a dangling reference\n    // since `r` goes out of scope in inner scope; get's dropped\n    println!(\"r: {}\", r);\n}\n</code></pre> <p>Rust is able to identify this above issue at compile time using an inbuilt feature of the compiler calld Borrow checker. It validates all borrowed values or references are valid.</p> <p>Here we annotate the lifetimes of the variables in the above code.</p> <pre><code>fn main() {\n    let r;                     // ----------+-- 'a\n    {                          //           |\n        let x = 5;             // -+---- 'b |\n        r = &amp;x;                //  |        |\n    }                          // -+        |\n    println!(\"r: {}\", r);      // ----------+\n}\n</code></pre> <p>Lifetime of a variable referes to how long the variable lives.</p> <p><code>r</code> lifetime is annotated by <code>'a</code> and lives till the end of main function. Then inside inner scope, <code>x</code> lifetime is annotated by <code>'b</code> and it lives only till the end of the scope. Now with this information, borrow checker can invalidate reference <code>r</code> since <code>x</code> lives long enough only inside the inner scope.</p> <p><pre><code>fn main() {\n    let x = 5;             // ----------+-- 'b\n    let r = &amp;x;            // --+-- 'a  |\n                           //   |       |\n    println!(\"r: {}\", r);  // ----------+\n}\n</code></pre> This programs runs completely fine, no compile time error, because <code>x</code> and <code>r</code> both lives long enough till the end of main function.</p>"},{"location":"cs/rust/tutorial/lifetimes/#generic-lifetime-annotations","title":"Generic Lifetime Annotations","text":"<p>There do arrive situations where we do need to help the compiler by specifying the lifetimes of variables.</p> <p>Let's look at by examples:</p> <pre><code>fn main() {\n    let string1 = String::from(\"abcd\");\n    let string2 = String::from(\"xyz\");\n\n    let result = longest_str(string1.as_str(), string2.as_str());\n    println!(\"The longest string is {}\", result);\n}\n\nfn longest_str(x: &amp;str, y: &amp;str) -&gt; &amp;str { // error: missing lifetime specifier\n    if x.len() &gt; y.len() {\n        x\n    } else {\n        y\n    }\n}\n</code></pre> <p>Here the <code>longest_str()</code> function returns a reference to string slice.</p> <p>From the point of view of borrow checker, how would it know that <code>result</code> when printed in <code>main()</code> is not a dangling pointer?</p> <p>The lifetime of the returned reference is determined by the if statement.</p> <ul> <li>Returning either <code>x</code> or <code>y</code> can have different lifetime.</li> <li>We also don't know the exact lifetime of <code>x</code> and <code>y</code>. <code>x</code> and <code>y</code> are just parameters to any variable existing in any part of the code</li> </ul> <p>To fix this, we need to use generic lifetime annotation.</p> <p>Generic lifetime annotations (or simply called lifetimes) describe the relationship between the lifetimes of multiple references and how they relate to each other. They don't change the lifetime.</p> <ul> <li>Generic lifetime annotations or lifetimes always start with an apostrophe/tick.</li> <li>Lifetimes of arguments being passed in are called input lifetimes.</li> <li>Lifetimes of returned values are called output lifetimes.</li> </ul> <p>Here is how you specify lifetimes: <pre><code>// &amp;i32        // a reference\n// &amp;'a i32     // a reference with an explicit lifetime\n// &amp;'a mut i32 // a mutable reference with an explicit lifetime\n</code></pre></p> <pre><code>// the lifetime of the returned reference will be the same\n// as the smallest lifetime of the arguments\nfn longest_str&lt;'a&gt;(x: &amp;'a str, y: &amp;'a str) -&gt; &amp;'a str {\n    if x.len() &gt; y.len() {\n        x\n    } else {\n        y\n    }\n}\n</code></pre> <p>So, if <code>x</code> has smaller lifetime than <code>y</code> than lifetime of returned reference will be same as <code>x</code> and vice versa if <code>y</code> has smaller lifetime.</p> <p>Now we told the borrow checker, whatever gets returned by <code>longest()</code> and saved in <code>result</code> will have the lifetime equal to the smallest lifetime of argument being passed in.</p> <p>The borrow checker can now check if the smallest lifetime is still valid when we are trying to print.</p> <p>Let's look at an example where the lifetimes is different with an updated <code>main()</code> function.</p> <pre><code>fn main() {\n    let string1 = String::from(\"abcd\"); // string1 lifetime will be till end of main\n\n    {\n        let string2 = String::from(\"xyz\");\n        let result = longest_str(string1.as_str(), string2.as_str());\n        println!(\"The longest string is {}\", result);\n    }\n}\n</code></pre> <p>Here, <code>result</code> lifetime will be same as <code>string2</code> since it has smallest lifetime being passed in <code>longest_str()</code> function which lasts till the inner scope.</p> <p><code>result</code> is still when <code>println!</code> gets called since <code>string2</code> is still valid.</p> <p>But when:</p> <pre><code>fn main() {\n    let string1 = String::from(\"abcd\"); // string1 lifetime will be till end of main\n\n    {\n        let string2 = String::from(\"xyz\");\n        let result = longest_str(string1.as_str(), string2.as_str());\n        //                                   error:^^^^^^^ `string2` doesn't live long enough\n    }\n    println!(\"The longest string is {}\", result);\n}\n</code></pre> <p>This gives us error because when we try to print out <code>result</code> with smaller lifetime equal to <code>string2</code>, it doesn't live long enough and is invalidates after the inner scope.</p>"},{"location":"cs/rust/tutorial/lifetimes/#thinking-in-terms-of-lifetimes","title":"Thinking in terms of lifetimes","text":"<p>We can define different lifetime depending on what the function is returning.</p> <p>Say we want the <code>longest_str()</code> function to always return <code>x</code>. In which case we don't need to annotate the lifetime for <code>y</code>. Our code is now valid!</p> <p>Why? Because we know <code>longest_str()</code> will return the reference whos lifetime is smallest and same as first parameter. And the first parameter <code>string1</code> lives till the end of <code>main()</code> before which we're calling <code>println!</code> macro.</p> <pre><code>fn main() {\n    let string1 = String::from(\"abcd\");\n\n    {\n        let string2 = String::from(\"xyz\");\n        let result = longest_str(string1.as_str(), string2.as_str());\n    }\n    println!(\"The longest string is {}\", result);\n}\n\nfn longest_str&lt;'a&gt;(x: &amp;'a str, y: &amp;str) -&gt; &amp;'a str {\n    x\n}\n</code></pre> <p>The lifetime of our return value always has to be tied to the lifetime of one our paramters.</p> <p>This is because if we return a reference from a function it has to be a reference that is something passed in. We can't return a reference to something that is created inside the function, which will be dropped once the function goes out of scope.</p> <p>So, this doesn't work: <pre><code>fn longest_str&lt;'a&gt;(x: &amp;str, y: &amp;str) -&gt; &amp;'a str {\n    let result = String::from(\"long enough\");\n    result.as_str()\n}\n</code></pre></p> <p>But this is fine, as it retuns a owned type, which transfers the ownership to caller: <pre><code>fn longest_str&lt;'a&gt;(x: &amp;str, y: &amp;str) -&gt; String {\n    let result = String::from(\"long enough\");\n    result\n}\n</code></pre></p>"},{"location":"cs/rust/tutorial/lifetimes/#lifetime-annotatinos-in-struct-definitions","title":"Lifetime annotatinos in struct definitions","text":"<p>Usually we use owned data type in structs, but if we want to use a reference then we need to specify lifetime annotations.</p> <pre><code>struct ImportantExcerpt&lt;'a&gt; {\n    part: &amp;'a str,\n // struct cannot outlive the reference passed into `part`\n}\n\nfn main() {\n    let novel = String::from(\"Call me Ishmael. Some years ago...\");\n    let first_sentence = novel.split('.').next().expect(\"Could not find\");\n    let i = ImportantExcept {\n        part: first_sentence,\n    };\n\n    // we can't use `i` once `first_sentence` goes out of scope.\n}\n</code></pre>"},{"location":"cs/rust/tutorial/lifetimes/#lifetime-elision","title":"Lifetime Elision","text":"<p>In this code, which we previously wrote, didn't had lifetime annotation. Remember right? It worked whether or not lifetimes were annotated.</p> <pre><code>fn main() {}\n\n// Using Lifetime elision rule 2\nfn first_word&lt;'a&gt;(s: &amp;'a str) -&gt; &amp;'a str {\n    let bytes = s.as_bytes();\n\n    for (i, &amp;item) in bytes.iter().enumerate() {\n        if item == b' ' {\n            return &amp;s[0..i];\n        }\n    }\n}\n</code></pre> <p>So there are scenarios where compiler can deterministically infer the lifetime by checking the three lifetime elision rules:</p> <p>Lifetime Elision rules</p> <ol> <li>Each parameter that is a reference gets its own lifetime parameter.</li> <li>If there is exactly one input lifetime parameter, that lifetime is assigned to all output lifetime parameters.</li> <li>If there are multiple input lifetime parameters, but one of them is <code>&amp;self</code> or <code>&amp;mut self</code> the lifetime of self is assigned to all output lifetime parameters.</li> </ol>"},{"location":"cs/rust/tutorial/lifetimes/#lifetime-annotations-in-method-definitions","title":"Lifetime Annotations in method definitions","text":"<p>We don't need to specify lifetimes for method <code>return_part()</code>.</p> <pre><code>struct ImportantExcerpt&lt;'a&gt; {\n    part: &amp;'a str,\n}\n\nimpl&lt;'a&gt; ImportantExcerpt&lt;'a&gt; {\n    fn return_part(&amp;self, announcement: &amp;str) -&gt; &amp;str {\n        println!(\"Attension please: {}\", announcement);\n        self.part\n    }\n}\n\nfn main() {\n    let novel = String::from(\"Call me Ishmael. Some years ago...\");\n    let first_sentence = novel.split('.').next().expect(\"Could not find\");\n    let i = ImportantExcept {\n        part: first_sentence,\n    };\n}\n</code></pre> <p>Why?</p> <ul> <li>According to rule, each reference gets a lifetime. <code>&amp;self</code> will get a lifetime, say <code>'a</code> and <code>announcement</code> will also get one, say <code>'b</code>.</li> <li>According to second rule, the output lifetime gets assignment from exactly one input lifetime, if we have only one input parameter. Here we have two, so rule 2 doesn't apply.</li> <li>According to third rule, if one of the parameters to a function is <code>&amp;self</code> or <code>&amp;mut self</code> then all output lifetimes will be same as `self. Yup! This apply here.</li> </ul>"},{"location":"cs/rust/tutorial/lifetimes/#static-lifetimes","title":"Static Lifetimes","text":"<p>The reference could live as long as the duration of the program.</p> <ul> <li>All string literals have static lifetimes, since they are stored in program's binary.</li> </ul>"},{"location":"cs/rust/tutorial/module-system/","title":"Rust's Module System","text":"<p>We'll learn to manage growing projects using package, crates and modules. There is a requirement for organization and encapsulation of code as it grows in size.</p> <p>Rust has a module system, starting with - package: <code>cargo new &lt;name&gt;</code> creates a new package, and package stores crates. - crates: crates could either be binary crate, something which is executable or a library crate which is code that can be used by other programs. Crates contains modules. - modules: modules allows us to organize a chunk of code and control the privacy rules.</p> <p>If we wanted to create a module like Authentication, the internal function could remain private but expose one login method.</p> <p>Rust also has workspaces meant for very large projects and allow us to store interrelated packages inside the workspace.</p> <p>Managing Growing Projects with Packages, Crates, and Modules</p>"},{"location":"cs/rust/tutorial/module-system/#packages-and-crates","title":"Packages and Crates","text":"<p>Let's start with creating a new package.</p> <pre><code>cargo new my-project\n</code></pre> <p>In the auto-generated <code>Cargo.toml</code> we can see that there are no crates defined, which doesn't mean our package dosn't have any crate. <pre><code>[package]\nname = \"hello_cargo\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html\n\n[dependencies]\n</code></pre></p> <p>Well, we actually do have one binary crate, for the <code>main.rs</code>.  Rust follows the convention that if you have <code>main.rs</code> then a binary crate with the same name as your package will be automatically created and <code>main.rs</code> will be the crate root, which also makes the root module.</p> <p>The crate root is the source file that the rust compiler starts at when building your crate.</p> <p>There is also a similar convention for library crate, for a file that might exist in <code>src</code> directory. Say, something like <code>lib.rs</code>.</p> <p><pre><code>.\n\u251c\u2500\u2500 Cargo.toml\n\u251c\u2500\u2500 .gitignore\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 lib.rs\n    \u2514\u2500\u2500 main.rs\n</code></pre> If <code>lib.rs</code> is defined in the root of our <code>src</code> directory, then rust will automatically create a library crate with the same name as your package, and <code>lib.rs</code> will be the crate root.</p> <p>Which means our <code>my-project</code> has two crates, one binary and other a library crate.</p> <p>Crate Rules</p> <ol> <li>A package must have at least one crate.</li> <li>A package could have zero library crate or one library crate.</li> <li>A package can have any number of binary crates.</li> </ol> <p>So any other file like <code>another_file.rs</code> will represent another binary crate.</p> <pre><code>.\n\u251c\u2500\u2500 Cargo.toml\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 bin\n|   \u2514\u2500\u2500 another_file.rs\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 lib.rs\n    \u2514\u2500\u2500 main.rs\n</code></pre>"},{"location":"cs/rust/tutorial/module-system/#defining-modules","title":"Defining Modules","text":"<p>Let's start by creating a new package <code>restaurant</code> that contain a library crate.</p> <pre><code>cargo new --lib restaurant\n</code></pre> <pre><code>.\n\u251c\u2500\u2500 Cargo.toml\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 lib.rs\n</code></pre> <p>In which <code>lib.rs</code> we automatically get a test module something like this, which we don't need right now, so delete it.</p> <pre><code>pub fn add(left: usize, right: usize) -&gt; usize {\n    left + right\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn it_works() {\n        let result = add(2, 2);\n        assert_eq!(result, 4);\n    }\n}\n</code></pre> <p>Our goal with new package is to create a library to help run a restaurant. Think about restaurant as two parts, front of house which serves customers, back of house where food is made.</p> <pre><code>// lib.rs\nmod front_of_house {\n    mod hosting {\n        fn add_to_waitlist() {}\n        fn seat_at_table() {}\n    }\n\n    mod serving {\n        fn take_order() {}\n        fn serve_order() {}\n        fn take_payment() {}\n    }\n}\n</code></pre> <p>We start with a module defined using <code>mod</code> keyword called <code>front_of_house</code>. Inside which we have two more module called <code>hosting</code> and <code>serving</code>. Modules can contain other modules, enums, structs, constants, traits and so on inside of them.</p> <p>So our module tree looks something like this:</p> <pre><code>crate\n \u2514\u2500\u2500 front_of_house\n     \u251c\u2500\u2500 hosting\n     \u2502   \u251c\u2500\u2500 add_to_waitlist\n     \u2502   \u2514\u2500\u2500 seat_at_table\n     \u2514\u2500\u2500 serving\n         \u251c\u2500\u2500 take_order\n         \u251c\u2500\u2500 serve_order\n         \u2514\u2500\u2500 take_payment\n</code></pre> <p>At top we have a module called <code>crate</code> that gets created by default for our crate root which is <code>lib.rs</code>.</p>"},{"location":"cs/rust/tutorial/module-system/#paths","title":"Paths","text":"<p>A good analogy for module tree is to thinking about them like the folder/directory tree on computer. If we wanted to reference a file inside a directory, in the same way we'd want to reference a item in a module, we'd be required to specify a path to that function.</p> <p>Check the code below, a simplied <code>front_of_house</code> module inside of which we have <code>hosting</code> module which declares <code>add_to_waitlist()</code> function.</p> <p>We want to call this function in <code>eat_at_restaurant()</code>, we need to specify the path (specified using identifier separated by double colon) which could either be:</p> <ul> <li>Absolute path</li> <li>Relative path: start from current module</li> </ul> <pre><code>mod front_of_house {\n    mod hosting {\n        fn add_to_waitlist() {}\n    }\n}\n\npub fn eat_at_restaurant() {\n    // Absolute path\n    crate::front_of_house::hosting::add_to_waitlist();\n\n    // Relative path\n    front_of_house::hosting::add_to_waitlist();\n}\n</code></pre>"},{"location":"cs/rust/tutorial/module-system/#module-privacy-rules","title":"Module Privacy Rules","text":"<p>Our code above will show error for <code>hosting</code> because <code>mod hosting</code> is private.</p> <p>Hiding implementation details</p> <p>This is because in Rust by default a child and module and everything inside of it is private from the perspective of the parent module.</p> <p>Child modules can see anything that's defined in their parent module.</p> <p>If we want to expose any entity inside our module for public access, include <code>pub</code> keyword in front of it.</p> <pre><code>mod front_of_house {\n    pub mod hosting {\n        // this function will be private from `hosting`'s perspective\n        pub fn add_to_waitlist() {}\n    }\n}\n</code></pre> <p>Let's look at another example with relative paths using <code>super</code> keyword.</p> <pre><code>// lib.rs\nfn server_order() {}\n\nmod back_of_house {\n    fn fix_incorrect_order() {\n        cook_order(); // can call because defined in same module\n        super::serve_order(); // super to reference parent module, i.e., crate\n    }\n\n    fn cook_order() {}\n}\n</code></pre> <p>Privacy rules with Structs.</p> <p>In this example we have module called <code>back_of_house</code> storing a Struct <code>Breakfast</code> and an <code>impl</code> block implementing as associated function <code>summer()</code>.</p> <pre><code>mod back_of_house {\n    struct Breakfast {\n        toast: String,\n        seasonal_fruit: String,\n    }\n\n    impl Breakfast {\n        fn summer(toast: &amp;str) -&gt; Breakfast {\n            Breakfast {\n                toast: String::from(toast),\n                seasonal_fruit: String::from(\"peaches\"),\n            }\n        }\n    }\n}\n\npub fn eat_at_restaurant() {\n    let mut meal = back_of_house::Breakfast::summer(\"wheat\");\n}\n</code></pre> <p>But this fails, because <code>Breakfast</code> as well as <code>summer()</code> associated function are both private by default.</p> <p>To fix, once again add <code>pub</code> keyword before each one.</p> <pre><code>mod back_of_house {\n    pub struct Breakfast {\n        toast: String,\n        seasonal_fruit: String,\n    }\n\n    impl Breakfast {\n        pub fn summer(toast: &amp;str) -&gt; Breakfast {\n            Breakfast {\n                toast: String::from(toast),\n                seasonal_fruit: String::from(\"peaches\"),\n            }\n        }\n    }\n}\n\npub fn eat_at_restaurant() {\n    let mut meal = back_of_house::Breakfast::summer(\"wheat\");\n}\n</code></pre> <p>If we want to change the toast of <code>meal</code> of type <code>Breakfast</code> we'll get an error, because field itself are also private by default.</p> <pre><code>pub fn eat_at_restaurant() {\n    let mut meal = back_of_house::Breakfast::summer(\"wheat\");\n    meal.toast = String::from(\"Rye\"); // error!\n}\n</code></pre> <p>Again add <code>pub</code> keyword infront of <code>toast</code> field.</p> <pre><code>pub struct Breakfast {\n    pub toast: String,\n    ...\n</code></pre> <p>We can't build <code>Breakfast</code> struct directly because it contains a private field, which is inaccessible.</p> <p>The same privacy rules also applies to Enums.</p>"},{"location":"cs/rust/tutorial/module-system/#the-use-keyword","title":"The Use Keyword","text":"<p>Here specifying the full path of a function (as in below example calling <code>add_to_waitlist()</code> called 3 three times) isn't pretty or ideal.</p> <pre><code>mod front_of_house {\n    pub mod hosting {\n        pub fn add_to_waitlist() {}\n    }\n}\n\npub fn eat_at_restaurant() {\n    front_of_house::hosting::add_to_waitlist();\n    front_of_house::hosting::add_to_waitlist();\n    front_of_house::hosting::add_to_watilist();\n}\n</code></pre> <p>To deal with this problem, Rust provides us the <code>use</code> keyword.</p> <p><code>use</code> keyword allow us to bring a path into a scope.</p> <p>Let's bring `hosting module into the scope:</p> <pre><code>use crate::front_of_house::hosting;\n\n// or use relative path; use only `self` to reference current module.\nuse self::front_of_house::hosting;\n\npub fn eat_at_restaurant() {\n    hosting::add_to_waitlist();\n    hosting::add_to_waitlist();\n    hosting::add_to_waitlist();\n}\n</code></pre> <p>Idiomatic use of paths</p> <ol> <li>It's idiomatic to bring the function's parent module into scope, since that would allow us to minimize the path we've to specify but we're also making it the function used isn't local.</li> <li> <p>If we're bringing Enums, Structs or other items into scope, it's idiomatic to specify the full path when using <code>use</code> keyword.</p> <p>An exception is that if you are bringing two items from different modules having same name, then bring parent module into scope so that names don't conflict.</p> </li> </ol> <p>As an example in the below example both function return a <code>Result</code> type defined in two different modules, without conflicting:</p> <pre><code>use std::fmt;\nuse std::io;\n\nfn function1() -&gt; fmt::Result {}\nfn function2() -&gt; io::Result&lt;()&gt; {}\n</code></pre> <p>Another way could be to rename one of the Result type when bringing it into scope.</p> <pre><code>use std::fmt::Result;\nuse std::io::Result as IoResult;\n\nfn function1() -&gt; Result {}\nfn function2() -&gt; IoResult&lt;()&gt; {}\n</code></pre> <p>Back to Restaurant example, let's talk about re-exporting.</p> <p>What if we wanted to make the function <code>add_to_waitlist()</code> available to any external code.</p> <pre><code>mod front_of_house {\n    pub mod hosting {\n        pub fn add_to_waitlist() {}\n    }\n}\nuse crate::front_of_house::hosting;\n\n// currently only `eat_at_restaurant()` is available outside of this file because of `pub` keyword\npub fn eat_at_restaurant() {\n    hosting::add_to_waitlist();\n    hosting::add_to_waitlist();\n    hosting::add_to_waitlist();\n}\n</code></pre> <p>To re-export the <code>hosting</code> module, add <code>pub</code> keyword infront of <code>use</code> statement. This allows external code to reference the <code>hosting</code> module as well as use it at the same time.</p> <pre><code>pub use crate::front_of_house::hosting;\n</code></pre> <p><code>use</code> keyword also allows us to bring in items from external dependencies as defined in <code>Cargo.toml</code> into a scope.</p> <p>For example, let's add a <code>rand</code> dependencies to <code>Cargo.toml</code>.</p> <pre><code># See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html\n\n[dependencies]\nrand = \"0.5.5\"\n</code></pre> <p>Then we can bring <code>Rng</code> trait into scope using <code>use</code> keyword in <code>lib.rs</code></p> <pre><code>use rand::Rng;\n\nmod front_of_house {\n    // ...\n}\n\npub fn eat_at_restaurant() {\n    let secret_number = rand::thread_rng().gen_range(1..101);\n    ...\n}\n</code></pre> <p>And we can bring multiple items using Nested paths: <pre><code>use rand::{Rng, CryptoRng, ErrorKind::Transient};\n</code></pre></p> <p>Instead of bringing <code>io</code> and <code>Write</code> into scope both in <code>io</code> module like this: <pre><code>use std::io;\nuse std::io::Write;\n</code></pre></p> <p>we can write it as: <pre><code>use std::io::{self, Write};\n</code></pre></p>"},{"location":"cs/rust/tutorial/module-system/#the-glob-operator","title":"The Glob Operator","text":"<p>Bring all the public items underneath a module into scope.</p> <p>Bringing all public items from <code>io</code> into scope. <pre><code>use std::io::*;\n</code></pre></p>"},{"location":"cs/rust/tutorial/module-system/#modules-in-separate-files","title":"Modules in Separate Files","text":"<p>As programs grow module get's large in size, in that we would want to move module definition into another file.</p> <p>Let's move <code>front_of_house</code> to new file:</p> <pre><code>// src/front_of_house.rs\npub mod hosting {\n    pub fn add_to_waitlist() {}\n}\n</code></pre> <p>and in <code>lib.rs</code> we'll write something like below which tells Rust to  define module <code>front_of_house</code> here but get contents from a different file/folder named same as <code>front_of_house</code>:</p> <pre><code>// src/lib.rs\n// or src/main.rs\nmod front_of_house;\n</code></pre> <p>Let's extract the definition of <code>hosting</code> module into separate file, making the whole module a directory, which can consist of different files.</p> <pre><code>// src/front_of_house/hosting.rs\npub fn add_to_waitlist() {}\n</code></pre> <p>You can either delete <code>src/front_of_house.rs</code> and add <code>src/front_of_house/mod.rs</code> with following content.</p> <pre><code>// src/front_of_house/mod.rs\npub mod hosting;\n</code></pre> <p>Or keep <code>src/front_of_house.rs</code> and change it's content to:</p> <pre><code>// src/front_of_house.rs\npub mod hosting;\n</code></pre>"},{"location":"cs/rust/tutorial/ownership/","title":"Ownership in Rust","text":"<p>Understanding Ownership</p>"},{"location":"cs/rust/tutorial/ownership/#understanding-ownnership","title":"Understanding ownnership","text":"<p>Ownernship model is a way to manage memory in Rust. To understand why we need this model, we need to first understand the typical way programming languages manages memory:</p> Pros Cons Garbage collection <ul><li>Error free*</li><li>Faster write time</li></ul> <ul><li>No control over memory</li><li>Slower and unpredictable runtime performance</li><li>Larger program size</li></ul> Manural memory management <ul><li>Control over memory</li><li>Faster runtime</li><li>Smaller program size</li></ul> <ul><li>Error prone</li><li>Slower write time</li></ul> Ownership model <ul><li>Control over memory</li><li>Error free*</li><li>Faster runtime</li><li>Smaller program size</li></ul> <ul><li>Slower write time. Learning curve (fighting with the borrow checker)</li></ul> <p>Warning</p> <p>Although Rust is Error Free in Memory management, you can opt out and do memory unsafe operations using <code>unsafe</code> block, at which you should know what you are doing.</p>"},{"location":"cs/rust/tutorial/ownership/#stack-and-heap","title":"Stack and Heap","text":"<p>During runtime our Rust program has access to both Stack and Heap.</p> <ul> <li>Stack: fixed size and cannot grow. Stack stores stack frames which are created for every function that executes, which stores the local variables of function being executed. The size of stack frame is calculated at compile time.Variables live only as long as the stack frame lives.</li> <li>Heap: Less organized, grows and shrink at run time, the data stored in the heap could be dynamic. We control the lifetime of the data</li> </ul> <pre><code>fn main() {\n    fn a() {\n        let x: &amp;str = \"hello\";\n        let y: i32 = 32;\n        b();\n    }\n\n    fn b() {\n        let x = String::from(\"world\");\n    }\n}\n</code></pre> <p></p> <p>When <code>a()</code> get executed it's stack frame is created, which calls <code>b()</code> which creates it's own stack frame. When <code>b()</code> finished it's execution, it's get popped off, all of the variables get's dropped. Then finally <code>a()</code> finishes and it's variables are dropped.</p> <p>Success</p> <ul> <li> <p>Pushing to Stack is faster than allocating it to heap, because the heap has to spend time looking for place to store new data.</p> </li> <li> <p>Accessing data on stack is faster than accessing data on because with heap you would have to follow the pointer.</p> </li> </ul>"},{"location":"cs/rust/tutorial/ownership/#ownership-rules","title":"Ownership Rules","text":"<p>Ownership Rules</p> <ol> <li>Each value in Rust has a variable that's called its owner.</li> <li>There can only be one owner at a time.</li> <li>When the owner goes out of scope, the value will be dropped.</li> </ol>"},{"location":"cs/rust/tutorial/ownership/#variable-scope","title":"Variable Scope","text":"<p><pre><code>fn main() {\n    { // s is not valid, because it's not yet declared\n        let s: &amp;str = \"hello\"; // s is valid from this point forward\n    } // scope ends, and s is no longer valid\n}\n</code></pre> Since <code>s</code> is str literal, as stated, it will be stored inside binary with reference in the stack.</p> <p>But if, we were to use <code>String</code> which is dynamically sized, <pre><code>fn main() {\n    { // s is not valid, because it's not yet declared\n        let s: String = String::from(\"hello\"); // s is valid from this point forward | allocate\n    } // scope ends, and s is no longer valid | deallocate\n}\n</code></pre> In manually memory managed languages like C++, you'd have to call <code>new</code> keyword and then deallocate after you're done with it with <code>delete</code> keyword.</p> <p>In Rust, this happens automatically. It autmatically allocates memory on Heap, and when scope ends it deallocates.</p>"},{"location":"cs/rust/tutorial/ownership/#memory-and-allocation","title":"Memory and Allocation","text":"<p>A simple type stored on Stack, such as integer, booleans, characters implement <code>Copy</code> trait, allowing them to be copied instead of being moved. <pre><code>fn main() {\n    let x = 5;\n    let y = x; // Copy the value\n}\n</code></pre></p> <pre><code>fn main() {\n    let s1 = String::from(\"hello\");\n    let s2 = s1;\n}\n</code></pre> <p>The <code>String</code> is made up of three parts: 1. A pointer pointing to the actual memory allocation on heap. 2. length of string. 3. Capacity, actual amount of memory allocated for String.</p> <p></p> <p><pre><code>s1 = s2;\n</code></pre> What happens when we assign <code>s1</code> to <code>s2</code>, the <code>String</code> data is copied, i.e., the pointer, the length, and the capacity that are on stack. The data on the heap that both the pointers of <code>s1</code> and <code>s2</code> remains as it is.</p> <p>To ensure memory safety, after this line, Rust invalidates <code>s1</code>.  This might sound like making a shallow copy, the concept of copying the pointer, without copying the data. But since Rust invalidates the first variable it's called a move.</p> <p>And so when running this program <pre><code>fn main() {\n    let s1 = String::from(\"hello\");\n    let s2 = s1; // Move, not a shallow copy\n\n    println!(\"{}, world!\", s1);\n}\n</code></pre> the compiler outputs:</p> <pre><code>error[E0382]: borrow of moved value: `s1`\n --&gt; src/main.rs:5:28\n  |\n2 |     let s1 = String::from(\"hello\");\n  |         -- move occurs because `s1` has type `String`, which does not implement the `Copy` trait\n3 |     let s2 = s1;\n  |              -- value moved here\n4 |\n5 |     println!(\"{}, world!\", s1);\n  |                            ^^ value borrowed here after move\n  |\n  = note: this error originates in the macro `$crate::format_args_nl` which comes from the expansion of the macro `println` (in Nightly builds, run with -Z macro-backtrace for more info)\n\nFor more information about this error, try `rustc --explain E0382`.\nwarning: `playground` (bin \"playground\") generated 1 warning\nerror: could not compile `playground` due to previous error; 1 warning emitted\n</code></pre> <p>To clone the string, call <code>clone</code> method on <code>s1</code>, which is bit expensive.</p> <pre><code>let s2 = s1.clone();\n</code></pre>"},{"location":"cs/rust/tutorial/ownership/#ownership-and-functions","title":"Ownership and Functions","text":"<pre><code>fn main() {\n    let s = String::from(\"hello\");\n    takes_ownership(s);\n    println!(\"{}\", s);\n}\n\nfn takes_ownership(some_string: String) {\n    println!(\"{}\", some_string);\n}\n</code></pre> <p>Running this the compiler complains: <pre><code>error[E0382]: borrow of moved value: `s`\n --&gt; src/main.rs:4:20\n  |\n2 |     let s = String::from(\"hello\");\n  |         - move occurs because `s` has type `String`, which does not implement the `Copy` trait\n3 |     takes_ownership(s);\n  |                     - value moved here\n4 |     println!(\"{}\", s);\n  |                    ^ value borrowed here after move\n  |\n  = note: this error originates in the macro `$crate::format_args_nl` which comes from the expansion of the macro `println` (in Nightly builds, run with -Z macro-backtrace for more info)\n\nFor more information about this error, try `rustc --explain E0382`.\nerror: could not compile `playground` due to previous error\n</code></pre></p> <p><code>s</code> cannot be borrowed after it has been moved, which happens when pass in <code>s</code> to <code>takes_ownership()</code>. This is memory safe since <code>some_string</code> gets dropped after the scope of <code>takes_ownership()</code> ends.</p> <p>Let's look at another example <pre><code>fn main() {\n    let s = String::from(\"hello\");\n    takes_ownership(s);\n    println!(\"{}\", s); // gives error, s is moved\n\n    let x = 5;\n    makes_copy(x);\n    println!(\"{}\", x); // works because primites get's copied to makes_copy\n}\n\nfn takes_ownership(some_string: String) {\n    println!(\"{}\", some_string);\n}\n\nfn makes_copy(some_integer: i32) {\n    println!(\"{}\", some_integer);\n}\n</code></pre></p> <p>Returning string moves ownership to <code>s1</code> <pre><code>fn main() {\n    let s1 = gives_ownership();\n    println!(\"s1 = {}\", s1);\n}\n\nfn gives_ownership() -&gt; String {\n    let some string = String::from(\"hello\");\n    some_string\n}\n</code></pre></p> <p>Take ownership and give it back</p> <pre><code>fn main() {\n    let s1 = gives_ownership();\n    let s2 = String::from(\"hello\");\n    let s3 = takes_and_gives_back(s2);\n}\n\nfn gives_ownership() -&gt; String {\n    let some string = String::from(\"hello\");\n    some_string\n}\n\nfn takes_and_gives_back(a_string: String) -&gt; String {\n    a_string\n}\n</code></pre>"},{"location":"cs/rust/tutorial/ownership/#references-borrowing","title":"References &amp; Borrowing","text":"<p>Rules of references</p> <ol> <li>At any given time, we can have either one mutable reference or any number of immutable references.</li> <li>References must always be valid.</li> </ol> <p>What if we wanted to use a variable without taking ownership? We can use references, which do not take ownernship of the underlying values.</p> <p>Maybe in situation like this, where we are just calculating the length of string:</p> <pre><code>fn main() {\n    let s1 = String::from(\"hello\");\n    let len = calculate_length(s1); // s1 gets moved to function\n    // trying to borrow a moved value, s1\n    println!(\"The length of '{}' is {}.\", s1, len);\n}\n\nfn calculate_length(s: String) -&gt; usize {\n    let length = s.len(); // len() returns the length of a String\n    length\n}\n</code></pre> <p>Let's make the parameters of <code>calculate_length()</code> to take a reference of <code>s</code> and pass a reference when calling this function. <pre><code>fn main() {\n    let s1 = String::from(\"hello\");\n    let len = calculate_length(&amp;s1); // s1 gets moved to function\n    // trying to borrow a moved value, s1\n    println!(\"The length of '{}' is {}.\", s1, len);\n}\n\nfn calculate_length(s: &amp;String) -&gt; usize {\n    let length = s.len(); // len() returns the length of a String\n    length\n}\n</code></pre></p> <p>Info</p> <p>Passing in references as function parameters is called borrowing because we're borrowing the value but we're not actually taking ownership.</p> <ul> <li>References are immutable by default.</li> </ul> <p>If we wanted to modify the value, without taking ownership, we would do something like this:</p> <p>Consider <code>change()</code> wants to modify <code>s1</code> using an immutable referece: <pre><code>fn main() {\n    let s1 = String::from(\"hello\");\n    change(&amp;s1);\n}\n\nfn change(some_string: &amp;String) {\n    some_string.push_str(\", world\");\n}\n</code></pre> We'll: 1. Make <code>s1</code> mutable, because mutable references will work only when there is mutable variable underneath. 2. Pass a mutable reference. 3. Accept a mutable reference in <code>change()</code>.</p> <pre><code>fn main() {\n    let mut s1 = String::from(\"hello\");\n    change(&amp;mut s1);\n}\n\nfn change(some_string: &amp;mut String) {\n    some_string.push_str(\", world\");\n}\n</code></pre> <p>Mutable references have one restriction</p> <p>You can only have one mutable reference to a particular piece of data in a particular scope.</p> <p>This prevents data races at compile time.</p> <p>You cannot do, something like this:</p> <pre><code>fn main() {\n    let mut s = String::from(\"hello\");\n\n    let r1 = &amp;mut s;\n    // cannot borrow `s` as mutable more than once at a time\n    let r2 = &amp;mut s;\n\n    println!(\"{}, {}\", r1, r2);\n}\n</code></pre> <p>To fix these, make these references immutable. <pre><code>fn main() {\n    let s = String::from(\"hello\");\n\n    let r1 = &amp;s;\n    let r2 = &amp;s;\n\n    println!(\"{}, {}\", r1, r2);\n}\n</code></pre></p> <p>But can we mix mutable references with immutable one?</p> <pre><code>fn main() {\n    let mut s = String::from(\"hello\");\n\n    let r1 = &amp;s; // these references are still immutable\n    let r2 = &amp;s;\n    // cannot borrow `s` as mutable because it is also borrows as immutable\n    let r3 = &amp;mut s;\n\n    println!(\"{}, {}\", r1, r2);\n}\n</code></pre> <p>Warning</p> <p>You can't have a mutable reference if an immutable reference already exists.</p> <p>Note</p> <p>Note that the scope of a reference starts when it's first introduced and ends when it's used for the last time. And so we can introduce a mutable reference after the <code>println!</code>:</p> <pre><code>fn main() {\n    let mut s = String::from(\"hello\");\n\n    let r1 = &amp;s;\n    let r2 = &amp;s;\n\n    println!(\"{}, {}\", r1, r2); // scope of r1, r2 ends\n    let r3 = &amp;mut s; // perfectly fine\n}\n</code></pre>"},{"location":"cs/rust/tutorial/ownership/#dangling-references","title":"Dangling References","text":"<p>A reference that points to invalid data.</p> <pre><code>fn main() {\n    let ref_to_string: &amp;String = dangle();\n}\n\n// this returns a reference to string\n// but when this function finishes executing, Rust drops `s`.\n// The reference is invalid since the variable will not exist\n// Rust prevents this from happening\nfn dangle() -&gt; &amp;String {\n    let s = String::from(\"hello\");\n    &amp;s\n}\n</code></pre> <p>The compiler complains:</p> <pre><code>error[E0106]: missing lifetime specifier\n --&gt; src/main.rs:9:16\n  |\n9 | fn dangle() -&gt; &amp;String {\n  |                ^ expected named lifetime parameter\n  |\n  = help: this function's return type contains a borrowed value, but there is no value for it to be borrowed from\nhelp: consider using the `'static` lifetime\n  |\n9 | fn dangle() -&gt; &amp;'static String {\n  |                 +++++++\n\nFor more information about this error, try `rustc --explain E0106`.\nerror: could not compile `playground` due to previous error\n</code></pre>"},{"location":"cs/rust/tutorial/ownership/#the-slice-type","title":"The Slice Type","text":"<p>Slices lets us reference a contiguous sequence of elements within a collection instead of referencing the the entire collection. Slices do not take ownership.</p> <p>Why slices are useful? Let's imagine a problem where we'd want to return the first word, we don't have a way to return part of the string, but we can return an index to end the first word.</p> <pre><code>fn main() {\n    let mut s = String::from(\"Hello world\");\n    let word = first_word(&amp;s);\n    s.clear();  // this makes `s` an empty string, `word` still remains 5.\n}\n\nfn first_word(s: &amp;String) -&gt; usize {\n    let bytes: &amp;[u8] = s.as_bytes();\n\n    // iter() let's us iterate `bytes` without taking ownership\n    for (i, &amp;item) in bytes.iter().enumerate() {\n        if item == b' ' {\n            return i;\n        }\n    }\n\n    s.len()\n}\n</code></pre> <p>There are few problems with this code: 1. The return value is not tied to the string, requires manual synchronization.</p> <p><code>word</code> remains 5, even though <code>s</code> is empty at line 4. 2. When we want to change the implementation. If we wanted to determine the second word, we'll return starting and ending index, which needs to be synced.</p> <p>To solve these issues, introduce string slices: <pre><code>fn main() {\n    let mut s = String::from(\"Hello world\");\n    let hello = &amp;s[0..5]; // creates a slice of s from 0 to 4 (5 is exclusive)\n    let world = &amp;s[6..11];\n    let word = first_word(&amp;s);\n    s.clear(); // error! mutable borrow occurs here\n\n    // But this won't work, as s.clear() mutates string,\n    //and we're passing immutable reference to `first_word()`\n    println!(\"The first word is: {}\", word);\n}\n\nfn first_word(s: &amp;String) -&gt; &amp;str { // return a string slice\n    let bytes: &amp;[u8] = s.as_bytes();\n\n    // iter() let's us iterate `bytes` without taking ownership\n    for (i, &amp;item) in bytes.iter().enumerate() {\n        if item == b' ' {\n            return &amp;s[0..i];\n        }\n    }\n\n    &amp;s[..]\n}\n</code></pre></p> <p>To fix this modify it as follows: <pre><code>fn main() {\n    let mut s = String::from(\"hello world\");\n    let s2: &amp;str = \"hello world\"; // string literals are acutally slices\n\n    // this still works; `&amp;String` gets coereced to `&amp;str`\n    // let word = first_word(&amp;s);\n\n    let word = first_word(s2);\n}\n\n// we want this function to also work with string slice\nfn first_word(s: &amp;str) -&gt; &amp;str { // return a string slice\n    let bytes: &amp;[u8] = s.as_bytes();\n\n    // iter() let's us iterate `bytes` without taking ownership\n    for (i, &amp;item) in bytes.iter().enumerate() {\n        if item == b' ' {\n            return &amp;s[0..i];\n        }\n    }\n\n    &amp;s[..]\n}\n</code></pre></p> <p>Note</p> <p>We can simplify string slices:</p> <pre><code>fn main() {\n    let mut s = String::from(\"Hello world\");\n    let hello = &amp;s[..5]; // references \"Hello\"\n    let world = &amp;s[6..]; // references from 6th to end\n    let hello_world = &amp;s[..]; // references whole string\n}\n</code></pre> <p>Slices on different types of collections <pre><code>fn main() {\n    let a = [1, 2, 3, 4, 5];\n    let slice = &amp;a[..2];\n}\n</code></pre></p>"},{"location":"cs/rust/tutorial/structs/","title":"Structs in Rust","text":"<p>Enums and Structs are the building blocks for creating new types.</p> <p>Structs allow us to group related data together. Think about them as object attributes/class attributes in object-oriented programming.</p> <p>Using Structs to Structure Related Data</p>"},{"location":"cs/rust/tutorial/structs/#using-structs","title":"Using Structs","text":"<p>Let's start with creating a struct for a user.</p> <pre><code>struct User {\n    username: String, // we want our fields to actually own string data\n    email: String,\n    sign_in_count: u64,\n    active: bool\n}\n\nfn main() {\n    // create new instance of `User`\n    let mut user1 = User {\n        email: String::from(\"happy@gmail.com\"),\n        username: String::from(\"Happy\"),\n        sign_in_count: 1\n        active: true,\n    }\n\n    // access the fields using dot notation\n    let name = user1.username;\n\n    // We can modify specific values in our struct using dot notation\n    // But make sure the instance itself is mutable\n    user1.username = String::from(\"wallabag\");\n}\n</code></pre>"},{"location":"cs/rust/tutorial/structs/#function-contructors","title":"Function Contructors","text":"<p>We can also use functions to construct new instances of <code>User</code>.</p> <pre><code>fn main() {\n    let user2 = build_user(\n        String::from(\"kevin@gmail.com\"),\n        String::from(\"Kevin\")\n    );\n\n\n}\nfn build_user(email: String, username: String) -&gt; User {\n    User {\n        email,  // field init shorthand syntax\n        username,\n        sign_in_count: 1,\n        active: true\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/structs/#reusing-instance-data","title":"Reusing Instance Data","text":"<p>Create new instances, using existing instances</p> <pre><code>fn main() {\n    let user3 = User {\n        email: String::from(\"daniel@gmail.com\"),\n        username: String::from(\"daniel\"),\n        ..user2   // all other fields assigned from `user2`\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/structs/#tuple-structs","title":"Tuple Structs","text":"<p>Create structs without named field, called tuple structs.  Useful when we want our enture tuple to have a name and be of different type than other tuples. <pre><code>fn main() {\n    // both color and point have the same 3 field types\n    // so, if a function expects a tuple type of Color,\n    // we can't pass it a tuple of `Point` and vice versa\n    struct Color(i32, i32, i32);\n    struct Point(i32, i32, i32);\n}\n</code></pre>"},{"location":"cs/rust/tutorial/structs/#example-use-cases","title":"Example Use Cases","text":"<p>We'll rewrite this program that calculates the area of rectangle, to understand Structs.</p> <pre><code>fn main() {\n    let width1 = 30;\n    let heigh1 = 50;\n\n    printlln!(\n        \"The area of the rectangle is {} square units.\").\n        area(width1, height1)\n    );\n}\n\nfn area(width: u32, height: u32) -&gt; u32 {\n    width * height\n}\n</code></pre> <p>This program works, but could be improved. Let's start grouping the <code>width</code> and <code>height</code> variable which are related.</p> <pre><code>fn main() {\n    let rect = (30, 50);\n\n    printlln!(\n        \"The area of the rectangle is {} square units.\").\n        area(rect)\n    );\n}\n\nfn area(dimesions: (u32, u32)) -&gt; u32 {\n    dimensions.0 * dimensions.1\n}\n</code></pre> <p>This also works fine. However it's not clear what the fields in the tuple represent. Let's use Structs.</p> <pre><code>struct Rectangle {\n    width: u32,\n    height: u32\n}\n\nfn main() {\n    let rect = Rectangle { width: 30, height: 50};\n\n    printlln!(\n        \"The area of the rectangle is {} square units.\").\n        area(&amp;rect)\n    );\n}\n\nfn area(rectangle: &amp;Rectangle) -&gt; u32 {\n    rectangle.width * rectangle.height\n}\n</code></pre> <p>Derived Traits It would be nice to see what our rectangle instances look like. What we want is something like:</p> <p><pre><code>println!(\"rect: {}\", rect);\n</code></pre> But this doesn't work, because since our custom type doesn't implement <code>std::fmt::Display</code> trait, which specifies how something should be printed.</p> <p>We can however use <code>{:?}</code> or <code>{:#?}</code> (which prints every field on newline) for pretty print, but requires <code>Debug</code> trait to be implemented for the custom type, helpful for developers. We can do this by adding <code>#[derive(Debug)]</code> or manually implement <code>Debug</code> trait for the struct.</p> <pre><code>#[derive(Debug)]\nstruct Rectangle {\n    width: u32,\n    height: u32\n}\n\nfn main() {\n    let rect = Rectangle { width: 30, height: 50};\n    println!(\"rect: {:?}\", rect);\n    ...\n</code></pre>"},{"location":"cs/rust/tutorial/structs/#method-syntax","title":"Method Syntax","text":"<p>How can we tie closely related functions to an instance of struct, named methods, instead of defining separately.</p> <p>This requires us to use <code>impl</code> block which houses methods for a particular Struct.</p> <pre><code>impl Rectangle {\n    fn area(&amp;self) -&gt; u32 {\n        self.width * self.height\n    }\n}\n\nfn main() {\n    let rect = Rectangle { width: 30, height: 50};\n    printlln!(\n        \"The area of the rectangle is {} square units.\").\n        rect.area()\n    );\n}\n</code></pre> <p>Note</p> <ol> <li>The first argument in a method is always <code>self</code> which is the instance the method is being called. We can also take a mutable reference, or in rare cases we can can take ownership.</li> <li>Rust has a feature called autmatic referencing or derefering, which allows us to use dot notation (instead of two different notations, like in C++) on the object directly or calling on the reference of the object.</li> </ol> <p>Let's create a method that takes in multiple parameters, called <code>can_hold</code>, which takes a reference to another rectangle and determine if the current rectangle instances can hold another rectangel inside itself.</p> <pre><code>impl Rectangle {\n    fn can_hold(&amp;self, other: &amp;Rectangle) -&gt; bool {\n        self.width &gt; other.width &amp;&amp; self.height &gt; other.height\n    }\n}\n\nfn main() {\n    let rect = Rectangle { width: 30, height: 50};\n    let rect1 = Rectangle { width: 20, height: 40};\n    let rect2 = Rectangle { width: 40, height: 50}\n\n    println!(\"rect can hold rect1: {}\", rect.can_hold(&amp;rect1));\n    println!(\"rect can hold rect1: {}\", rect.can_hold(&amp;rect2));\n}\n</code></pre>"},{"location":"cs/rust/tutorial/structs/#associated-functions","title":"Associated Functions","text":"<p>We can also define associated functions inside <code>impl</code> block, but unlike methods, they are not tied to an instance of our struct (think of them static methods).</p> <p>We can create a new <code>impl</code> block to write a associated function that construct a square.</p> <p>Note</p> <ul> <li>Notice, there is no <code>self</code> argument for associated functions.</li> <li>Use <code>::</code> on <code>Rectangle</code> struct to call associated functions.</li> <li>We can also use them to write constructors for that type.</li> </ul> <pre><code>impl Rectangle {\n    fn square(size: u32) -&gt; Rectangle {\n        Rectangle {\n            width: size,\n            height: size\n        }\n    }\n}\n\nfn main() {\n    let sq = Rectangle::square(25);\n}\n</code></pre>"},{"location":"cs/rust/tutorial/traits/","title":"Traits in Rust","text":"<p>Traits define shared behavior for structs.</p> <p>Traits: Defining Shared Behavior</p>"},{"location":"cs/rust/tutorial/traits/#defining-traits","title":"Defining Traits","text":"<p>Let's look at the below code:</p> <pre><code>pub struct NewsArticle {\n    pub author: String,\n    pub headline: String,\n    pub content: String,\n}\n\npub struct Tweet {\n    pub username: String,\n    pub content: String,\n    pub reply: bool,\n    pub retweet: bool,\n}\n</code></pre> <p>We want is the ability to summarize a news article or a Tweet, so that we can post it in text aggregation feed of our system.</p> <p>We can use Trait to define that shared behavior between <code>NewsArticle</code> and <code>Tweet</code> to summarize.</p> <p>Traits allow us to define a set of methods that are shared across different types. Every type that implements this trait should have the method defined in the trait.</p> <pre><code>pub trait Summary {\n    fn summarize(&amp;self) -&gt; String;\n}\n</code></pre> <p>Implement <code>Summary</code> trait for <code>NewsArticle</code> and <code>Tweet</code> <pre><code>pub struct NewsArticle {\n    pub author: String,\n    pub headline: String,\n    pub content: String,\n}\n\n// then we'll implement `Summary` trait for `NewsArticle`\nimpl Summary for NewsArticle {\n    fn summarize(&amp;self) -&gt; String {\n        format!(\"{}, by {}\", self.headline, self.author)\n    }\n}\n</code></pre></p> <pre><code>pub struct Tweet {\n    pub username: String,\n    pub content: String,\n    pub reply: bool,\n    pub retweet: bool,\n}\n\nimpl Summary for Tweet {\n    fn summarize(&amp;self) -&gt; String {\n        format!(\"{}: {}\", self.username, self.content)\n    }\n}\n</code></pre> <p>Let's write the main function and try running this:</p> <pre><code>fn main() {\n    let tweet = Tweet {\n        username: String::from(\"@johndoe\"),\n        content: String::from(\"Hello World!\"),\n        reply: false,\n        retweet: false\n    };\n    let article = NewsArticle {\n        author: String::from(\"John Doe\"),\n        headline: String::from(\"The Sky is Falling\"),\n        content: String::from(\"The sky is not actually falling.\")\n    };\n\n    println!(\"Tweet summary: {}\", tweet.summarize());\n    println!(\"Article summary: {}\", article.summarize());\n}\n</code></pre> <p>Outputs:</p> <pre><code>Tweet summary: @johndoe: Hello World!\nArticle summary: The Sky is Falling, by John Doe\n</code></pre>"},{"location":"cs/rust/tutorial/traits/#orphan-rule","title":"Orphan Rule","text":"<p>We can implement a trait on a type as long as the trait or the type is defined within the crate.</p> <p>This rule ensures that other people\u2019s code can\u2019t break your code and vice versa. Without the rule, two crates could implement the same trait for the same type, and Rust wouldn\u2019t know which implementation to use.</p>"},{"location":"cs/rust/tutorial/traits/#default-implementations","title":"Default Implementations","text":"<p>Instead of expecting every type that implements a trait to define the body of the those functions, we can (when needed) specify the default implementation which can be overrided.</p> <pre><code>pub trait Summary {\n    fn summarize(&amp;self) -&gt; String {\n        String::from(\"(Read more...)\")\n    }\n}\n</code></pre> <p>When you don't want to override the default implementation just specify that the type implement the trait, and only override the function that you want to:</p> <pre><code>impl Summary for NewsArticle {}\n</code></pre> <p>Now the program outputs:</p> <pre><code>Tweet summary: @johndoe: Hello World!\nArticle summary: (Read more...)\n</code></pre> <p>Default implementations can other methods inside our trait definition:</p> <pre><code>pub trait Summary {\n    fn summarize_author(&amp;self) -&gt; String;\n\n    fn summarize(&amp;self) -&gt; String {\n        format!(\"(Read more from {}...)\", self.summarize_author())\n    }\n}\n</code></pre> <p>Now we do need to implement <code>summarize_author()</code> for both <code>NewsArticle</code> and <code>Tweet</code> but not <code>summarize()</code> for <code>NewsArticle</code> as it follows default implementation:</p> <pre><code>impl Summary for NewsArticle {\n    // no `summarize()`; default implementation for that\n\n    fn summarize_author(&amp;self) -&gt; String {\n        format!(\"{}\", self.author)\n    }\n}\n</code></pre> <pre><code>impl Summary for Tweet {\n    fn summarize_author(&amp;self) -&gt; String {\n        format!(\"@{}\", self.username)\n    }\n\n    fn summarize(&amp;self) -&gt; String {\n        format!(\"{}: {}\", self.username, self.content)\n    }\n}\n</code></pre> <p>This changes the output for <code>NewsArticle</code></p> <pre><code>Tweet summary: @johndoe: Hello World!\nArticle summary: (Read more from John Doe...)\n</code></pre>"},{"location":"cs/rust/tutorial/traits/#trait-bounds","title":"Trait Bounds","text":"<p>Using Traits as parameters.</p> <p>Check the code below, which follows our summarization system above:</p> <pre><code>// `notify()` has one parameter `item` which is a reference\n// to something that implements `Summary` trait\npub fn notify(item: &amp;impl Summary) {\n    println!(\"Breaking news! {}\", item.summarize())\n}\n</code></pre> <p>Now in the main function:</p> <pre><code>fn main() {\n     let article = NewsArticle {\n        author: String::from(\"John Doe\"),\n        headline: String::from(\"The Sky is Falling\"),\n        content: String::from(\"The sky is not actually falling.\")\n    };\n\n    notify(&amp;article);\n}\n</code></pre> <p>Outputs:</p> <pre><code>Breaking news! (Read more from John Doe...)\n</code></pre>"},{"location":"cs/rust/tutorial/traits/#trait-bound-syntax","title":"Trait Bound Syntax","text":"<p>This works for straightforward cases but's it's actually syntax sugar for something called Trait Bound, which looks like this:</p> <pre><code>// T generic limited to somthing that implements `Summary` trait\npub fn notify&lt;T: Summary&gt;(item: &amp;T) {\n    println!(\"Breaking news! {}\", item.summarize())\n}\n</code></pre> <p>Let's look at more complex cases. What if we wanted <code>item1</code> and <code>item2</code> to be exactly same syntax:</p> <pre><code>// impl syntax: nah. this syntax won't work\n// `item1` and `item2` can be two different type both implementing `Summary` trait\npub fn notify(item1: &amp;impl Summary, item2: &amp;impl Summary) {\n    // ...\n}\n\n// Trait Bound syntax: yup.\n// `T` generic will now be same type that implements `Summary` trait\npub fn notify&lt;T: Summary&gt;(item1: &amp;T, item2: &amp;T) {\n    // ...\n}\n</code></pre>"},{"location":"cs/rust/tutorial/traits/#specifying-multiple-traits","title":"Specifying multiple Traits","text":"<p>We can also specify multiple traits:</p> <pre><code>// Something which implements both `Summary` and `Display` trait\npub fn notify(item: &amp;(impl Summary + Display), item2: &amp;impl Summary) {\n    // ...\n}\n\npub fn notify&lt;T: Summary + Display&gt;(item1: &amp;T, item2: &amp;T) {\n    // ...\n}\n</code></pre>"},{"location":"cs/rust/tutorial/traits/#fixing-readability","title":"Fixing Readability","text":"<p>Specifying trait bounds can hinder readability</p> <pre><code>fn some_function&lt;T: Display + Clone, U: Clone + Debug&gt;(t: &amp;T, u: &amp;U) -&gt; i32 {\n    // ...\n}\n</code></pre> <p>To fix this we can use <code>where</code> clause:</p> <pre><code>fn some_function&lt;T, U&gt;(t: &amp;T, u: &amp;U) -&gt; i32\nwhere T: Display + Clone,\n        U: Clone + Debug\n{\n\n}\n</code></pre>"},{"location":"cs/rust/tutorial/traits/#returning-types-the-implement-traits","title":"Returning Types the Implement Traits","text":"<p>Following the original example, we can return only one type that implements a trait, useful in iterators and closures.</p> <pre><code>pub trait Summary {\n    fn summarize_author(&amp;self) -&gt; String;\n\n    fn summarize(&amp;self) -&gt; String {\n        format!(\"(Read more from {}...)\", self.summarize_author())\n    }\n}\n\n// returning any type that implements `Summry` trait; not a concrete type\nfn returns_summarizable() -&gt; impl Summary {\n    Tweet {\n        username: String::from(\"horse_ebooks\"),\n        content: String::from(\n            \"of course, as you probably already know, people\",\n        ),\n        reply: false,\n        retweet: false,\n    }\n}\n\nfn main() {\n    println!(\"{}\", returns_summarizable().summarize());\n}\n</code></pre> <p>Outputs:</p> <pre><code>horse_ebooks: of course, as you probably already know, people\n</code></pre> <p>We can only return one type</p> <p>This is not allowed. This has to do with how the compiler implements the <code>impl</code> syntax.</p> <pre><code>fn returns_summarizable(switch: bool) -&gt; impl Summary {\n    if switch {\n        NewsArticle {\n            headline: String::from(\n                \"Penguins win the Stanley Cup Championship!\",\n            ),\n            author: String::from(\"Iceburgh\"),\n            content: String::from(\n                \"The Pittsburg Penguins once again are the best \\\n                hockey team in the NHL.\",\n            ),\n        }\n    } else {\n        Tweet {\n            username: String::from(\"horse_ebooks\"),\n            content: String::from(\n                \"of course, as you probably already know, people\",\n            ),\n            reply: false,\n            retweet: false,\n        }\n    }\n}\n</code></pre> <p>Compiler will complain:</p> <pre><code>error[E0308]: `if` and `else` have incompatible types\n</code></pre>"},{"location":"cs/rust/tutorial/traits/#conditionally-implement-methods","title":"Conditionally Implement Methods","text":"<pre><code>use std::fmt::Display;\n\nstruct Pair&lt;T&gt; {\n    x: T,\n    y: T,\n}\n\n// every `Pair` struct will have `new()` associative function\nimpl&lt;T&gt; Pair&lt;T&gt; {\n    fn new(x: T, y: T) -&gt; Self {\n        Self { x, y }\n    }\n}\n\n// however `cmp_display()` method is only available to `Pair`\n// structs where the type of `x` and `y` implements `Display`\n// `PartialOrd` traits\nimpl&lt;T: Display + PartialOrd&gt; Pair&lt;T&gt; {\n    fn cmp_display(&amp;self) {\n        if self.x &gt;= self.y {\n            println!(\"The largest member is x = {}\", self.x);\n        } else {\n            println!(\"The largest member is y = {}\", self.y);\n        }\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/traits/#blanket-implementations","title":"Blanket Implementations","text":"<p>We can implement a trait on a type that implements another trait.</p> <pre><code>// implement `ToString` trait on any type `T` that implements\n// `Display` trait\nimpl&lt;T: Display&gt; ToString for T {\n    // ...\n}\n</code></pre>"},{"location":"cs/rust/tutorial/traits/#difference-between-self-and-self","title":"Difference between self and Self","text":"<p>Difference between <code>self</code> and <code>Self</code>?</p> <p>Self is the type of the current object. It may appear either in a <code>trait</code> or an <code>impl</code>, but appears most often in <code>trait</code> where it is a stand-in for whatever type will end up implementing the <code>trait</code> (which is unknown when defining the trait):</p> <p><pre><code>trait Clone {\n    fn clone(&amp;self) -&gt; Self;\n}\n</code></pre> Implement <code>Clone</code>:</p> <pre><code>impl Clone for MyType {\n    // I can use either the concrete type (known here)\n    fn clone(&amp;self) -&gt; MyType;\n\n    // Or I can use Self again, it's shorter after all!\n    fn clone(&amp;self) -&gt; Self;\n}\n\n// or\nimpl Clone for MyType {\n    fn new(a: u32) -&gt; Self {\n        // ...\n    }\n}\n</code></pre> <p><code>self</code> is the name used in a <code>trait</code> or an <code>impl</code> for the first argument of a method. Using another name is possible, however there is a notable difference:</p> <ul> <li>if using self, the function introduced is a method</li> <li>if using any other name, the function introduced is an associated function</li> </ul> <p>In Rust, there is no implicit this argument passed to a type's methods: we have to explicitly pass the \"current object\" as a method parameter. This would result in:</p> <pre><code>impl MyType {\n    fn doit(this: &amp;MyType, a: u32) { ... }\n}\n\n// but we can write the shorter way\nimpl MyType {\n    fn doit(&amp;self, a: u32) { ... }\n}\n</code></pre> <p>So, basically:</p> <pre><code>self =&gt; self: Self\n&amp;self =&gt; self: &amp;Self\n&amp;mut self =&gt; self: &amp;mut Self\n</code></pre> <p>Source:  What's the difference between self and Self? (Stack Overflowr)</p>"},{"location":"cs/rust/tutorial/advanced/advanced-functions-and-closures/","title":"Advanced Functions and Closures in Rust","text":"<p>Advanced Functions and Closures</p>"},{"location":"cs/rust/tutorial/advanced/advanced-functions-and-closures/#function-pointers","title":"Function Pointers","text":"<p>We can also pass functions to functions, like passing closures to functions.</p> <p>This is useful when we want to pass a function we already defined instead of creating a new closure.</p> <p>To pass in a function as an argument, we can use a function pointer specified using <code>fn</code>:</p> <pre><code>fn add_one(x: i32) -&gt; i32 {\n    x + 1\n}\n\n// `f` accept a function with one input i32 and returns i32\nfn do_twice(f: fn(i32)) -&gt; i32, arg: i32) -&gt; i32 {\n    f(arg) + f(arg)\n}\n\nfn main() {\n    let answer = do_twice(add_one, 5);\n\n    println!(\"The answer is: {}\", answer);\n}\n</code></pre> <p>Unlike closures <code>fn</code> is a type rather than a trait. So, we can use <code>fn</code> directly.</p> <p>If we recall from Closures section:</p> <p>fn traits </p> <p>there are three closure traits:</p> <ol> <li><code>Fn</code>: specifies that the closure the variables in its environment immutably</li> <li><code>FnMut</code>: specifies the closure capture the variables in its environment mutably</li> <li><code>FnOnce</code>: specifies that the closure takes ownership of the values in its environment (thus cosuming the variables)</li> </ol> <p>Function pointers implement all these three closure traits.</p> <p>i.e., if we have a function that expects a closure, we can pass it a closure or function pointer. That's why it's preferable to write function that expects a closure instead of just pointer type.</p> <p>In above code we can update <code>do_twice()</code> function to take either closure or function like this:</p> <pre><code>// 1. specify generic\n// 2. change f: T\n// 3. Introduce trait bound using `Fn`\nfn do_twice&lt;T&gt;(f: T, arg: i32) -&gt; i32\nwhere T: Fn(i32) -&gt; i32 {\n    f(arg) + f(arg)\n}\n</code></pre> <p><code>Fn</code> is closure trait bound whereas <code>fn</code> which is a function pointer type.</p> <p>The case in which we may only want to accept function pointer instead of functions pointers and closures is if you're interfacing with external code that does not support closures. <code>C</code> functinos don't closures.</p> <p>Another example where we can use closure or predefined function:</p> <pre><code>fn main() {\n    let list_of_numbers = vec![1, 2, 3];\n    let list_of_strings: Vec&lt;String&gt; =\n        list_of_numbers\n            .iter()\n            .map(|i| i.to_string()) // map takes closure or function pointer called for every int\n            .collect();\n    println!(\"{:?}\", list_of_strings);\n}\n</code></pre> <p>To pass in a function pointer to <code>map()</code> method, we can do using fully qualified syntax:</p> <pre><code>// passing `to_string()` method on ToString trait\n// because there can be other method named `to_string` in this scope\n.map(ToString::to_string)\n</code></pre> <p>Anoter useful pattern that exploits an implementation detail of tuple structs and tuple struct enum variants.</p> <pre><code>fn main() {\n    enum Status {\n        Value(u32),\n        Stop,\n    }\n}\n</code></pre> <p>Tuple struct uses parantheses <code>()</code> to initialize values inside the tuple struct which looks like function call.</p> <p>Actually these initializers are implemented as functions that take in arguments and return an instance of that tuple struct.</p> <p>That means we can use these initializers as function pointers:</p> <pre><code>// creating vector of Statuses\n// by calling map on range passing in `Value` variant\n// map treats `Value` as a function pointer with argument being `u32`\n// return value being `Value` variant\nlet list_of_statuses: Vec&lt;Status&gt; =\n    (0u32..20).map(Status::Value).collect();\n</code></pre> <p>Obviously closures can also be passed in.</p>"},{"location":"cs/rust/tutorial/advanced/advanced-functions-and-closures/#returning-closures","title":"Returning Closures","text":"<p>Returning closures from functions.</p> <pre><code>fn returns_closure() -&gt; {\n    |x| x + 1\n}\n</code></pre> <p>This function returns a closure, but it's unfinished. Closures are represented using traits, so the function won't return a concrete type.</p> <p>What we want to say is we want to return something that implements the <code>Fn</code> trait taking in an integer and returning an integer.</p> <pre><code>fn returns_closure() -&gt; impl Fn(i32) -&gt; i32 {\n    |x| x + 1\n}\n</code></pre> <p>The above syntax might not work in all situation. For example, like in this where we return a closue based on input argument:</p> <pre><code>fn returns_closure(a: i32) -&gt; impl Fn(i32) -&gt; i32 { // this syntax only work when returning one type\n    if a &gt; 0 {\n        move |b| a + b\n    } else {\n        move |b| a - b\n      //^^^^^^^^^^^^^^ error: no two closures, even if identical have same type\n      // consider boxing your closure and/or using it as a trait object\n    }\n}\n</code></pre> <p>So instead, we need to return a trait object, wrapped in smart pointer so compiler can know it's size  like this:</p> <pre><code>fn returns_closure(a: i32) -&gt; Box&lt;dyn Fn(i32) -&gt; i32&gt; {\n    if a &gt; 0 {\n        Box::new(move |b| a + b)\n    } else {\n        Box::new(move |b| a - b)\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/advanced/advanced-traits/","title":"Advanced Traits Rust","text":"<p>Default Generic Type Parameters, Fully Qualified syntax</p> <p>Advanced Traits</p>"},{"location":"cs/rust/tutorial/advanced/advanced-traits/#associated-types","title":"Associated Types","text":"<p>Associated types are placeholders which we can add to your trait and then methods can use that placeholder.</p> <pre><code>pub trait Iterator {\n    type Item;  // associated type named `Item`\n\n    fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt;;\n}\n\nfn main() {}\n</code></pre> <p>When we implemented our <code>Iterator</code> trait, we'll specify a concrete type for <code>Item</code>.</p> <p>This way we can define a trait which uses some type that's unknown until we implement the trait.</p> <p>Like the <code>next()</code> method which returns the next item in the iteration, however the type of that item is unknown until the trait is implemented. If the trait is implemented for vector of <code>i32</code>, then <code>Item</code> will be <code>i32</code>.</p> <p>Difference between generics and Associated types</p> <p>Although they both allow us to define a type without specifying the concrete value.</p> <p>The difference is with associated types we can only have one concrete type per implementation.</p> <p>With generics we can have multiple concrete types per implementation.</p> <p>Let's say we have a struct <code>Counter</code> and then we implement <code>Iterator</code> trait for our new struct:</p> <pre><code>pub trait Iterator {\n    type Item;  // associated type named `Item`\n\n    fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt;;\n}\n\nstruct Counter {}\n\nimpl Iterator for Counter {\n    type Item = u32;        // specify associated type\n\n    fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; {\n        Some(0)             // just an example\n    }\n}\n\nimpl Iterator for Counter {\n//^^^^^^^^^^^^^^^^^^^^^^^ error: conflicting implementation of trait `Iterator` for type `Counter`\n    type Item = u16;\n\n    fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; {\n        Some(0)\n    }\n}\n</code></pre> <p>Here we can't have another implementation where <code>Item</code> is something different.</p> <p>If we use generics instead, and this compiles fine:</p> <pre><code>pub trait Iterator&lt;T&gt; {\n    fn next(&amp;mut self) -&gt; Option&lt;T&gt;;\n}\n\nstruct Counter {}\n\nimpl Iterator&lt;u32&gt; for Counter {\n    fn next(&amp;mut self) -&gt; Option&lt;u32&gt; {\n        Some(0)\n    }\n}\n\nimpl Iterator&lt;u16&gt; for Counter {\n    fn next(&amp;mut self) -&gt; Option&lt;u16&gt; {\n        Some(0)\n    }\n}\n</code></pre> <p>So when should you use which one?</p> <ul> <li>Ask the question to yourself does it make sense to have multiple implementations for a single type or just one implementation.</li> </ul>"},{"location":"cs/rust/tutorial/advanced/advanced-traits/#default-generic-type-parameters-operator-overloading","title":"Default Generic Type Parameters &amp; Operator Overloading","text":"<p>Generic type parameters could specify a default concrete type allowing implementors to not have to specify a concrete type (unless different from default).</p> <p>Great usecase for this is when we need to customize the behavior of an operator, aka, Operator Overloading.</p> <p>Rust allows us to customize the semantics of certain operators that have associated traits in the standard libary's <code>ops</code> module.</p> <pre><code>use std::ops::Add;\n\n#[derive(Debug, PartialEq)]\nstruct Point {\n    x: i32,\n    y: i32,\n}\n\n// add operator overloading for `Point`\nimpl Add for Point {\n    type Output = Point;\n\n    fn add(self, other: Point) -&gt; Point {\n        Point {\n            x: self.x + other.x,\n            y: self.y + other.y,\n        }\n    }\n}\n\nfn main() {\n    assert_eq!(\n        Point { x: 1, y: 0 } + Point { x: 2, y: 3},\n        Point { x: 3, y: 3}\n    );\n}\n</code></pre> <p>If we were to look at the imlementation of the add trait it would look like this:</p> <pre><code>// a generic called `Rhs` (Right hand side) with default `Self`\ntrait Add&lt;Rhs=Self&gt; {\n    type Output;\n\n    fn add(self, rhs: Rhs) -&gt; Self::Output;\n}\n</code></pre> <p>Here <code>Rhs</code> has default concrete type, <code>Self</code> or the type that's implementing the <code>Add</code> trait, since if we are going to add two things together, they are probably of same type.</p> <p>That's why we didn't need to specify concrete type when we implemented <code>Add</code> trait for <code>Point</code> because <code>Add</code> trait has a default concrete type and that will return a <code>Point</code>.</p> <p>What about specifying the type passed into the add method then?</p> <pre><code>use std::ops::Add;\n\nstruct Millimeters(u32);\nstruct Meters(u32);\n\nimpl Add&lt;Meters&gt; for Millimeters {\n    type Output = Millimeters;\n\n    fn add(self, other: Meters) -&gt; Millimeters {\n        Millimeters(self.0 + (other.0 * 1000))\n    }\n}\n</code></pre> <p>Here, we wanted the <code>Rhs</code> generic passed into the <code>add()</code> method to be <code>Meters</code>. We wanted the ability to add <code>Meters</code> to the <code>Millimeters</code> and <code>Millimeters</code> be returned.</p> <p>Use default generic type parameters: 1. To extend a type without breaking existing code 2. To allow customization for specific cases which most users won't need.</p>"},{"location":"cs/rust/tutorial/advanced/advanced-traits/#calling-method-with-the-same-name","title":"Calling Method with the Same Name","text":"<p>Rust allows us to have two traits with same method and implement both those traits on one type.</p> <p>It's also possible to implement a method on the type itself with the same name as the methods inside the traits. If we get into the situation where we have the same name then we need to tell Rust which method we'd like to call.</p> <pre><code>trait Pilot {\n    fn fly(&amp;self);\n}\n\ntrait Wizard {\n    fn fly(&amp;self);\n}\n\nstruct Human;\n\nimpl Human {\n    fn fly(&amp;self) {\n        println!(\"*waving arms furiously*\");\n    }\n}\n\nimpl Pilot for Human {\n    fn fly(&amp;self) {\n        println!(\"This is you captain speaking.\");\n    }\n}\n\nimpl Wizard for Human {\n    fn fly(&amp;self) {\n        println!(\"Up!\");\n    }\n}\n\nfn main() {\n    let person = Human; // struct has empty fields\n    person.fly()\n}\n</code></pre> <p>Running this produces:</p> <pre><code>*waving arms furiously*\n</code></pre> <p>which is a method implemented on <code>Human</code> struct.</p> <p>If we wanted to call <code>fly()</code> method from <code>Pilot</code> trait or <code>Wizard</code> trait then we need syntax like this:</p> <pre><code>fn main() {\n    let person = Human;\n    Pilot::fly(&amp;person); // calling `fly()` method of `Pilot` trait\n    Wizard::fly(&amp;person); // calling `fly()` method of `Wizard` trait\n}\n</code></pre> <p>produces:</p> <pre><code>This is you captain speaking.\nUp!\n</code></pre> <p>Because <code>fly()</code> method takes <code>self</code> has a parameter, if we had two different structs which both implemented the <code>Wizard</code> trait for example, Rust wouldknow which fly method to call based on the type of <code>self</code>.</p> <p>However this is not true for Associated functions because they don't take <code>self</code> as parameter. Let demonstrate that:</p> <pre><code>trait Pilot {\n    fn fly();\n}\n\ntrait Wizard {\n    fn fly();\n}\n\nstruct Human;\n\nimpl Human {\n    fn fly() {\n        println!(\"*waving arms furiously*\");\n    }\n}\n\nimpl Pilot for Human {\n    fn fly() {\n        println!(\"This is you captain speaking.\");\n    }\n}\n\nimpl Wizard for Human {\n    fn fly() {\n        println!(\"Up!\");\n    }\n}\n\nfn main() {\n    // call associated functions\n    Human::fly()\n}\n</code></pre> <p>Running this produces:</p> <pre><code>*waving arms furiously*\n</code></pre> <p>The associated functions that get's called by default is the associated function on our struct.</p> <p>But, what if we wanted to run the associated function defined in implemented traits? For that we need Fully qualified syntax:</p> <pre><code>fn main() {\n    // fully qualified syntax\n    &lt;Human as Wizard&gt;::fly()\n}\n</code></pre> <p>updating the main function with this above function produces:</p> <pre><code>Up!\n</code></pre>"},{"location":"cs/rust/tutorial/advanced/advanced-traits/#supertraits","title":"Supertraits","text":"<p>We may have a trait that's dependent on functionality from another trait, i.e., our trait is dependent on other trait being implemented. In that case the trait we rely is called Supertrait.</p> <p>For example:</p> <pre><code>use std::fmt;\n\n// this print output with surrounding `*`\n// **********\n// *        *\n// * (1, 3) *\n// *        *\n// **********\ntrait OutlinePrint {\n    fn outline_print(&amp;self) {\n        let output = self.to_string();\n        let len = output.len();\n        println!(\"{}\", \"*\".repeat(len + 4));\n        println!(\"*{}*\", \" \".repeat(len + 2));\n        println!(\"* {} *\", output);\n        println!(\"*{}*\", \" \".repeat(len + 2));\n        println!(\"{}\", \"*\".repeat(len + 4));\n    }\n}\n</code></pre> <p>We'll get an error, because we don't know if <code>self</code> the type for which <code>OutlinePrint</code> is going to be implemented will implement <code>to_string()</code>. This method in implemented in <code>Display</code> trait. **Our trait <code>OutlinePrint</code> depends on <code>Display</code> trat. So we do want to make sure that anything that implements <code>OutlinePrint</code> also implements <code>Display</code> trait.</p> <p>To encode this requirement:</p> <pre><code>// this trait depends on fmt::Display\ntrait OutlinePrint: fmt::Display {\n    fn outline_print(&amp;self) {\n        let output = self.to_string();\n        let len = output.len();\n        println!(\"{}\", \"*\".repeat(len + 4));\n        println!(\"*{}*\", \" \".repeat(len + 2));\n        println!(\"* {} *\", output);\n        println!(\"*{}*\", \" \".repeat(len + 2));\n        println!(\"{}\", \"*\".repeat(len + 4));\n    }\n}\n</code></pre> <p>Let's see what happens when we implement this trait on our <code>Point</code> struct without implementing <code>Display</code> struct on it:</p> <pre><code>struct Point {\n    x: i32,\n    y: i32,\n}\n\nimpl OutlinePrint for Point {}\n//   ^^^^^^^^^^^^ error: `Point` doesn't implement `std::fmt::Display`.\n</code></pre> <p>We'll get an error, to fix that just implement the <code>Display</code> trait:</p> <pre><code>impl Dfmt::Display for Point {\n    fn fmt(&amp;self, f: &amp;mut fmt::Formatter) -&gt; fmt::Result {\n        write!(f, \"({}, {})\", self.x, self.y)\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/advanced/advanced-traits/#newtype-pattern","title":"Newtype Pattern","text":"<p>We learned about Orphan rule in traits section (Rustlang book, Ch 10).</p> <p>Orphan Rule</p> <p>The Newtype pattern allows us to get around this restriction.</p> <p>We do this by creating a tuple struct with one field being the type we're wrapping. This wrapper around our type is local to our crate so we can implement a new trait.</p> <p>For example, here we want to implement <code>Display</code> trait for a vector.</p> <pre><code>use std::fmt;\n\nstruct Wrapper(Vec&lt;String&gt;);\n\nimpl fmt::Display for Wrapper {\n    fn fmt(&amp;self, f: &amp;mut fmt::Formatter) -&gt; fmt::Result {\n        write!(f, \"[{}]\", self.0.join(\", \"))\n    }\n}\n\nfn main() {\n    let w = Wrapper(\n        vec![String::from(\"hello\"), String::from(\"world\")]\n    );\n    println!(\"w = {}\", w);\n}\n</code></pre> <p>which produces:</p> <pre><code>w = [hello, world]\n</code></pre> <p>The downside of this pattern is that wrapper is a new type so it's not possible to call method defined on <code>Vec</code> type or type stored inside the wrapper directly from Wrapper. However if we did want our new type to implement every method on the type it's holding then we can implement the <code>Deref</code> trait such that dereferncing the wrapper return the inner value.</p> <p>However if we only wanted our new type to have a subset of methods defined on the inner type then we'd have to implement each of those method manually</p>"},{"location":"cs/rust/tutorial/advanced/advanced-types/","title":"Advanced Types in Rust","text":"<p>Advanced Types </p>"},{"location":"cs/rust/tutorial/advanced/advanced-types/#newtype-pattern","title":"Newtype Pattern","text":"<p>In previous section we learned about Newtype Pattern in the context of implementing a trait on a given type.</p> <p>Newtype Pattern </p> <p>In this example we want to implement <code>Display</code> trait on a <code>Vector</code> type, however both are defined outisde of our trait. We get around this by defining a new Wrapper type, which a tuple struct containing a vector.</p> <pre><code>use std::fmt;\n\nstruct Wrapper(Vec&lt;String&gt;);\n\nimpl fmt::Display for Wrapper {\n    fn fmt(&amp;self, f: &amp;mut fmt::Formatter) -&gt; fmt::Result {\n        write!(f, \"[{}]\", self.0.join(\", \"))\n    }\n}\n\nfn main() {\n    let w = Wrapper(\n        vec![String::from(\"hello\"), String::from(\"world\")]\n    );\n    println!(\"w = {}\", w);\n}\n</code></pre> <p>Other uses for Newtype Pattern can be to increase type safety.</p> <p>For example, we have two functions, one function took an age as a paramter and another function took in an employee id as a parameter. The type of both parameters is <code>u32</code>. To avoid mixing of age and employee id when calling the function then we can create a new type which wraps an <code>u32</code>.</p> <p>Like, we can construct tuple struct for <code>Age</code> and <code>ID</code>:</p> <pre><code>struct Age(u32);\nstruct ID(u32);\n</code></pre> <p>Another use of Newtype Pattern is to abstract away implementation details.</p> <p>For example, we can create <code>People</code> type which wraps a hash map of integers to strings.</p> <p>In an essence, Newtype Pattern is a lightweight way to achieve encapsulation.</p>"},{"location":"cs/rust/tutorial/advanced/advanced-types/#type-aliases","title":"Type Aliases","text":"<p>Rust also allows to create type aliases to give existing types new names.</p> <pre><code>fn main() {\n    type Kilometers = i32; // not a new type; just a synonym\n\n    let x: i32 = 5;\n    let y: Kilometers = 5;\n\n    println!(\"x + y = {}\", x + y);\n}\n</code></pre> <p>The main usecase of type aliases is to reduce repetition:</p> <pre><code>fn main() {\n    // `f` of a very lengthy type\n    let f: Box&lt;dyn Fn() + Send + 'static&gt; =\n        Box::new(|| println!(\"hi\"));\n\n    fn takes_long_type(f: Box&lt;dyn Fn() + Send 'static&gt;) {\n        // --snip--\n    }\n\n    fn returns_long_type() -&gt; Box&lt;dyn Fn() + Send + 'static&gt; {\n        // --snip--\n    }\n}\n</code></pre> <p>Instead of writing this type over and over again, we can create type alias:</p> <pre><code>// much easier to read &amp; write\nfn main() {\n    type Thunk = Box&lt;dyn Fn() + Send + 'static&gt;;\n\n    let f: Thunk = Box::new(|| println!(\"hi\"));\n\n    fn takes_long_type(f: Thunk) {\n        // --snip--\n    }\n\n    fn returns_long_type() -&gt; Thunk {\n        // --snip--\n    }\n}\n</code></pre> <p>Type aliases also convey meaning for the type. For example, above here <code>Thunk</code> here is a word for code that will be evaluated at some later point.</p>"},{"location":"cs/rust/tutorial/advanced/advanced-types/#never-type","title":"Never Type","text":"<p>The Never Type is a special type denoted with <code>!</code> meaning that the the function will never return.</p> <pre><code>fn bar() -&gt; ! {\n    // --snip--\n}\n</code></pre> <p>Why this might be useful?</p> <p>Recall that in Chapter 2, we built a guessing game and we had some code which parsed user input into an integer, something like this:</p> <p><pre><code>fn main() {\n    while game_in_progress {\n        let guess: u32 = match guess.trim().parse() {\n            Ok(num) =&gt; num,\n            Err(_) =&gt; continue,\n        };\n    }\n}\n</code></pre> which took a <code>guess</code> and parse it and return a <code>u32</code> however if the parsing failed we call <code>continue</code> to skip this current iteration.</p> <p>But here <code>guess</code> is <code>u32</code>, so how one <code>match</code> arm returns a <code>u32</code> and other with <code>continue</code>.</p> <p>That's because <code>continue</code> has a Never Type. Rust will loook at both arms of this match expression:</p> <pre><code>Ok(num) =&gt; num,     // return u32\nErr(_) =&gt; continue, // never type, can never return\n</code></pre> <p>Thus. Rust confirms that the return type of this expression is an <code>u32</code> integer.</p> <p>If we get an <code>Err</code> variant in the above case, <code>continue</code> will not return anything, instead it will move the control back to the top of the loop, never assigning to <code>guess</code> and only assigning <code>u32</code>.</p> <p>*Never Type<code>is useful with</code>panic!` macro as well.</p> <p>For example the <code>Option&lt;T&gt;</code> enum has a method called <code>unwrap()</code> like this which evaluates <code>self</code>:</p> <pre><code>impl&lt;T&gt; Option&lt;T&gt; {\n    pub fn unwrap(self) -&gt; T {\n        match self {\n            Some(val) =&gt; val,  // returns val if `Some` variant\n            None =&gt; panic!(    // otherwise panic! won't return anything from this function\n                \"called `Option::unwrap()` on a `None` value\"\n            ),\n        }\n    }\n}\n</code></pre> <p><code>panic!</code> returns a Never Type.</p> <p>A loop also has Never Type</p> <p><pre><code>fn main() {\n    print!(\"forever \");\n\n    loop {\n        print!(\"and over \");\n    }\n}\n</code></pre> However this wouldn't be true if we had <code>break</code> statement inside of the loop because <code>break</code> will cause the loop to terminate.</p> <p><code>print!</code> is another macro with similar function to <code>println!</code> but without any line ending.</p>"},{"location":"cs/rust/tutorial/advanced/advanced-types/#dynamically-sized-types","title":"Dynamically Sized Types","text":"<p>Dynamically Sized Types or Usigned Type are types whose size we can only know at runtime.</p> <p>Example of this is <code>str</code> type.</p> <pre><code>fn main() {\n    let s1: str = \"Hello there!\";\n//      ^^ error: the size for values of type `str` cannot be known at compilation time\n    let s2: str = \"How's it going?\";\n}\n</code></pre> <p>For both <code>s1</code> and <code>s2</code> Rust can't determine the size of these store types at compile time. If we tried to compile this, we'll get the above error.</p> <p>Instead we need to use borrowed version of <code>str</code>, <code>&amp;str</code>:</p> <pre><code>fn main() {\n    let s1: &amp;str = \"Hello there!\";\n    let s2: &amp;str = \"How's it going?\";\n}\n</code></pre> <p>The string slice <code>&amp;str</code> stores two values: 1. An address pointing to the location of string in memory 2. The length of the string</p> <p>Both the address value and the length of the string have a type of <code>usize</code>, we know their size at compile time.</p> <p>This is how dynamically sized types are used in Rust. They have extra data structure which stores the size of the dynamic information.</p> <p>The golden rule for DSTs is that we should always put them behind some sort of pointer..</p> <p>In previous example <code>str</code> was behind <code>&amp;</code> (Reference) but <code>Box&lt;T&gt;</code> or <code>Rc&lt;T&gt;</code> would also have just worked fine.</p>"},{"location":"cs/rust/tutorial/advanced/advanced-types/#traits-and-dynamically-sized-types","title":"Traits and Dynamically Sized Types","text":"<p>Traits are also Dynamically Sized Types. Traits object always needs to be behind some sort of pointer.</p> <p>Rust has a special trait called the size trait to determine whether a type <code>Sized</code> can be known at compile time or not. The <code>Sized</code> trait is autmatically implemented for every type whose size is known at compile time.</p> <p>Rust implicitly adds the <code>Sized</code> trait bound to every generic function.</p> <pre><code>fn generic&lt;T&gt;(t: T) {\n    // --snip--\n}\n</code></pre> <p>Rust will automatically add <code>Sized</code> trait bound like so:</p> <pre><code>fn generic&lt;T: Sized&gt;(t: T) {\n    // --snip--\n}\n</code></pre> <p>By default generic functions will only work on types whose size is known at compile time, however we ca use the belo syntax to relax this retriction:</p> <pre><code>// `T` may be Sized or not\n// Since `T` can be unsized we have to put `T` behind some pointer\nfn generic&lt;T: ?Sized&gt;(t: &amp;T) {\n</code></pre>"},{"location":"cs/rust/tutorial/advanced/macros/","title":"Macros in Rust","text":"<p>Macros</p>"},{"location":"cs/rust/tutorial/advanced/macros/#macros-and-functions-difference","title":"Macros and Functions Difference","text":"<p>Macros allows us to write code which writes other code known as Meta Programming</p> <p>Think of macros kind of like a function where the input is code and output is also code that is transformed in some way.</p> <p>We've seen macros until now a lot of time:</p> <ul> <li><code>println!</code></li> <li><code>vec!</code></li> </ul> <p>By using macros we can reduce the amount of code we have to write and maintain which is similar to functions, but there are few differences:</p> <ul> <li>Functions must declare the number of parameters they could accept whereas macros can accept a variable number of parameters.</li> <li>Functions are called at runtime whereas macros are expanded before our program finishes compiling.</li> </ul> <p>So yes, macros are powerful but it also introduces complexity, since your code writes other code, meaning macros being harder to read, understand and maintain.</p> <p>Rust has two forms of macros: 1. Declarative macros 2. Procedural macros</p>"},{"location":"cs/rust/tutorial/advanced/macros/#declarative-macros","title":"Declarative Macros","text":"<p>Declarative macros are most common macros used across Rust code, allows us to write something similar to a match expression.</p> <p>For this example, we'll create a new project exactly like library crate we're using before with <code>lib.rs</code> as well:</p> <p><pre><code>.\n\u251c\u2500\u2500 Cargo.toml\n\u251c\u2500\u2500 .gitignore\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 lib.rs\n    \u2514\u2500\u2500 main.rs\n</code></pre> We can pass different type to <code>vec!</code> macro generating different <code>Vec&lt;T&gt;</code> types and pass in variable amount of arguments.</p> <pre><code>// main.rs\nfn main() {\n    let v1: Vec&lt;u32&gt; = vec![1, 2, 3];\n    let v2: Vec&lt;&amp;str&gt; = vec![\"a\", \"b\", \"c\", \"d\", \"e\"];\n}\n</code></pre> <p>In <code>lib.rs</code> we've implemented how <code>vec!</code> macro is implemented (a simplified version of <code>vec!</code> macro from std library):</p> <pre><code>// lib.rs\n#[macro_export]\nmacro_rules! vec {\n    ( $( $x:expr ),* ) =&gt; {\n        {\n            let mut temp_vec = Vec::new();\n            $(\n                temp_vec.push($x);\n            )*\n            temp_vec\n        }\n    };\n}\n</code></pre> <p>Let's go through it:</p> <ul> <li><code>#[macro_export]</code> means that whenever the crate in which this macro is defined is brought into scope this macro should be made available.</li> <li>start defining the macro with <code>macro_rules!</code> followed by name of our macro <code>vec</code> which then follows <code>{}</code> containing the body</li> <li> <p>The body is similar to match expression with one arm (but can have many),</p> <ul> <li><code>( $( $x:expr ),* )</code> is a pattern to match followed by a code block.</li> <li>If inputs to our macro match this pattern then the code runs otherwise error is generated since we have only one arm.</li> <li>The pattern syntax used in macros is different than the pattern syntax in match expression because we're matching against actual code versus value.</li> </ul> </li> </ul> <pre><code>  ( $( $x:expr ),* )\n//  ^^^^^^^^^^^^|\u2514\u2500\u2500 * to indicate our pattern can match zero or more times\n//  |           \u2514\u2500\u2500 suggest comma could appear after the code which matches the pattern\n//  | implies: capture any value that match the pattern inside the (),\n//  | for use in replacement code\n</code></pre> <p><code>$x:expr</code> matches any Rust expression and assigns it to variable <code>$x</code>.</p> <p>When calling our macro with input: <code>vec![1, 2, 3]</code> our pattern here is going to match three times once for every expression in the code we pass in.</p> <ul> <li>First, the <code>expr</code> will match with <code>1</code> assign it to <code>x</code>, then with <code>2</code> assign it to <code>x</code> and lastly match it with <code>3</code> and assign it to <code>x</code>.</li> <li>The body stores the code that's going to be generated. First we create a mutable vector <code>temp_vec</code>.</li> <li> <p>Then we have this, which says generate this line of code for every match that we get,</p> <p><pre><code>$(\n    temp_vec.push($x);\n)*\n</code></pre> with <code>$x</code> replaced with whatever we match on. So the output will look something like this:</p> <pre><code>{\n    let mut temp_vec = Vec::new();\n    temp_vec.push(1);\n    temp_vec.push(2);\n    temp_vec.puhs(3);\n    temp_vec\n}\n</code></pre> </li> </ul> <p>Obviously this is a very simple example of macros.</p> <p>This book should give you a deeper dive into writing macros in Rust:</p> <p>The Little Book of Rust Macros </p>"},{"location":"cs/rust/tutorial/advanced/macros/#procedural-macros","title":"Procedural Macros","text":"<p>Procedural Macros are like functions. They take code as input, operate on that code and produce code as output.</p> <p>In contrast, declarative macros match against which match against patterns and repace code with other code.</p> <p>Their are three kinds of procedural macros: 1. Custom Derived 2. Attribute like 3. Function like</p> <p>Procedural macros must be defined in their own crate with a custom crate type. All the three kinds of procedural macros are all defined using a similar syntax:</p> <pre><code>use proc_macro;\n\n#[some_attribute]\npub fn some_name(input: TokenStream) -&gt; TokenStream {\n    // ...\n}\n</code></pre> <p>The name of the function <code>some_name</code> is the name of our procedural macro and input is going to be <code>TokenStream</code>, the code on which we're operating on and the output is also going to be a <code>TokenStream</code>, the code macro is going to produce. - Tokens are smallest individual elements of a program, reperesenting keyword, identifiers, operators etc. Our function must also have an attribute which specifies what kind of procedural macro we're creating.</p> <p>So how can we make our own custom derived macro, <code>hello_macro</code> implementing a trait also named <code>hello_macro</code> which will have an associated function with a default implementation that prints \"hello macro\".</p> <p>This is how we'll use it:</p> <pre><code>// lib.rs\n\n// bringing our macro into scope\n// why we need two? we'll talk about that later\nuse hello_macro::HelloMacro;\nuse hello_macro_derive::HelloMacro;\n\n// derive attribute specifying our macro\n#[derive(HelloMacro)]\nstruct Pancakes;\n\nfn main() {\n    Pancakes::hello_macro();\n}\n</code></pre> <p>The derive attribute will implement our <code>hello_macro</code> trait for the <code>Pancakes</code> struct, allowing <code>hello_macro()</code> from <code>Pancake</code> struct.</p> <p>To implement our proc/procedural macro we'll create a new project in new directory:</p> <pre><code>cargo new hello_macro --lib\ncd hello_macro               # change directory\ncode .                       # open your editor\n</code></pre> <p>and edit the <code>lib.rs</code> in this directory:</p> <pre><code>// lib.rs\npub trait HelloMacro {\n    fn hello_macro();\n}\n</code></pre> <p>Our new macro has one associated function <code>hello_macro()</code>. Now we did wanted to have a default implementation but we have a problem. We could simply write a default implementation if all it would do was to print something.</p> <p>But imagine we wanted to do a little bit comlicated by printing \"hello macro\" followed by the type on which the trait was implemented (for example here it's <code>Pancakes</code>) on.</p> <p>Rust doesn't have Reflective capabilities so we can't look up the name of the type (<code>Pancakes</code>) at runtime.</p> <p>The solution is to use our macro to generate default implementation.</p> <p>We talked earlier that the procedural macros have to be defined in their own crate. So we're going to create a new library crate inside of our <code>hello_macro</code> crate:</p> <pre><code>cargo new hello_macro_derive --lib\n</code></pre> <p>This should create something like this:</p> <pre><code>hello_macro\n\u251c\u2500\u2500 hello_macro_derive\n\u2502   \u251c\u2500\u2500 src\n\u2502   \u2502   \u2514\u2500\u2500 lib.rs\n\u2502   \u251c\u2500\u2500 .gitignore\n\u2502   \u251c\u2500\u2500 target/\n\u2502   \u251c\u2500\u2500 Cargo.lock\n\u2502   \u2514\u2500\u2500 Cargo.toml\n\u251c\u2500\u2500 src\n\u2502   \u2514\u2500\u2500 lib.rs\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 target/\n\u251c\u2500\u2500 Cargo.lock\n\u2514\u2500\u2500 Cargo.toml\n</code></pre> <p>The naming convention when structuring crates and macros crates states, if we have a custom derived macro then we'll name the crate whatever our crate name was (in our case <code>hello_macro</code>) and append <code>_derive</code> (resulting in <code>hello_macro_derive</code>).</p> <p>Because these two crates are tightly coupled we created our macro crate inside of our library crate, although each crate has to be published separately and code using them has to bring each crate into scope.</p> <p>Let's check <code>Cargo.toml</code> file in our newly created <code>hello_macro_derive</code> crate and edit it to signify it's a special trait for macro</p> <pre><code>[package]\nname = \"hello_macro_derive\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n# enable this\n[lib]\nproc-macro = true\n\n# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html\n\n# add these dependencies\n[dependencies]\nsyn = \"1.0\"\nquote = \"1.0\"\n</code></pre> <p>Now let's open <code>hello_macro_derive/src/lib.rs</code> file and replace the code:</p> <pre><code>extern crate proc_macro;\n\nuse proc_macro::TokenStream;\nuse quoate::quote;\nuse syn;\n\n#[proc_macro_derive(HelloMacro)]\npub fn hello_macro_derive(input: TokenStream) -&gt; TokenStream {\n    // Construct a Abstract Syntax Tree of Rust code from TokenStream\n    // that we can manipulate\n    let ast = syn::parse(input).unwrap();\n\n    // Build the trait implementation\n    impl_hello_macro(&amp;ast)\n}\n</code></pre> <ul> <li><code>proc_macro</code> is a crate that comes with Rust, so don't have to declare it in <code>Cargo.toml</code> file but we do need to import it so we have <code>extern crate</code> statement.<ul> <li><code>proc_macro</code> crate allows us to read and manipulate Rust code</li> </ul> </li> <li><code>syn</code> crate (short for syntax) allows us to take a string of Rust code into a syntax tree data structure.</li> <li><code>quote</code> crate can take this syntax tree data structure and turn it back into Rust code.</li> <li>Then we have defined our custom derived macro broken into two parts:<ul> <li>first will be reponsible for parsing <code>TokenStream</code> input into a syntax tree and is going to be the same for almost all procedural macros</li> <li><code>impl_hello_macro()</code> function is reponsible for transforming that syntax tree The part which actually manipulates the syntax tree is going to be different.</li> </ul> </li> <li>The function is annotated with <code>proc_macro_derive</code> indicating this is a custom derived macro with the name <code>HelloMacro</code></li> </ul> <p>Let's see the <code>impl_hello_macro()</code> function written in continuation of the previous function:</p> <pre><code>fn impl_hello_macro(ast: &amp;syn::DeriveInput) -&gt; TokenStream {\n    let name = &amp;ast.ident;   // the name of the type we're working on\n    let gen = quote! {       // use quote macro to output some Rust code\n        impl HelloMacro for #name {\n            fn hello_macro() {\n                println!(\n                    \"Hello, Macro! My name is {}!\",\n                    stringify!(#name)\n                );\n            }\n        }\n    };\n    gen.into()\n}\n</code></pre> <p>Inside the <code>quote!</code> macro we just want to implement <code>HelloMacro</code> for our type indicated by template variable <code>#name</code> made available by <code>quote!</code> macro. This will be replaced by <code>quote!</code> macro. Then we provide a custom implementation for <code>hello_macro()</code> associated function. The <code>stringify!</code> macro will take an expression and turn it into a string without evaluating the expression (like the <code>format!</code> macro would do). At last we just get the outuput saved in <code>gen</code> and turn in <code>TokenStream</code> by calling <code>into()</code> method on it.</p> <p>Running <code>cargo build</code> to build <code>hello_macro_derive</code> crate.</p> <p>Then cd out and build <code>hello_macro</code> crate:</p> <pre><code>cd ..\ncargo build\n</code></pre> <p>Now we can switch back to our main project where we were testing our macros and add our macro crates as dependencies:</p> <p>Since we didn't published them, we just provide path to them.</p> <pre><code>[package]\nname = \"advanced\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html\n\n# add these dependencies\n[dependencies]\nhello_macro = { path = \"../hello_macro\" }\nhello_macro_derive = { path = \"../hello_macro/hello_macro_derive\" }\n</code></pre> <p>And this should compile fine with <code>cargo run</code>.</p>"},{"location":"cs/rust/tutorial/advanced/macros/#attribute-like-macros","title":"Attribute-like macros","text":"<p>These are similar to custom derived macros except instead of generating code for the derive attribute we can create a custom attribute.</p> <p>Also custom derived macros only work on structs &amp; enums whereas attribute-like macros work on other types such as functions.</p> <p>For example we're building a web framework and we want to create a new attribute called <code>route</code> which takes in https method and the route. <pre><code>// This macro will generate code which will map a specific\n// http request to a given function\n// a get request to the `index()` function\n#[route(GET, \"/\")]\nfn index() {\n    // ...\n}\n\n// annotation to define an attribute-like macro like this with\n#[proc_macro_attribute]\npub fn route(\n    attr: TokenStream, // contents of the attribute | GET, \"/\"\n    item: TokenStream, // contents of the item the attribute is attached to | fn index() {}\n) -&gt; TokenStream {\n    // ...\n}\n</code></pre></p>"},{"location":"cs/rust/tutorial/advanced/macros/#function-like-macros","title":"Function-like macros","text":"<p>They look like function calls however they are more flexible.  Firstly they could take a variable number of arguments and secondly they operate on Rust code.</p> <p>In this example we want to generate a function like macro sql which will SQL statement as as argument, validate it's syntax and generate code to allow us to execute that statement.</p> <pre><code>let sql = sql!(SELECT * FROM posts WHERE id=1);\n\n// the only difference being we annotate the function with `proc_macro`\n// instead of `proc_macro_derive`\n#[proc_macro]\npub fn sql(input: TokenStream) -&gt; TokenStream {\n    // ...\n}\n</code></pre>"},{"location":"cs/rust/tutorial/advanced/unsafe/","title":"Writing Unsafe Rust","text":"<p>Unsafe Rust</p>"},{"location":"cs/rust/tutorial/advanced/unsafe/#unsafe-superpowers","title":"Unsafe Superpowers","text":"<p>Up until now the code we have wrote followed Rust memory safety guidelines and checked at compile time.</p> <p>To opt out of these memory safety guarantees then we need to use unsafe Rust.</p> <p>This exist for two reasons:</p> <ol> <li> <p>Static Analysis is conservative by nature.</p> <p>Rust will reject a valid program if it can't guarantee that the program is memory safe even though you as a developer know it is. 2. Underlying computer hardware is inherently unsafe.</p> <p>If Rust can't allow you to do certain unsafe operations then you couldn't do certain tasks. Since it is systems programming language so it must allow you to do low-level systems programming which sometimes require unsafe code.</p> </li> </ol> <p>Unsafe code is written in <code>unsafe</code> block and gives you five abilities:</p> <ol> <li>Dereference a raw pointer</li> <li>Call an unsafe function or method</li> <li>Access or modify a mutable static variable</li> <li>Implement an unsafe trait</li> <li>Access fields of unions</li> </ol> <p>Although unsafe doens't disable borrow checker or disable Rust safety checks. For e.g., if you have a reference inside <code>unsafe</code> it'll still be checked.</p> <p>It's upto developer to make sure that memory inside <code>unsafe</code> is handled appropriately. Keep <code>unsafe</code> block small and manageable. We can also enclose unsafe code in a safe abstration and provide a safe API.</p>"},{"location":"cs/rust/tutorial/advanced/unsafe/#dereferncing-a-raw-pointer","title":"Dereferncing a Raw Pointer","text":"<p>The compiler ensures references are valid.</p> <p>Dandling References</p> <p>Unsafe Rust has two types of raw pointers similar to references.   T 1. Immutable raw pointer (<code>*const &lt;T&gt;</code>)     The pointer can't be directly assigned after it has been dereferenced. 2. Mutable raw pointer (<code>*mut &lt;T&gt;</code>)</p> <p>The <code>*</code> here isn't dereference operator.</p> <p><pre><code>fn main() {\n    let mut num = 5;\n\n    let r1 = &amp;num as *const i32;   // Immutable raw pointer\n    let r2 = &amp;mut num as *mut i32; // Mutable raw pointer\n}\n</code></pre> We don't use <code>unsafe</code> keyword here. That's because Rust allows us to create raw pointers but doesn't allow us to dereference them unless done in <code>unsafe</code> block.</p> <p>The <code>as</code> keyword is used to cast a immutable reference and mutable reference to their corresponding raw pointer types.</p> <p>Raw pointer are different from Smart pointers: - Raw pointers are allowed to ignore Rust borrowing rules by having mutable and immutable pointer or multiple mutable pointers to the same location in memory. - Raw pointers are also not guaranteed to point to valid memory. - Raw pointer are allowed to be null. - Raw pointer don't implement any automatic cleanup.</p> <p>We do know that the raw pointers we created above are valid, but that won't be true always incase of raw pointers. for e.g., let create a pointer to an arbitrary memory address:</p> <pre><code>fn main() {\n    let address = 0x012345usize;\n    let r3 = address as *const i32;\n}\n</code></pre> <p>In this example there might be valid memory or there might not be at that <code>address</code>. This can lead to undefined behavior. Compiler might try to optimize the code such that there is no memory access or we may get segmentation fault.</p> <p>Let's try to dereference our original <code>r1</code> &amp; <code>r2</code> raw pointers by using <code>unsafe</code>:</p> <pre><code>fn main() {\n    let mut num = 5;\n\n    let r1 = &amp;num as *const i32;\n    let r2 = &amp;mut num as *mut i32;\n\n    unsafe {\n        println!(\"r1 is: {}\", *r1);\n        println!(\"r2 is: {}\", *r2);\n    }\n}\n</code></pre> <p>Running this works fine:</p> <pre><code>r1 is: 5\nr2 is: 5\n</code></pre> <p>If we were to create an immutable and mutable reference to the same location in memory then the program would not compile because that would violate ownership rules. Raw pointers allow us to bypass those rules but can lead to data races.</p>"},{"location":"cs/rust/tutorial/advanced/unsafe/#calling-unsafe-function-or-method","title":"Calling Unsafe Function or Method","text":"<p>Unsafe functions or methods look the same as regular functions or methods, except they have a <code>unsafe</code> keyword at the beginning of their definition.</p> <p><code>unsafe</code> in this context means that the function requires correct arguments otherwise it could lead to undefined behavior.</p> <p>So do make sure to read documentation suggesting the requirements of unsafe function for upholding the functions contracts.</p> <p>Unsafe functions must be called inside other <code>unsafe</code> functions or inside <code>unsafe</code> block.</p> <pre><code>fn main() {\n    unsafe fn dangerous() {\n        // no need for unsafe block inside unsafe function\n    }\n\n    unsafe {\n        dangerous();\n    }\n\n    // unsafe {\n        dangerous();\n    //  ^^^^^^^^^^^ error: this operation is unsafe and requires an unsafe function or block\n    // }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/advanced/unsafe/#creating-a-safe-abstration","title":"Creating a Safe Abstration","text":"<p>Just because a function contains unsafe code doesn't make it an unsafe function.</p> <p>You can wrap unsafe code inside a safe one.</p> <pre><code>fn main() {\n    let mut v = vec![1, 2, 3, 4, 5, 6];\n\n    let r = &amp;mut v[..]; // create a mutable slice `r` of `v`\n\n    let (a, b) = r.split_at_mut(3); // this will return tuple\n\n    assert_eq!(a, &amp;mut [1, 2, 3]);\n    assert_eq!(b, &amp;mut [4, 5, 6]);\n}\n</code></pre> <p><code>split_at_mut()</code> is a safe method implemented on mutable slices which will split the slice into two slices along the index passed in.</p> <p>Imagine we wanted to split this function using only safe Rust code. Which might look something like this:</p> <pre><code>// for simplicity we implement function not a method\nfn slit_at_mut(sliceL &amp;mut [i32], mid: usize) -&gt; (&amp;mut [i32], &amp;mut [i32]) {\n    let len = slice.len();\n\n    assert_eq!(mid &lt;= len);\n\n    // return a tuple with two slices\n    // first slice everything upto midpoint, second slice everything after midpoit\n    (&amp;mut slice[..mid], &amp;mut slice[mid..])\n    //    ^^^^^              ^^^^^\n    // error: cannot borrow `*slice` as mutable more than once a time\n}\n</code></pre> <p>When creating the tuple we're immutably borrowing slice twice int the same scope. The borrow checker doesn't know we're borrowing different parts of the slice.</p> <p>We know it valid so let's use <code>unsafe</code> block here:</p> <p><pre><code>use std::slice;\n\nfn split_at_mut(slice: &amp;mut [i32], mid: usize) -&gt; (&amp;mut [i32], &amp;mut [i32]) {\n    let len = slice.len();\n    let ptr = slice.as_mut_ptr();  // get a raw mutable pointer\n\n    assert_eq!(mid &lt;= len);\n\n    unsafe {\n        (\n            // unsafe fn; create a new slice taking pointer to data and length\n            slice::from_raw_parts_mut(ptr, mid),\n            slice::from_raw_parts_mut(\n                // ptr.add() return a pointer at a given offset\n                ptr.add(mid), len - mid\n            )\n        )\n    }\n}\n</code></pre> We know that slices are a pointer to some data and the length of that data.</p> <p>The Slice Type</p> <p><code>ptr.add()</code> and <code>slice::from_raw_parts_mut()</code> are unsafe because it expects the poiter passed in to be valid. The function <code>split_at_mut()</code> itself is safe and can be called from safe Rust code.</p>"},{"location":"cs/rust/tutorial/advanced/unsafe/#extern-functions-to-call-external-code","title":"extern Functions to Call External Code","text":"<p>Sometimes our Rust code may need to interact with code in different language.</p> <p>For this purpose Rust has <code>extern</code> keyword which facilitates the creation and use of foreign function interface, FFI.</p> <p>A foreign language interface is a way for a programming language to define a function that another language or a foreing language could call.</p> <pre><code>extern \"C\" {\n    fn abs(input: i32) -&gt; i32;\n}\n\nfn main() {\n    unsafe {\n        println!(\"Absolute value of -3 according to C: {}\", abs(-3));\n    }\n}\n</code></pre> <p>Here we set up an integration with <code>abs()</code> function from C standard library. Calling a function defined withing an <code>extern</code> block 'cause we don't know the language we're calling into has the same rules and guarantees as Rust. <p>It's developer responsibility that functions defined an extern block are safe to call.</p> <ul> <li>Inside <code>extern</code> we specify the name and signature of the foreign function we want to call.</li> <li> <p>The <code>\"C\"</code> defines which Application Binary Interface or ABI the external function uses. ABI defines how to call the function at the assembly level.</p> <p>The <code>\"C\"</code> ABI is the most common ABI and follows the C programming lagnuage API.</p> </li> </ul> <p>We can also allow other languages to call our Rust functions by using the <code>extern</code> keyword in the function signature:</p> <p><code>#[no_mangle]</code> annotation is required to let the Rust compiler know not to mangle the name of our function. Mangling is when the compiler changes the name of a function to give it more informatioin for other parts of the compilation process.</p> <pre><code>#[no_mangle]\npub exter \"C\" fn call_from_c() {\n    println!(\"Just called a Rust function from C!\");\n}\n</code></pre>"},{"location":"cs/rust/tutorial/advanced/unsafe/#accessing-or-modifying-mutable-static-variable","title":"Accessing or Modifying Mutable Static Variable","text":"<p>Uptil now we haven't talked about Global variables in Rust; although are supported but can cause problem with Rust ownership rules.</p> <p>If two threads are accessing the same mutable global state then it could cause a data race.</p> <p>In Rust global Variables are called Static Variables</p> <pre><code>// naming convention is to use screaming snakecase with type annotation\n// with static lifetime\nstatic HELLO_WORLD: &amp;str = \"Hello, world!\";\n\nfn main() {\n    println!(\"name is: {}\", HELLO_WORLD);\n}\n</code></pre> <p>Constants and immutable static variables are similar with the difference being static variables have fixed address in memory. Constants are allowed to duplicate their data whenever they are used. Compiler can replace all the ocuurence of constants with concrete value.</p> <p>Static variables can be mutable but accessing and modifyig mutable static variables is unsafe.</p> <pre><code>static mut COUNTER: u32 = 0;\n\nfn add_to_count(inc: u32) {\n    // modification is unsafe\n    unsafe {\n        COUNTER += inc;\n    }\n}\n\nfn main() {\n    add_to_count(3);\n\n    // accessing is unsafe\n    unsafe {\n        println!(\"COUNTER: {}\", COUNTER);\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/advanced/unsafe/#implementing-an-unsafe-trait","title":"Implementing an Unsafe Trait","text":"<p>A trait it unsafe when at least one of it's method is unsafe.</p> <pre><code>unsafe trait Foo {\n    // methods go here\n}\n\nunsafe impl Foo for i32 {\n    // method implementation go here\n}\n\nfn main() {}\n</code></pre>"},{"location":"cs/rust/tutorial/advanced/unsafe/#accessing-fields-of-a-union","title":"Accessing Fields of a Union","text":"<p>A union is similar to struct but only one field is used for each instance. Unions are primarily used to interface with C unions and it's unsafe to access fields of a union because Rust cannot guarantee what the tyoe of data stored in the union is for given instance.</p>"},{"location":"cs/rust/tutorial/advanced/unsafe/#whe-to-use-unsafe-code","title":"Whe to Use Unsafe Code","text":"<p>Using unsafe isn't wrong or goes against the belief of Rust. But it's a developer responsibility to know what he/she is doing.</p>"},{"location":"cs/rust/tutorial/cargo/","title":"Index","text":"<p>title: \"Cargo\" description: \"\" lead: \"\" date: 2022-09-20T07:58:11+01:00 lastmod: 2022-09-20T07:58:11+01:00 draft: false images: [] weight: 17</p>"},{"location":"cs/rust/tutorial/cargo/cargo-workspaces/","title":"Cargo Workspaces","text":"<p>What happens when the program keep growing and have multiple library crates? That's where cargo workspace comes for the rescue.</p> <ul> <li>Workspaces help you manage multiple related packages that are being developed</li> <li>Packages in a workspace share common dependency resolution, since they have only one <code>Cargo.lock</code> file.</li> <li>Packages in a workspace also share one output directory and release profiles.</li> </ul> <p>Cargo Workspaces</p>"},{"location":"cs/rust/tutorial/cargo/cargo-workspaces/#creating-a-workspace","title":"Creating a Workspace","text":"<p>We are going to create one binary that depends on one library. - First library having <code>add_one</code> function.</p> <p>Begin by creating a directory and opening it in an editor:</p> <pre><code>mkdir add\ncd add\ncode .\n</code></pre> <p>Next we'll add a <code>Cargo.toml</code> file to configure our workspace, by creating a <code>worspace</code> section instead of a <code>package</code> section. - Then we'll specify the members of the workspace called workspace members specifying the path to the packages.</p> <pre><code>[workspace]\n\nmembers = [\n  \"adder\"\n]\n</code></pre> <p>Then we'll add a new package <code>adder</code> and even try building it:</p> <pre><code>cargo new adder\ncargo build\n</code></pre> <p>The newly created package will not have a target folder or <code>Cargo.lock</code> file but instead at root of our workspace, signifying the packages in a workspace are meant to depend on each other. Meaning if each package had its own target directory, when you would compile that package, you would have to also compile all its dependencies. But now they are combinely managed in a single workspace with same dependencies, reducing the amount of compilation required.</p>"},{"location":"cs/rust/tutorial/cargo/cargo-workspaces/#creating-second-pacakge-in-workspace","title":"Creating Second pacakge in Workspace","text":"<p>Update the root <code>Cargo.toml</code> file:</p> <pre><code>[workspace]\n\nmembers = [\n  \"adder\",\n  \"add-one\",\n]\n</code></pre> <p>We'll add a second package <code>add-one</code> and specifying <code>--lib</code> for library:</p> <pre><code>cargo new add-one --lib\n</code></pre> <p>Update the <code>add-one/src/lib.rs</code> file:</p> <pre><code>pub fn add_one(x: i32) -&gt; i32 {\n    x +1\n}\n</code></pre> <p>Next, we need to specify that our <code>adder</code> binary depends on <code>add-one</code> library. We'll do this by updating <code>adder</code>'s <code>Cargo.toml</code> file: - Cargo by default don't assume that crates within a workspace depend on each other.</p> <pre><code>[dependencies]\nadd-one = { path = \"../add-one\" }\n</code></pre> <p>Now we can use our newly created library in <code>adder</code> binary, in <code>adder/src/main.rs</code>:</p> <pre><code>use add_one;\n\nfn mian() {\n  let num = 10;\n  println!(\n      \"Hello, world! {} plus one is {}!\",\n      num,\n      add_one::add_one(num)\n  );\n}\n</code></pre> <p>To build our workspace run the build command from the root of workspace:</p> <pre><code>cargo build\n</code></pre> <p>Next we can the adder binary from the root of our workspace by running:</p> <pre><code>cargo run --package adder\n</code></pre>"},{"location":"cs/rust/tutorial/cargo/cargo-workspaces/#external-dependencies","title":"External dependencies","text":"<p>Since all the packages uses one single <code>Cargo.lock</code> for dependency resolution, this ensures that the packages are compatible with each other. - If we add a dependency to <code>add-one</code> package and <code>adder</code> package, they both will resolve to the same version.</p> <p>If we add a <code>rand</code> dependency to <code>Cargo.toml</code> file of <code>add-one</code> package and use it somewhere in <code>lib.rs</code>:</p> <pre><code>[dependencies]\nrand = \"0.8.3\"\n</code></pre> <pre><code>use rand;\n\npub fn add_one(x: i32 ) -&gt; i32 {\n  // ...\n</code></pre> <p>Then from the root of the package we can build the workspace: <code>cargo build</code> which adds <code>rand</code> as a dependency for the <code>add-one</code> package.</p> <p>We can't use the <code>rand</code> dependency however in <code>adder</code> package until we add it as a dependency for <code>adder</code> in it's <code>Cargo.toml</code> file:</p> <pre><code>[dependencies]\nadd-one = { path = \"../add-one\" }\nrand = \"0.8.3\"\n</code></pre>"},{"location":"cs/rust/tutorial/cargo/cargo-workspaces/#adding-a-test-to-a-workspace","title":"Adding a Test to a Workspace","text":"<p>Let's add one test module inside <code>lib.rs</code> file of <code>add-one</code> package:</p> <pre><code>pub fn add_one(x: i32) -&gt; i32 {\n  //...\n  // ..\n\n#[cfg(test)]\nmod tests {\n  use super::*;\n\n  #[test]\n  fn it_works() {\n    assert_eq!(3, add_one(2));\n  }\n}\n</code></pre> <p>Now run <code>cargo test</code> from workspace root. This will run all package-tests and documentation tests.</p> <p>If we wanted to run tests for a specific package we could run it using <code>-p</code> or <code>--package</code> option specfying the specific package:</p> <pre><code>cargo test -p add-one\n</code></pre> <p>To publish a package from a workspace we have to do it individually for each package.</p>"},{"location":"cs/rust/tutorial/cargo/cargo-workspaces/#installing-binaries-from-cratesio","title":"Installing Binaries from Crates.io","text":"<p>Although this isn't meant as a replacement for package managers like <code>dnf</code> or <code>apt</code> but as a convenient tool to install tools published on crates.io having a binary target. - All binaries are stored in Rust installation routes bin directory.</p> <p>If installed via <code>rustup</code> this would be the path to that bin directory (can be requirement to add it to PATH, so that other programs use the Rust installed binaries):</p> <pre><code>~/.cargo/bin\n</code></pre> <p>Let try this out by installing <code>riprep</code>, the implementation of <code>grep</code> in Rust.</p> <pre><code>cargo install ripgrep\n</code></pre> <p>!! info \"Extending Cargo\"</p> <pre><code>If you have a binary named starting with `cargo-`, say `cargo-something`, then this can be used by `cargo` as a command (sub command) to extend its functionality:\n\n```bash\ncargo something\n```\n</code></pre>"},{"location":"cs/rust/tutorial/cargo/publishing-crate/","title":"Publishing a Rust Crate","text":"<p>More About Cargo and Crates.io</p>"},{"location":"cs/rust/tutorial/cargo/publishing-crate/#release-profiles","title":"Release Profiles","text":"<p>Release profiles allow us to configure how the code is compiled.</p> <p>Cargo has two main profiles: - <code>dev</code>: provide good defaults out of the box for development</p> <pre><code>`cargo build` compiles and build with `dev` profile. `dev` build is also unoptimized and contains debug information\n</code></pre> <ul> <li> <p><code>release</code>: good defaults for release build</p> <p>To build with release profile: <code>cargo build --release</code></p> </li> </ul> <p>We can customize these settings in <code>Cargo.toml</code> file:</p> <pre><code>[package]\nname = \"...\"\n...\n\n[dependencies]\n\n[profile.dev]\nopt-level = 0\n\n[profile.release]\nopt-level = 3\n</code></pre> <p>Here we added two sections for dev and release profile specifying <code>opt-level</code> settings which stands for optimization level, ranging from 0 to 3, 0 being no optimization and 3 for highest level of optimization.</p> <p>So during development stages, we want the compilation to be as fast as possible trading off runtime performance with no-optimization and vice-versa during release.</p> <p>For full list of all settings for profile section, refer documentation:</p> <p>Profiles</p>"},{"location":"cs/rust/tutorial/cargo/publishing-crate/#documentation-comments","title":"Documentation Comments","text":"<p>Documentation comments are useful when documenting your API for documentation purposes.</p> <p>Documentation comments starts with three slashes, supports markdown and Rust can convert these comments into deployable Rust HTML documentation.</p> <pre><code>/// Adds one to the number given\n///\n/// # Examples\n///\n/// ```\n/// let arg = 5;\n/// let answer = my_crate::add_one(arg);\n///\n/// assert_eq!(6, answer);\n/// ```\npub fn add_one(x: i32) -&gt; i32 {\n    x + 1\n}\n</code></pre> <p>To build our doc (for testing purpose <code>open</code> in a web browser), open up a terminal and fire this command:</p> <pre><code>cargo doc --open\n</code></pre>"},{"location":"cs/rust/tutorial/cargo/publishing-crate/#commonly-used-sections-in-documentation","title":"Commonly used sections in documentation","text":"<p>Here are some other sections (created using <code>#</code> as in <code>/// # Examples</code> ) that crate developers commonly use:</p> <ul> <li>Panics: scenario in which functions would panic, so caller of the function don't use the function in those scenarios</li> <li>Errors: what kind of <code>Err</code> are generated when the function returns a <code>Result</code> and in which conditions. So callers can handle the error.</li> <li>Safety: If the function is <code>unsafe</code> to call, this section should describe why the function is unsafe and covering the invariants that the function expects callers to uphold.</li> </ul>"},{"location":"cs/rust/tutorial/cargo/publishing-crate/#documentation-comments-as-tests","title":"Documentation Comments as Tests","text":"<p>The documentation tests are signified by the <code>assert_eq!</code> macro and runs when you run <code>cargo test</code> to test the code credibility. This keeps the documentation and code in sync.</p>"},{"location":"cs/rust/tutorial/cargo/publishing-crate/#commenting-contained-items","title":"Commenting Contained Items","text":"<p>This style of comments documents the item containing the comment:</p> <pre><code>//! # My Crate\n//!\n//! `my_create` is a collection of utilities to make performing certain\n//! calculations more convenient\n</code></pre>"},{"location":"cs/rust/tutorial/cargo/publishing-crate/#exporting-a-public-api","title":"Exporting a Public API","text":"<p>Let's create a new crate called <code>adhadse_art</code> which has a library crate: <code>lib.rs</code> consisting of two modules: <code>kinds</code> &amp; <code>utils</code>:</p> <p>In <code>lib.rs</code>:</p> <pre><code>//! # Adhadse Art\n//!\n//! A library for modeling artistic concepts.\n\npub mod kinds {\n    /// The primary colors according to the RYB color model.\n    pub enum PrimaryColor {\n        Red,\n        Yellow,\n        Blue,\n    }\n\n    /// The secondary colors according to the RYB color model\n    pub enum SecondaryColor {\n        Orange,\n        Green,\n        Purple,\n    }\n}\n\npub mod utils {\n    use crate::kinds::*;\n\n    /// Combines two primary colors in equal amounts to create\n    /// a secondary color.\n    pub fn mix(c1: PrimaryColor, c2: PrimaryColor) -&gt; SecondaryColor {\n        // --snip--\n        // ANCHOR_END: here\n        SecondaryColor::Orange\n        // ANCHOR: here\n    }\n}\n</code></pre> <p>To access the two enums, we might do something like this:</p> <pre><code>use adhadse_art::kinds::PrimaryColor;\nuse adhadse_art::utils::mix;\n\nfn main() {\n    let red = PrimaryColor::Red;\n    let yellow = PrimaryColor::Yellow;\n    mix(red, yellow);\n}\n</code></pre> <p>This structure might make sense to us, the developer internally.</p> <p>But what if we wanted to give access to these function &amp; enums to users at the top level without referencing the respective modules?</p> <p>To do this go to <code>lib.rs</code> and reexport the enums and the <code>mix()</code> function:</p> <pre><code>//! calculations more convenient\n\npub use self::kinds::PrimaryColor;\npub use self::kinds::SecondaryColor;\npub use self::utils::mix;\n\npub mod kinds {\n    // ...\n    // ...\n</code></pre> <p>then we can use these items from top level directly in <code>main.rs</code>:</p> <pre><code>use adhadse_art::PrimaryColor;\nuse adhadse_art::mix;\n\nfn main() {\n    // ...\n}\n</code></pre>"},{"location":"cs/rust/tutorial/cargo/publishing-crate/#setting-up-cratesio-account","title":"Setting up Crates.io account","text":"<ol> <li>Login into crates.io with your GitHub account</li> <li>Go to \"Account Settings\" and click create \"New Token\" for API Access after givng the token a name.</li> <li>This will create a new token, which should be kept private.</li> <li>Copy the command and run it from the project's dir to save it to <code>~/.cargo/credentials</code> and login.</li> <li>We can now publish our crate but not before checking the metadata.</li> </ol>"},{"location":"cs/rust/tutorial/cargo/publishing-crate/#adding-metadata-to-a-new-crate","title":"Adding Metadata to a New Crate","text":"<p>The metadata for crate is stored in <code>Cargo.toml</code> file at the top, something like this at top level: - Keep in mind that the crate name should be unique, should not conflict with any other crate already published on crates.io.</p> <pre><code>[package]\nname = \"adhadse_art\"\nversion = \"0.1.0\"\nauthors = [\"Anurag Dhadse\"]\nedition = \"2021\"\n</code></pre> <p>Before we can publish, we'll need to add <code>description</code> and <code>license</code> fields, so let's quickly add that following other fields:</p> <pre><code>description = \"A library for modeling artistic concepts\"\nlicense = \"MIT\"\n</code></pre> <p>and then publish it:</p> <pre><code>cargo publish\n</code></pre>"},{"location":"cs/rust/tutorial/cargo/publishing-crate/#removing-version-from-cratesio","title":"Removing Version from Crates.io","text":"<p>Although you can't delete the crate once its published, to avoid breaking dependencies other project might be using. But we can stop versions from being downloaded, in case of a security flaw or discontinuation.</p> <p>To do this use <code>cargo yank</code> command passing it the version to yank:</p> <pre><code>cargo yank --vers 0.1.0\n</code></pre> <p>Now, those who is using this specific version right now, will be allowed to download the library, BUT for anyone who want to use it for the first time won't be allowed to download.</p> <p>To undo this, simply pass in <code>--undo</code>:</p> <pre><code>cargo yank --vers 0.1.0 --undo\n</code></pre>"},{"location":"cs/rust/tutorial/cli-app/","title":"CLI App","text":""},{"location":"cs/rust/tutorial/cli-app/cli-app-in-rust-2/","title":"CLI App in Rust - Part 2","text":"<p>Developing the Library\u2019s Functionality with Test-Driven Development</p>"},{"location":"cs/rust/tutorial/cli-app/cli-app-in-rust-2/#test-driven-development","title":"Test Driven Development","text":"<p>Let's move our <code>run()</code>, <code>Config</code> struct and it's <code>impl</code> block into a library crate and declutter our <code>main.rs</code>.</p> <p>Create a <code>src/lib.rs</code>.</p> <pre><code>nano src/lib.rs\n</code></pre> <p>And move those functions we discssed above in this file (and also move relevant import statements):</p> <ul> <li>We'll also want to make our <code>run()</code> function, <code>Config</code> struct and it's fields as well as <code>new()</code> associated function in <code>Config</code>'s <code>impl</code> block public so that anything outside our library crate can use that.</li> </ul> <pre><code>// src/lib.rs\nuse std::fs;\nuse std::error::Error;\n\npub fn run(config: Config) -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {\n    let contents = fs::read_to_string(config.filename)?;\n\n    println!(\"With text:\\n{}\", contents);\n\n    Ok(())\n}\n\npub struct Config {\n    pub query: String,\n    pub filename: String\n}\n\nimpl Config {\n    pub fn new(args: &amp;[String]) -&gt; Result&lt;Config, &amp;str&gt; {\n        if args.len() &lt; 3 {\n            return Err(\"not enough arguments\");\n        }\n\n        let query = args[1].clone();\n        let filename = args[2].clone();\n\n        Ok(Config { query, filename })\n    }\n}\n</code></pre> <p>Now in <code>main.rs</code> we're going to import these and use them:</p> <pre><code>// src/main.rs\nuse std::env;\nuse std::process;\n\nuse minigrep::Config; // import with `minigrep`; the name of our crate\n\nfn main() {\n    let args: Vec&lt;String&gt; = env::args().collect();\n\n    let config = Config::new(&amp;args).unwrap_or_else(|err| {\n        println!(\"Problem parsing arguments: {}\", err);\n        process::exit(1); // pass in status code 1\n    });\n\n    println!(\"Searching for {}\", config.query);\n    println!(\"In file {}\", config.filename);\n\n    // use minigrep::run()\n    if let Err(e) = minigrep::run(config) {\n        println!(\"Application error: {}\", e);\n        process::exit(1);\n    }\n}\n</code></pre> <p>Now let's test this out with <code>cargo run the poem.txt</code></p> <pre><code>   Compiling minigrep v0.1.0 (/home/adhadse/Downloads/minigrep)\n    Finished dev [unoptimized + debuginfo] target(s) in 0.70s\n     Running `target/debug/minigrep the poem.txt`\nSearching for the\nIn file poem.txt\nWith text:\nI'm nobody! Who are you?\nAre you nobody, too?\nThen there's a pair of us - don't tell!\nThey'd banish us, you know.\n\nHow dreary to be somebody!\nHow public, like a frog\nTo tell your name the livelong day\nTo an admiring bog!\n</code></pre> <p>And it compiles successfully and runs just as fine!</p>"},{"location":"cs/rust/tutorial/cli-app/cli-app-in-rust-2/#writing-tests","title":"Writing tests","text":"<p>We'll write a test that fails then implement the logic that'll make the test pass and if necessary refactor our code and make sure our test is still passing.</p> <ul> <li>Currently, our program just spits out the content of the file. Instead we want it to search for query string inside the file and only print out line that contain our query. For that we'll create a function <code>search()</code> but before as said, we'll write a test to test that functionality (which obviously will fail since the functionality isnt't implemented yet).</li> </ul> <pre><code>// src/lib.rs\nimpl Config {\n    // ...\n}\n\npub fn search(query: &amp;str, contents: &amp;str) -&gt; Vec&lt;&amp;str&gt; {\n//             error: missing lifetime specifier  ^\n// this function's return type contains a borrowed value but the signature\n// does not say whether it is borrowed from `query` or `contents`.\n    vec![]\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn one_result() {\n        let query = \"duct\";\n        let contents = \"\\\nRust:\nsafe, fast, productive.\nPick three.\";\n\n        assert_eq!(vec![\"safe, fast, productive.\"], search(query, contents));\n    }\n}\n</code></pre> <p>For the above error, we know how to fix it, by specifying the lifetimes associated for the output value with one the input parameters:</p> <p>Info</p> <p>Anytime we return a reference from a function we have to tie the lifetime of that reference to the lifetime of one of the input parameters.</p> <p>So we want the returned vector to have a lifetime tied to the <code>contents</code> input parameter, because retrun strings will simply be lines inside the contents string.</p> <pre><code>pub fn search&lt;'a&gt;(query: &amp;str, contents: &amp;'a str) -&gt; Vec&lt;&amp;'a str&gt; {\n    vec![]\n}\n</code></pre> <p>Now if we run <code>cargo test</code>, as expected the tests fails:</p> <pre><code>    Finished test [unoptimized + debuginfo] target(s) in 0.44s\n     Running unittests src/lib.rs (target/debug/deps/minigrep-41ffe2f4028fe4d9)\n\nrunning 1 test\ntest tests::one_result ... FAILED\n\nfailures:\n\n---- tests::one_result stdout ----\nthread 'tests::one_result' panicked at 'assertion failed: `(left == right)`\n  left: `[\"safe, fast, productive.\"]`,\n right: `[]`', src/lib.rs:44:9\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n\n\nfailures:\n    tests::one_result\n\ntest result: FAILED. 0 passed; 1 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n</code></pre> <p>Let's write test that will make this test pass:</p> <pre><code>// search() return a Vec of string, where each string is a line which\n// contains our query\npub fn search&lt;'a&gt;(query: &amp;str, contents: &amp;'a str) -&gt; Vec&lt;&amp;'a str&gt; {\n    let mut results = Vec::new();\n\n    for line in contents.lines() {\n        if line.contains(query) {\n            results.push(line);\n        }\n    }\n\n    results\n}\n</code></pre> <p>now, if we run our test, we get passing test:</p> <pre><code>   Compiling minigrep v0.1.0 (/home/adhadse/Downloads/minigrep)\n    Finished test [unoptimized + debuginfo] target(s) in 0.35s\n     Running unittests src/lib.rs (target/debug/deps/minigrep-41ffe2f4028fe4d9)\n\nrunning 1 test\ntest tests::one_result ... ok\n\ntest result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n\n     Running unittests src/main.rs (target/debug/deps/minigrep-1d6aff2511469b21)\n\nrunning 0 tests\n\ntest result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n\n   Doc-tests minigrep\n\nrunning 0 tests\n\ntest result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n</code></pre> <p>Now, let's use our tested <code>search()</code> function inside <code>run()</code> function:</p> <pre><code>// src/lib.rs\npub fn run(config: Config) -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {\n    let contents = fs::read_to_string(config.filename)?;\n\n    // println!(\"With text:\\n{}\", contents);\n    for line in search(&amp;config.query, &amp;contents) {\n        println!(\"{}\", line);\n    }\n\n    Ok(())\n}\n</code></pre> <p>When we run our program: <code>cargo run the poem.txt</code>:</p> <pre><code>   Compiling minigrep v0.1.0 (/home/adhadse/Downloads/minigrep)\n    Finished dev [unoptimized + debuginfo] target(s) in 0.37s\n     Running `target/debug/minigrep the poem.txt`\nSearching for the\nIn file poem.txt\nThen there's a pair of us - don't tell!\n</code></pre> <p>If we test it for a query that doesn't exist: <code>cargo run dog poem.txt</code>, we get 0 line print out:</p> <pre><code>   Compiling minigrep v0.1.0 (/home/adhadse/Downloads/minigrep)\n    Finished dev [unoptimized + debuginfo] target(s) in 0.37s\n     Running `target/debug/minigrep the poem.txt`\nSearching for dog\nIn file poem.txt\n</code></pre>"},{"location":"cs/rust/tutorial/cli-app/cli-app-in-rust-2/#environment-variables","title":"Environment Variables","text":"<p>Currently our <code>search()</code> function logic is case sensitive, so we'll add an option to perform case insensitive searching, but instead of using command line arguments we'll use environment variables (to learn about them).</p> <p>So, again let's write a new test for case insensitivity that for now fails:</p> <ul> <li>Also change first test case name from <code>one_test()</code> to <code>case_sensitive()</code>.</li> </ul> <pre><code>// src/lib.rs\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    // fn one_result() {\n    fn case_sensitive {    // change first test case name\n        let query = \"duct\";\n\n        // and add one more line to make sure the function\n        // don't return last line since it contains `Duct` for case sensitive case.\n        let contents = \"\\\nRust:\nsafe, fast, productive.\nPick three.\nDuct tape.\";\n        // ...\n    }\n\n    #[test]\n    fn case_insensitive() {\n        let query = \"rUsT\";\n        // two occurence first line: `Rust` and last line in `Trust`\n        let contents = \"\\\nRust:\nsafe, fast, productive.\nPick three.\nTrust me.\";\n\n        assert_eq!(\n            vec![\"Rust:\", \"Trust me.\"],  // we expect this function to return\n            search_case_insensitive(query, contents)\n        );\n    }\n}\n</code></pre> <p>Now, let's define <code>search_case_insensitive()</code> right above our <code>tests</code> module:</p> <pre><code>// src/lib.rs\npub fn search_case_insensitive&lt;'a&gt;(\n    query: &amp;str,\n    contents: &amp;'a str,\n) -&gt; Vec&lt;&amp;'a str&gt; {\n    vec![]\n}\n</code></pre> <p>Running our test now outputs:</p> <pre><code>    Finished test [unoptimized + debuginfo] target(s) in 0.53s\n     Running unittests src/lib.rs (target/debug/deps/minigrep-41ffe2f4028fe4d9)\n\nrunning 2 tests\ntest tests::case_sensitive ... ok\ntest tests::case_insensitive ... FAILED\n\nfailures:\n\n---- tests::case_insensitive stdout ----\nthread 'tests::case_insensitive' panicked at 'assertion failed: `(left == right)`\n  left: `[\"Rust:\", \"Trust me.\"]`,\n right: `[]`', src/lib.rs:60:9\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n\n\nfailures:\n    tests::case_insensitive\n\ntest result: FAILED. 1 passed; 1 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n</code></pre> <p>And as expected <code>case_sensitive</code> test pass, but <code>case_insensitive</code> test fails.</p> <p>Let's implement our <code>search_case_insensitive()</code> function to make this failing test pass.</p> <pre><code>// src/lib.rs\n\n// converts query and line to lowercase and then search\npub fn search_case_insensitive&lt;'a&gt;(\n    query: &amp;str,\n    contents: &amp;'a str,\n) -&gt; Vec&lt;&amp;'a str&gt; {\n    let query = query.to_lowercase();\n    let mut results = Vec::new();\n\n    for line in contents.lines() {\n        // line.to_lowercase() returns new string, so no\n        // modification to what would be actually printed.\n        if line.to_lowercase().contains(&amp;query) {\n            results.push(line);\n        }\n    }\n\n    results\n}\n</code></pre> <p>Running our test now, everything passes:</p> <pre><code>   Compiling minigrep v0.1.0 (/home/adhadse/Downloads/minigrep)\n    Finished test [unoptimized + debuginfo] target(s) in 0.69s\n     Running unittests src/lib.rs (target/debug/deps/minigrep-41ffe2f4028fe4d9)\n\nrunning 2 tests\ntest tests::case_insensitive ... ok\ntest tests::case_sensitive ... ok\n\ntest result: ok. 2 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n\n     Running unittests src/main.rs (target/debug/deps/minigrep-1d6aff2511469b21)\n\nrunning 0 tests\n\ntest result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n\n   Doc-tests minigrep\n\nrunning 0 tests\n\ntest result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n</code></pre> <p>But how would our program figure out which search function to use? We'll use environment variables.</p> <p>Let's update our <code>Config</code> struct to account for case sensitivity for a search:</p> <ul> <li>replace our <code>run()</code> to use case_sensitive field, looping over <code>results</code></li> <li>Bring <code>env</code> module into scope</li> <li>modify <code>new()</code> function on <code>Config</code></li> </ul> <pre><code>use std::fs;\nuse std::error::Error;\nuse std::env;\n\npub fn run(config: Config) -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {\n    let contents = fs::read_to_string(config.filename)?;\n\n    // call differnt function depending on Config.case_sensitive\n    let results = if config.case_sensitive {\n        search(&amp;config.query, &amp;contents)\n    } else {\n        search_case_insensitive(&amp;config.query, &amp;contents)\n    }\n\n    // loop over results\n    for line in results {\n        println!(\"{}\", line);\n    }\n}\n\npub struct Config {\n    // ...\n    pub case_sensitive: bool\n}\n\nimpl Config {\n    pub fn new(args: &amp;[String]) -&gt; Result&lt;Config, &amp;str&gt; {\n        if args.len() &lt; 3 {\n            return Err(\"not enough arguments\");\n        }\n\n        let query = args[1].clone();\n        let filename = args[2].clone();\n        // `is_err()` returns a bool; if not set set to false otherwise true\n        let case_sensitive = env::var(\"CASE_INSENSITIVE\").is_err();\n\n        Ok(Config { query, filename, case_sensitive })\n    }\n}\n\n// ...\n// ...\n</code></pre> <p>Let's try this out: <code>cargo run to poem.txt</code></p> <pre><code>   Compiling minigrep v0.1.0 (/home/adhadse/Downloads/minigrep)\n    Finished dev [unoptimized + debuginfo] target(s) in 0.33s\n     Running `target/debug/minigrep to poem.txt`\nSearching for to\nIn file poem.txt\nAre you nobody, too?\n</code></pre> <p>And when we export our expect env variable:</p> <pre><code>export CASE_INSENSITIVE=true\n</code></pre> <p>and then test, <code>cargo run to poem.txt</code></p> <pre><code>    Finished dev [unoptimized + debuginfo] target(s) in 0.00s\n     Running `target/debug/minigrep to poem.txt`\nSearching for to\nIn file poem.txt\nAre you nobody, too?\nHow dreary to be somebody!\nTo tell your name the livelong day\n</code></pre> <p>and again we can uset the environment variable:</p> <pre><code>uset CASE_INSENSITIVE\n</code></pre>"},{"location":"cs/rust/tutorial/cli-app/cli-app-in-rust-2/#writing-to-standard-error","title":"Writing to Standard Error","text":"<p>Lastly, we'll change our <code>main()</code> function to print errors to standard error instead of standard output. - Command line programs are expected to send errors to the standard error stream so that if the user wanted to say, send output stream to a file they would still see errors on the screen.</p> <p>Let's see what happens when we stream output to a file:</p> <pre><code>cargo run &gt; output.txt\n</code></pre> <p>It doesn't output anything to terminal but in our file, all errors are written out:</p> <pre><code>    Finished dev [unoptimized + debuginfo] target(s) in 0.00s\n     Running `target/debug/minigrep`\n</code></pre> <pre><code>Problem parsing arguments: not enough arguments\n</code></pre> <p>To fix this just replace <code>println!</code> with <code>eprintln!</code> for errors:</p> <pre><code>// src/main.rs\nuse std::env;\nuse std::process;\n\nuse minigrep::Config; // import with `minigrep`; the name of our crate\n\nfn main() {\n    let args: Vec&lt;String&gt; = env::args().collect();\n\n    let config = Config::new(&amp;args).unwrap_or_else(|err| {\n        eprintln!(\"Problem parsing arguments: {}\", err);   // eprintln!\n        process::exit(1); // pass in status code 1\n    });\n\n    println!(\"Searching for {}\", config.query);\n    println!(\"In file {}\", config.filename);\n\n    // use minigrep::run()\n    if let Err(e) = minigrep::run(config) {\n        eprintln!(\"Application error: {}\", e);            // eprintln!\n        process::exit(1);\n    }\n</code></pre> <p>This time if we run the previous command, we do see output. And <code>output.txt</code> is empty</p> <pre><code>   Compiling minigrep v0.1.0 (/home/adhadse/Downloads/minigrep)\n    Finished dev [unoptimized + debuginfo] target(s) in 0.26s\n     Running `target/debug/minigrep`\nProblem parsing arguments: not enough arguments\n</code></pre> <p>And with correct arguments standard output is directed to <code>output.txt</code> and no errors are printed out to standard error strem (in terminal) <code>cargo run to poem.txt &gt; output.txt</code>:</p> <pre><code>    Finished dev [unoptimized + debuginfo] target(s) in 0.00s\n     Running `target/debug/minigrep to poem.txt`\n</code></pre> <p>and <code>output.txt</code> is:</p> <pre><code>Searching for to\nIn file poem.txt\nAre you nobody, too?\nHow dreary to be somebody!\nTo tell your name the livelong day\nTo an admiring bog!\n</code></pre>"},{"location":"cs/rust/tutorial/cli-app/cli-app-in-rust/","title":"CLI App in Rust","text":"<p>We'll be creating a simple clone of a popular command line tool called grep which basically allows us to search for a string withing a file. We'll call our project <code>minigrep</code>.</p> <p>An I/O Project: Building a Command Line Program</p>"},{"location":"cs/rust/tutorial/cli-app/cli-app-in-rust/#accepting-command-line-arguments","title":"Accepting Command Line Arguments","text":"<p>We want the user to pass in a string and a file name, which we'll accept via command line arguments to our program.</p> <ul> <li>For that we'll import <code>env</code> module and create a variable called <code>args</code>. <code>args()</code> function gives us an iterator over the arguments passed to our program and collect function will turn that iterator into a collection. That's why we needed to specify the type for <code>args</code> variable.</li> <li>Then we'll print our arguments</li> </ul> <pre><code>// src/main.rs\nuse std::env;\n\nfn main() {\n    let args: Vec&lt;String&gt; = env::args().collect();\n    println!(\"{:?}\", args);\n}\n</code></pre> <p>Running this with <code>cargo run</code> prints one argument by default when we don't pass in any arguments which is the path to our binary:</p> <pre><code>  Compiling minigrep v0.1.0 (/home/adhadse/Downloads/minigrep)\n    Finished dev [unoptimized + debuginfo] target(s) in 0.62s\n     Running `target/debug/minigrep`\n[\"target/debug/minigrep\"]\n</code></pre> <p>Let's try passing in some arguments:</p> <p><pre><code>cargo run needle haystack\n</code></pre> produces:</p> <pre><code>    Finished dev [unoptimized + debuginfo] target(s) in 0.00s\n     Running `target/debug/minigrep needle haystack`\n[\"target/debug/minigrep\", \"needle\", \"haystack\"]\n</code></pre> <p>We only care about query and filename, so we'll create two variables to store them.</p> <pre><code>fn main() {\n    let args: Vec&lt;String&gt; = env::args().collect();\n\n    // reference of element at index 1, index 0 is just binary path\n    let query = &amp;args[1];\n    let filename = &amp;args[2];\n\n    println!(\"Searching for {}\", query);\n    println!(\"In file {}\", filename);\n}\n</code></pre> <p>and passing in query and filename as arguments: <code>cargo run test sample.txt</code></p> <pre><code>   Compiling minigrep v0.1.0 (/home/adhadse/Downloads/minigrep)\n    Finished dev [unoptimized + debuginfo] target(s) in 0.26s\n     Running `target/debug/minigrep test sample.txt`\nSearching for test\nIn file sample.txt\n</code></pre>"},{"location":"cs/rust/tutorial/cli-app/cli-app-in-rust/#reading-a-file","title":"Reading a File","text":"<p>Let's create a new file at the root of our project called <code>poem.txt</code> with some beautiful poem in it:</p> <p>A poem by Emily Dickinson</p> <pre><code>I'm nobody! Who are you?\nAre you nobody, too?\nThen there's a pair of us - don't tell!\nThey'd banish us, you know.\n\nHow dreary to be somebody!\nHow public, like a frog\nTo tell your name the livelong day\nTo an admiring bog!\n</code></pre> <p>So how are we going to read the contents of the file? We'll use <code>fs</code> module from <code>std</code>:</p> <ul> <li>The <code>read_to_string()</code> returns a string by reading a file wrapped in <code>Result&lt;T&gt;</code> type. If it returns an <code>Err</code> variant we want to exit our program, so we use <code>expect()</code> method on top to print a message while panicking.</li> </ul> <pre><code>use std::env;\nuse std::fs;\n\nfn main() {\n    // ...\n    println!(\"In file {}\", filename);\n\n    let contents = fs::read_to_string(filename)\n        .expect(\"Something went wrong readint the file\");\n\n    // if everything else goes successfully\n    println!(\"With text:\\n{}\", contents);\n}\n</code></pre> <p>Let's run our program by giving it the filename and query: <code>cargo run the poem.txt</code>:</p> <pre><code>    Finished dev [unoptimized + debuginfo] target(s) in 0.00s\n     Running `target/debug/minigrep the poem.txt`\nSearching for the\nIn file poem.txt\nWith text:\nI'm nobody! Who are you?\nAre you nobody, too?\nThen there's a pair of us - don't tell!\nThey'd banish us, you know.\n\nHow dreary to be somebody!\nHow public, like a frog\nTo tell your name the livelong day\nTo an admiring bog!\n</code></pre>"},{"location":"cs/rust/tutorial/cli-app/cli-app-in-rust/#refactoring","title":"Refactoring","text":"<p>There are few things we could improve: - Our main function does two things: it is reponsible for parsing arguments as well as reading the cotents of a file. Ideally we want function (even main function) to have one responsibility, i.e., keep the reposnibility of a function scoped. - We also have these two arguments <code>query</code> and <code>filename</code> connected, but that connection isn't expressed in our program. - When we read the contents of the file and it fails our message simply says \"Something went wrong reading the file\". The message is more or less useless in describing what actually went wrong. - We also don't have a centralized place to handle errors. The function can fail when wrong no of arguments are passed or when there is some problem when reading a file.</p> <p>The pattern to follow when <code>main()</code> of binary crate has too many responsibilites developed by Rust community is to create a library crate and then have the binary crate call the function in the library crate.</p> <p>But before that let's extract out some logic:</p> <pre><code>// main.rs\nuse std::env;\nuse std::fs;\n\nfn main() {\n    let args: Vec&lt;String&gt; = env::args().collect();\n\n    let (query, filename) = parse_config(&amp;args);\n\n    println!(\"Searching for {}\", query);\n    println!(\"In file {}\", filename);\n\n    let contents = fs::read_to_string(filename)\n        .expect(\"Something went wrong readint the file\");\n\n    println!(\"With text:\\n{}\", contents);\n}\n\nfn parse_config(args: &amp;[String]) -&gt; (&amp;str, &amp;str) {\n    // reference of element at index 1, index 0 is just binary path\n    let query = &amp;args[1];\n    let filename = &amp;args[2];\n\n    (query, filename)\n}\n</code></pre> <ul> <li>Now even though we have extracted the logic, the two string represent the  <code>query</code> and <code>filename</code> but it's still not very clear that these two string are connected.</li> </ul> <p>To fix that problem let's create a struct called <code>Config</code>:</p> <pre><code>// main.rs\n// ...\n\nfn main() {\n    let args: Vec&lt;String&gt; = env::args().collect();\n\n    let config = parse_config(&amp;args);\n\n    println!(\"Searching for {}\", config.query);\n    println!(\"In file {}\", config.filename);\n\n    let contents = fs::read_to_string(config.filename)\n        .expect(\"Something went wrong readint the file\");\n\n    println!(\"With text:\\n{}\", contents);\n}\n\nstruct Config {\n    query: String,\n    filename: String\n}\n\nfn parse_config(args: &amp;[String]) -&gt; Config {\n    // we could use lifetimes to make thing effecient but for now this is easier.\n    let query = args[1].clone();\n    let filename = args[2].clone();\n\n    Config { query, filename }\n}\n</code></pre> <p>Still our <code>parse_config()</code> is tied to our config struct but our program doesn't express this coupling. What we need is an associated function for <code>Config</code> named <code>new()</code> to generate a <code>Config</code> object (a convention for constructor function):</p> <pre><code>// main.rs\n// ...\n\nfn main() {\n    let args: Vec&lt;String&gt; = env::args().collect();\n\n    let config = Config::new(&amp;args);\n\n    // ...\n}\n\nstruct Config {\n    // ...\n}\n\nimpl Config {\n    fn new(args: &amp;[String]) -&gt; Config {\n        let query = args[1].clone();\n        let filename = args[2].clone();\n\n        Config { query, filename }\n    }\n}\n</code></pre> <p>Now let's fix error handling. If we don't pass enough arguments to our program (<code>cargo run</code>), the error message is not so useful or straight away confusing for user to understand what the program expects:</p> <pre><code>   Finished dev [unoptimized + debuginfo] target(s) in 0.00s\n     Running `target/debug/minigrep`\nthread 'main' panicked at 'index out of bounds: the len is 1 but the index is 1', src/main.rs:26:21\n</code></pre> <p>We need more better error message:</p> <pre><code>// main.rs\n// ...\n\nimpl Config {\n    fn new(args: &amp;[String]) -&gt; Config {\n        if args.len() &lt; 3 {\n            panic!(\"not enough arguments\");\n        }\n\n        let query = args[1].clone();\n        let filename = args[2].clone();\n\n        Config { query, filename }\n    }\n}\n</code></pre> <p>which should now output if we don't pass enough arguments:</p> <pre><code>   Compiling minigrep v0.1.0 (/home/adhadse/Downloads/minigrep)\n    Finished dev [unoptimized + debuginfo] target(s) in 0.35s\n     Running `target/debug/minigrep`\nthread 'main' panicked at 'not enough arguments', src/main.rs:27:13\n</code></pre> <p>But the error still contains a lot more information not so useful to a user. That's because we are calling <code>panic!</code> macro. <code>panic!</code> is much more useful when we have a programming error rather than a usage error.</p> <p>Let's fix this by: - Returning a <code>Result&lt;T&gt;</code> by our <code>new()</code> function of <code>Config</code> with Config in <code>Ok</code> case or a <code>&amp;str</code> (string slic) in the <code>Err</code> case representing error message. - And instead or panicking return an <code>Err</code> type. - Plus import <code>process</code> to help us exit program without panicking.</p> <p>We'll use <code>unwrap_or_else()</code> to handle our <code>Err</code> case passing in a closure.</p> <p>Closures</p> <pre><code>use std::env;\nuse std::fs;\nuse std::process;\n\nfn main() {\n    let args: Vec&lt;String&gt; = env::args().collect();\n\n    // this will return `config` in `Ok` case or print err and exit program\n    let config = Config::new(&amp;args).unwrap_or_else(|err| {\n        println!(\"Problem parsing arguments: {}\", err);\n        process::exit(1); // pass in status code 1\n    });\n\n    // ...\n}\n\n// ...\n\nimpl Config {\n    fn new(args: &amp;[String]) -&gt; Result&lt;Config, &amp;str&gt; {\n        if args.len() &lt; 3 {\n            return Err(\"not enough arguments\");\n        }\n\n        let query = args[1].clone();\n        let filename = args[2].clone();\n\n        Ok(Config { query, filename })\n    }\n}\n</code></pre> <p>Running this produces a much nicer, readable output with <code>cargo run</code>:</p> <pre><code>   Compiling minigrep v0.1.0 (/home/adhadse/Downloads/minigrep)\n    Finished dev [unoptimized + debuginfo] target(s) in 0.25s\n     Running `target/debug/minigrep`\nProblem parsing arguments: not enough arguments\n</code></pre> <p>Next, we'll create a function named <code>run()</code> which will contain logic that doesn't have to do with setting up the configuration or handling errors but is only repsonsible for reading the contents of the file and printing it out:</p> <ul> <li>Also return a <code>Result&lt;T&gt;</code> instead of panicking when something goes wrong while reding the file. In the success case  we return a Unit type <code>()</code> and in <code>Err</code> case we return <code>Error</code>. the <code>Box&lt;dyn Error&gt;</code> just means return any type of error.</li> </ul> <pre><code>// imports ...\nuse std::error:: Error;\n\nfn main() {\n    // ...\n\n    // Delete this section in main\n    // let contents = fs::read_to_string(config.filename)\n    //     .expect(\"Something went wrong readint the file\");\n\n    // println!(\"With text:\\n{}\", contents);\n\n    // since we only care about `Err` variant\n    if let Err(e) = run(config) {\n        println!(\"Application error: {}\", e);\n        process::exit(1);\n    }\n}\n\nfn run(config: Config) -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {\n    // the `?` immediately return with the Error from the function if this result in Err\n    let contents = fs::read_to_string(config.filename)?;\n\n    println!(\"With text:\\n{}\", contents);\n\n    Ok(()) // Return `Ok` variant passing in Unit `()`\n}\n\n// ...\n</code></pre> <p>Running this program for a file that doesn't exist return:</p> <pre><code>    Finished dev [unoptimized + debuginfo] target(s) in 0.00s\n     Running `target/debug/minigrep the file.txt`\nSearching for the\nIn file file.txt\nApplication error: No such file or directory (os error 2)\n</code></pre>"},{"location":"cs/rust/tutorial/concurrency/","title":"Index","text":"<p>title: \"Concurrency\" description: \"\" lead: \"\" date: 2022-09-20T07:58:19+01:00 lastmod: 2022-09-20T07:58:19+01:00 draft: false images: [] weight: 25</p>"},{"location":"cs/rust/tutorial/concurrency/creating-threads/","title":"Concurrency in Rust - Creating Threads","text":"<p>Info</p> <p>Concurrent programming is when different parts of your program execute independently.</p> <p>Parallel programming is when different parts of your program execute at the same time.</p> <p>Because of Rust's type system and powerful ownership model, we can write concurrent programs with lots of errors being caught at compile time. Meaning we can write code that is free of subtle bugs and easy to refactor, thus we call it fearless concurrency.</p> <p>Fearless Concurrency</p>"},{"location":"cs/rust/tutorial/concurrency/creating-threads/#using-threads","title":"Using Threads","text":"<p>An executed program's code is ran within a process and the operating system manages multiple processes at once.</p> <p>Withing a program we can have independent parts that run simultaneously, ran by threads. This can improve the performance of our program since, multiple parts are running at the same time at the cost of complexity.</p> <p>Since with concurrency, we don't have control over which part run at which time, we can few problems., such as race conditions, where threads will try to access data/resource in inconsistent order.</p> <p>Another such problem is Deadlocks where we can have two threads that are both waiting for a resource that the other thread has thus making both threads wait indefinitely.</p> Source: Wikipedia <p>Also, because execution order is non-deterministic bugs can appear that can only happen in certain situation, hard to be caught during testing.</p> <p>There are two main types of threads:</p> <ul> <li>One-to-One threads or OS threads: means that when we create a thread in our program it maps to an operating system thread</li> <li>M-to-n threads or Program threads: many programming language provide their own implementation of threads, these threads are not mapped to OS threads., i.e., we can many program threads being mapped to fewer OS threads. That's why it's called M-to-n model because we have \"m\" program threads that maps to \"n\" OS threads.</li> </ul> <p>Each model has its own advantages and disadvantages. The most important trade-off for Rust is runtime support, the code that is included by the programming language in every single binary. Rust aims to have extremely small runtime in fact almost no runtime at all, trading-off out-of-the-box features, the more feature we include out of the box, the larger the runtime will be.</p> <p>Since program threads will require larger runtime, Rust only include OS threads in its standard library. For M-to-n threads crates providing such functionalities are there.</p>"},{"location":"cs/rust/tutorial/concurrency/creating-threads/#creating-a-new-thread-with-spawn","title":"Creating a New Thread with spawn","text":"<pre><code>use std::{thread, time::Duration};\n\nfn main() {\n    // call `spawn()` function and pass in closure\n    thread::spawn(|| {\n        // loop through 1..10\n        for i in 1..10 {\n            println!(\"hi number {} from the spawned thread!\", i);\n            // just pause execution of this thread\n            thread::sleep(Duration::from_millis(1));\n        }\n    });\n\n    // looping in main through 1..5\n    for i in 1..5 {\n        println!(\"hi number {} from main thread!\", i);\n        thread::sleep(Duration::from_millis(1));\n    }\n}\n</code></pre> <p>Output might look different if you ran it, since the order is non-deterministic.</p> <pre><code>hi number 1 from main thread!\nhi number 1 from the spawned thread!\nhi number 2 from main thread!\nhi number 2 from the spawned thread!\nhi number 3 from the spawned thread!\nhi number 3 from main thread!\nhi number 4 from the spawned thread!\nhi number 4 from main thread!\nhi number 5 from the spawned thread!\n</code></pre> <p>The main thread finished printing all of its numbers from 1 to 4 (since the range was exclusive)  but the spawn thread didn't finish printing all of its number from 1 to 10. This is because, when main thread ends, the spawn thread is stopped no matter if it finished executing or not. </p> <p>Let's modify the code to let spawned thread to finish its execution.</p>"},{"location":"cs/rust/tutorial/concurrency/creating-threads/#using-join-handles","title":"Using join handles.","text":"<ol> <li>Store the return type returned by <code>thread::spawn()</code> which is a <code>JoinHandle&lt;T&gt;</code> object.</li> <li>Call <code>JoinHandle&lt;T&gt;.join()</code> method to wait for the spawned thread to finish.</li> </ol> <pre><code>use std::{thread, time::Duration};\n\nfn main() {\n    // call `spawn()` function and pass in closure\n    let handle = thread::spawn(|| {\n        // loop through 1..10\n        for i in 1..10 {\n            println!(\"hi number {} from the spawned thread!\", i);\n            // just pause execution of this thread\n            thread::sleep(Duration::from_millis(1));\n        }\n    });\n\n    // looping in main through 1..5\n    for i in 1..5 {\n        println!(\"hi number {} from main thread!\", i);\n        thread::sleep(Duration::from_millis(1));\n    }\n\n    // call `unwrap()` because `join()` returns a `Result&lt;T&gt;`\n    handle.join().unwrap();\n}\n</code></pre> <p>Calling <code>join()</code> will block (stop from doing any further work/exit entirely) the thread currently running (which in this is main thread) until the thread associated with the handle (the spawn thread).</p> <p>Running this updated code outputs, what we expect:</p> <pre><code>hi number 1 from main thread!\nhi number 1 from the spawned thread!\nhi number 2 from main thread!\nhi number 2 from the spawned thread!\nhi number 3 from main thread!\nhi number 3 from the spawned thread!\nhi number 4 from main thread!\nhi number 4 from the spawned thread!\nhi number 5 from the spawned thread!\nhi number 6 from the spawned thread!\nhi number 7 from the spawned thread!\nhi number 8 from the spawned thread!\nhi number 9 from the spawned thread!\n</code></pre> <p>If we move our call to <code>join()</code> method right after spawning the thread, like this:</p> <pre><code>use std::{thread, time::Duration};\n\nfn main() {\n    // call `spawn()` function and pass in closure\n    let handle = thread::spawn(|| {\n        // loop through 1..10\n        for i in 1..10 {\n            println!(\"hi number {} from the spawned thread!\", i);\n            // just pause execution of this thread\n            thread::sleep(Duration::from_millis(1));\n        }\n    });\n\n    handle.join().unwrap();\n\n    // ...\n</code></pre> <p>Produces:</p> <pre><code>hi number 1 from the spawned thread!\nhi number 2 from the spawned thread!\nhi number 3 from the spawned thread!\nhi number 4 from the spawned thread!\nhi number 5 from the spawned thread!\nhi number 6 from the spawned thread!\nhi number 7 from the spawned thread!\nhi number 8 from the spawned thread!\nhi number 9 from the spawned thread!\nhi number 1 from main thread!\nhi number 2 from main thread!\nhi number 3 from main thread!\nhi number 4 from main thread!\n</code></pre> <p>This is because the main thread kept waiting because we called <code>join()</code> right after spawning a new thread.</p>"},{"location":"cs/rust/tutorial/concurrency/creating-threads/#using-move-closures-with-threads","title":"Using move Closures with Threads","text":"<p>Up until now the thread didn't upon any variables outside of the thread. Things become wierd when variables are shared among variables.</p> <p>Here, in this code we just want to print out <code>v</code> from inside of a spawned thread. But this gives error:</p> <pre><code>use std::thread;\n\nfn main() {\n    let v = vec![1, 2, 3];\n\n    let handle = thread::spawn(|| {\n        //                     ^^ closure may outlive the current function,\n        // but it borrow `v`, which is owned by the current function.\n        println!(\"Here's a vector: {:?}\", v);\n    });\n\n    handle.join().unwrap();\n}\n</code></pre> <p>Rust is trying to capture environment variables, and think we only need reference to <code>v</code> since we are printing <code>v</code>. But that's a problem because Rust doesn't know how long the spawn thread will run for so it doesn't know if <code>v</code> will always be a valid reference or not.</p> <p>Say, what if we dropped the variables after spawning the thread. This could lead to unintentional behavior. So we aren't allowed to have a reference of the variables passed to spawned threads.</p> <pre><code>//...\nlet handle = thread::spawn(|| {\n    println!(\"Here's a vector: {:?}\", v);\n});\ndrop(v);\n\n//...\n</code></pre> <p>Instead we need to take ownership using the <code>move</code> keyword. <code>move</code> keyword tell Rust to not infer the values that closure as borrowed instead we want explicitly to move values inside the closure/ closure to take ownership. After this we'll not be able to call <code>v</code> in main thread.</p> <pre><code>    // ...\n    let handle = thread::spawn(move || {\n        println!(\"Here's a vector: {:?}\", v);\n    });\n\n    handle.join().unwrap();\n}\n</code></pre>"},{"location":"cs/rust/tutorial/concurrency/message-passing/","title":"Concurrency in Rust - Message Passing","text":"<p>Message passing between threads is one very popular approach to safe concurrency.</p> <p>The Go programming language has a slogan that summarizeds this approach:</p> <p>Do not communicate by sharing memory instead share memory by communicating.</p> <p>Rust provides channels via standard library to enable message passing. A channel in programming has two halves, the transmitter and the receiver. </p> <p>A transmitter is the upstream location where the message originates and at receiver it ends the message transmission.</p> <p>One part of our code calls method on the transmitter passing in the data you want to send and another part of our is listening to the receiver for arriving messages. The channel is said to be closed if either the transmitter or the receiver half is dropped.</p> <p>Using Message Passing to Transfer Data Between Threads</p>"},{"location":"cs/rust/tutorial/concurrency/message-passing/#transfer-data-between-threads","title":"Transfer Data Between Threads","text":"<p>We'll bring in <code>mpsc</code> module from the standard library into scope. <code>mpsc</code> stands for Multi-produce, single-consumer FIFO queue.</p> <p>So, in Rust, we can have multiple producers of messages but only single receiver of messages.</p> <pre><code>use std::sync::mpsc;\nuse std::thread;\n\nfn main() {\n    // to create a channel call `channel()` method on `mpsc` module.\n    // this returns a sender and receiver\n    let (tx, rx) = mpsc::channel();\n\n    // in order to sender to send message, we need to move `tx` into\n    // the closure\n    thread::spawn(move || {\n        let msg = String::from(\"hi\");\n        // unwrap because if receiving ends get dropped for some reason\n        // while sending message, `send()` will return an error\n        // Right now we panic; but for production use case, handle it gracefully\n        tx.send(msg).unwrap();\n    });\n\n    // the receiver end\n    let received = rx.recv().unwrap();\n    println!(\"Got: {}\", received);\n}\n</code></pre> <p><code>rx&lt;T&gt;</code> also have <code>try_recv()</code> which doesn't block main thread execution instead it will return a result type immediately. <code>try_recv()</code> is useful when we want our thread to do other work, say, for example a loop to check with <code>try_recv()</code> for new message otherwise do other work.</p> <p>Running this produces:</p> <pre><code>Got: hi\n</code></pre>"},{"location":"cs/rust/tutorial/concurrency/message-passing/#channels-and-ownership-transference","title":"Channels and Ownership Transference","text":"<p>Ownership rules help us prevent errors in our concurrent code.</p> <p>For example let's try to use <code>msg</code> after sending it down via channel:</p> <pre><code>use std::sync::mpsc;\nuse std::thread;\n\nfn main() {\n    let (tx, rx) = mpsc::channel();\n\n    thread::spawn(move || {\n        let msg = String::from(\"hi\");\n        tx.send(msg).unwrap();       // send takes ownership of value\n        println!(\"msg is {}\", msg);\n        //                    ^^^ borrow of moved value: `msg`\n        //                    value borrowed here after move\n    });\n\n    let received = rx.recv().unwrap();\n    println!(\"Got: {}\", received);\n}\n</code></pre> <p>This is problematic, since once we send the message via channel and could use it, in that case we could potentially modify or drop the variable.</p>"},{"location":"cs/rust/tutorial/concurrency/message-passing/#sending-multiple-values","title":"Sending multiple values","text":"<p>Let's modify the code to send multiple message to prove concurrency.</p> <p>To do that, create a vector of values:</p> <pre><code>use std::sync::mpsc;\nuse std::thread;\nuse std::time::Duration;\n\nfn main() {\n    let (tx, rx) = mpsc::channel();\n\n    thread::spawn(move || {\n        let vals = vec![\n            String::from(\"hi\"),\n            String::from(\"from\"),\n            String::from(\"the\"),\n            String::from(\"thread\"),\n        ];\n\n        // then loop through `val` and send each message\n        for val in vals {\n            tx.send(val).unwrap();\n            thread::sleep(Duration::from_secs(1));\n        }\n    });\n\n    // treat `rx` as an iterator;\n    // every iteration will have a value passed into the channel\n    // when channel closes iteration ends\n    for received in rx {\n        println!(\"Got: {}\", received);\n    }\n}\n</code></pre> <p>Running this generates, with a second delay:</p> <pre><code>Got: hi\nGot: from\nGot: the\nGot: thread\n</code></pre>"},{"location":"cs/rust/tutorial/concurrency/message-passing/#creating-multiple-produces","title":"Creating multiple produces","text":"<p>Say, we have two threads that send messages.</p> <pre><code>use std::sync::mpsc;\nuse std::thread;\nuse std::time::Duration;\n\nfn main() {\n    let (tx, rx) = mpsc::channel();\n\n    // clone original `tx` sender\n    let tx2 = tx.clone();\n\n    thread::spawn(move || {\n        let vals = vec![\n            String::from(\"hi\"),\n            String::from(\"from\"),\n            String::from(\"the\"),\n            String::from(\"thread\"),\n        ];\n\n        for val in vals {\n            tx.send(val).unwrap();\n            thread::sleep(Duration::from_secs(1));\n        }\n    });\n\n    thread::spawn(move || {\n        let vals = vec![\n            String::from(\"move\"),\n            String::from(\"messages\"),\n            String::from(\"for\"),\n            String::from(\"you\"),\n        ];\n\n        for val in vals {\n            tx2.send(val).unwrap();   // `tx2` instead of `tx`\n            thread::sleep(Duration::from_secs(1));\n        }\n    });\n\n    // now we have two threads passing down messages down the channel\n    for received in rx {\n        println!(\"Got: {}\", received);\n    }\n}\n</code></pre> <p>Running this produces a non-deterministic output, might differ for you each time:</p> <pre><code>Got: hi\nGot: move\nGot: from\nGot: messages\nGot: for\nGot: the\nGot: you\nGot: thread\n</code></pre>"},{"location":"cs/rust/tutorial/concurrency/sharing-state/","title":"Concurrency in Rust - Sharing State","text":"<p>Message passing a one way to pass data between concurrent threads. Other way to do is usin Sharing state.</p> <p>Transferring data using shared state concurrency we have some piece of data that multiple threads can read and write to.</p> <p>Shared-State Concurrency</p>"},{"location":"cs/rust/tutorial/concurrency/sharing-state/#the-api-of-mutex","title":"The API of Mutex","text":"<p>What are mutexes?</p> <p>Mutex is an abbreviation for Mutual Exclusion.</p> <p>That means for a piece of data only one thread can have access to the data at a given time.</p> <p>To achieve the mutexes use locking system. When a thread wants access to a piece of data behind a mutex, it will signal that it wants access to the data and acquire the mutexe's lock.</p> <p>The lock is a data structure that keeps track of which thread has exclusive access to a piece of data.</p> <p>Lock disallow access to that piece of data to any another thread. Once the thread is done with the data, it can unlock the piece of data and let other threads to get lock of it/access it.</p> <p>Mutex are known for being hard to use due to:</p> <ol> <li>We need to acquire the lock before we access the data.</li> <li>We have to release the lock when we are done with the data so that other threads can access.</li> </ol> <p>But don't worry, Rust got you covered so that you don't get the locking/unlocking wrong.</p> <pre><code>use std::sync::Mutex;\n\nfn main() {\n    // to create a new Mutex\n    let m = Mutex::new(5);\n\n    // let's access the data in Mutex\n    {\n        // call `lock()` to acquire lock\n        // block current thread until that lock is able to be acquired\n        let mut num = m.lock().unwrap();\n        *num = 6;      // mutate the value\n    }\n\n    println!(\"m = {:?}\", m);\n}\n</code></pre> <p>Calling <code>lock()</code> this returns an <code>Option&lt;T&gt;</code> because if there is already a thread that has a lock to that data and that thread panics, then calling <code>lock()</code> will fail. Calling <code>unwrap()</code> will return a <code>MutexGuard&lt;T&gt;</code> smart pointer whose <code>Deref</code> trait points to inner data of the Mutex. <code>MutexGuard&lt;T&gt;</code> implements <code>Drop</code> trait such that when it goes out of scope, it releases lock to the data.</p> <p>Running this produces:</p> <pre><code>m = Mutex { data: 6, poisoned: false, .. }\n</code></pre>"},{"location":"cs/rust/tutorial/concurrency/sharing-state/#sharing-mutex-between-multiple-threads","title":"Sharing Mutex between Multiple Threads","text":"<p>We'll create a new mutex that holds the integer value and we'll spin up 10 threads, each incrementing the value to reach the result of 10.</p> <pre><code>use std::sync::Mutex;\nuse std::thread;\n\nfn main() {\n    let counter = Mutex::new(0);\n    let mut handles = vec![];\n\n    // create threads\n    for _ in 0..10 {\n        let handle = thread::spawn(move || {\n            //                     ^^^^ use of moved value: `counter`\n            let mut num = counter.lock().unwrap();\n            *num += 1;\n        });\n        handels.push(handle);\n    }\n\n    for handle in hanldes {\n        handle.join().unwrap();\n    }\n\n    println!(\"Result: {}\", *counter.lock().unwrap());\n}\n</code></pre> <p>But we have an error. <code>counter</code> was already moved in the previous iteration to the first thread we created.</p>"},{"location":"cs/rust/tutorial/concurrency/sharing-state/#multiple-ownership-with-multiple-threads","title":"Multiple Ownership with Multiple Threads","text":"<p>We want to allow counter to have multiple owners and this can be made possible with <code>Rc&lt;T&gt;</code> smart pointer.</p> <p>So, let's update the code:</p> <pre><code>use std::sync::Mutex;\nuse std::thread;\nuse std::rc::Rc;\n\nfn main() {\n    let counter = Rc::new(Mutex::new(0)); // wrap mutex in `Rc`\n    let mut handles = vec![];\n\n    // create threads\n    for _ in 0..10 {\n        // a counter that shadows original counter\n        let counter = Rc::clone(&amp;counter);\n\n        let handle = thread::spawn(move || {\n            //       ^^^^^^^^^^^^^ error: `Rc&lt;Mutex&lt;i32&gt;&gt;` cannot be sent between threads safely\n            let mut num = counter.lock().unwrap();\n            *num += 1;\n        });\n        handles.push(handle);\n    }\n\n    for handle in handles {\n        handle.join().unwrap();\n    }\n\n    println!(\"Result: {}\", *counter.lock().unwrap());\n}\n</code></pre> <p>But this creates new error. <code>Rc&lt;T&gt;</code> gives us the functionality we want, but it's not thread safe.</p> <p>For our use case we want Atomic Reference counting smart pointer. Atomics are like primitive types except they can be shared among threads.</p> <pre><code>use std::sync::{Arc, Mutex};\nuse std::thread;\n\nfn main() {\n    let counter = Arc::new(Mutex::new(0)); // wrap mutex in `Rc`\n    let mut handles = vec![];\n\n    // create threads\n    for _ in 0..10 {\n        // a counter that shadows original counter\n        let counter = Arc::clone(&amp;counter);\n\n        let handle = thread::spawn(move || {\n            let mut num = counter.lock().unwrap();\n            *num += 1;\n        });\n        handles.push(handle);\n    }\n\n    for handle in hanldes {\n        handle.join().unwrap();\n    }\n\n    println!(\"Result: {}\", *counter.lock().unwrap());\n}\n</code></pre> <p>Running this produces:</p> <pre><code>Result: 10\n</code></pre> <p>Even though counter is immutable but we could mutate the inner value via a mutable reference, that's because <code>Mutex&lt;T&gt;</code> uses interior mutability.</p>"},{"location":"cs/rust/tutorial/concurrency/sharing-state/#refcellrc-and-mutexarc","title":"RefCell/Rc and Mutex/Arc","text":"<p>RefCell/Rc and Mutex/Arc</p> <p>In the same way that the <code>RefCell&lt;T&gt;</code> allows us to mutate value inside <code>Rc&lt;T&gt;</code>; <code>Mutex&lt;T&gt;</code> allows us to mutate value inside <code>Arc&lt;T&gt;</code> smart pointer.</p> <p><code>RefCell&lt;T&gt;</code> comes with the risk of creating circular dependencies &amp; <code>Mutex&lt;T&gt;</code> comes with the risk of creating deadlocks.</p>"},{"location":"cs/rust/tutorial/functional/","title":"Functional","text":""},{"location":"cs/rust/tutorial/functional/closures/","title":"Closures in Rust","text":"<p>What are closures?</p> <p>Closures are kind of function but anonymous (unnamed), have ability to be stored as variables and passed as input parameters to others functions.</p> <p>Closures: Anonymous Functions that Capture Their Environment</p>"},{"location":"cs/rust/tutorial/functional/closures/#example-program","title":"Example Program","text":"<p>Let's say we are creating a backend for a fitness that generates customized workouts for a user based on various health factors. Now part of this system might perform some really expensive calculation. Something like this:</p> <pre><code>use std::thread;\nuse std::time::Duration;\n\nfn simulated_expensive_calculation(intensity: u32) -&gt; u32 {\n    println!(\"Calculating slowly...\");\n    thread::sleep(Duration::from_secs(2));\n    intensity\n}\n\nfn main() {\n    let simulated_intensity = 10;    // some number coming in from user input; here just hardcoded\n    let simulated_random_number = 7; // provide a variety in generated workout\n\n    // generate a workout\n    generate_workout(simulated_intensity, simulated_random_number);\n}\n\nfn generate_workout(intensity: u32, random_number: u32) {\n    if intensity &lt; 25 {\n        println!(\n            \"Today, do {} pushups!\",\n            simulated_expensive_calculation(intensity)\n        );\n        println!(\n            \"Next, do {} situps!\",\n            simulated_expensive_calculation(intensity)\n        );\n    } else {\n        if random_number == 3 {\n            println(\"Take a break today! Rember to stary Hydrated!\");\n        } else {\n            println!(\n                \"Today, run for {} minutes!\",\n                simulated_expensive_calculation(intensity)\n            );\n        }\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/functional/closures/#refactoring-with-functions","title":"Refactoring with functions","text":"<p>We are calling our expensive function at multiple places, if we change the function definition, a lot of refactoring will be required. Plus we are calling the expensive function repetitvely.</p> <p>Let's refactor it:</p> <pre><code>fn generate_workout(intensity: u32, random_number: u32) {\n    let expensive_result = simulated_expensive_calculation(intensity);\n    if intensity &lt; 25 {\n        println!(\"Today, do {} pushups!\", expensive_result);\n        println!(\"Next, do {} situps!\", expensive_result);\n    } else {\n        if random_number == 3 {\n            // but we don't need a call our expensive function if random number is 3\n            println(\"Take a break today! Rember to Hydrated!\");\n        } else {\n            println!(\"Today, run for {} minutes\", expensive_result );\n        }\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/functional/closures/#refactoring-using-closures","title":"Refactoring using Closures","text":"<p>Closure help keep the logic at one place while keeping it anonymous.</p> <p>But the problem of calling the expensive operation remains in first <code>if</code> block.</p> <pre><code>fn generate_workout(intensity: u32, random_number: u32) {\n    // `num` is the input parameter inside | |\n    // if closure is just one life you don't need {}\n    //\n    // `expensive_closure` don't store the return value, but the closure\n    let expensive_closure = |num| {\n        println!(\"Calculating slowly..\");\n        thread::sleep(Duration::from_secs(2));\n        num\n    };\n\n    // BUT we're still calling expensive operation twice!\n    if intensity &lt; 25 {\n        println!(\"Today, do {} pushups!\", expensive_closure(intensity));\n        println!(\"Next, do {} situps!\", expensive_closure(intensity));\n    } else {\n        if random_number == 3 {\n            // but we don't need a call our expensive function if random number is 3\n            println(\"Take a break today! Rember to Hydrated!\");\n        } else {\n            println!(\"Today, run for {} minutes\",  expensive_closure(intensity)) );\n        }\n    }\n}\n</code></pre> <p>Why no type annotation for closure's input parameters?</p> <p>Functions provide explicity interface exposed to users so agreeing on the types of inputs are return values are important.</p> <p>Closures on the other hand are relevant in a much narrow context and so compiler happily determines the input and output types.</p> <ul> <li>The first type being passed in to the closure becomes the concrete type of the paremeter. There is no dynamic typing.</li> </ul>"},{"location":"cs/rust/tutorial/functional/closures/#generic-parameters-and-fn-traits","title":"Generic Parameters and Fn Traits","text":"<p>We could have solved the problem of calling the expensive function multiple times by storing the result in a variable.</p> <p>But, we can do better.</p> <p>We'll use Memoization pattern, by creating a struct to hold the closure and the result of our closure with generic type <code>T</code> bounded by trait <code>Fn</code>.</p> <p><code>Fn</code> trait is provided by standard library and all closures (and regular functions) implement one of the three <code>Fn</code> traits: 1. <code>Fn</code>: immutable borrows values. 2. <code>FnMut</code>: mutably borrows values. 3. <code>FnOnce</code>: takes ownershup of the variables inside the closure. <code>Once</code> here indicates that closures are not allowed to take ownership of same variables more than once.  Meaning this closures can only be called once.</p> <pre><code>struct Cacher&lt;T&gt;\nwhere\n    T: Fn(u32) -&gt; u32,\n{\n    calculation: T,     // the calculation function\n    value: Option&lt;u32&gt;  // stored calculated value\n}\n</code></pre> <pre><code>impl&lt;T&gt; Cacher&lt;T&gt;\nwhere\n    T: Fn(u32) -&gt; u32,\n{\n    fn new(calculation: T) -&gt; Cacher&lt;T&gt; {\n        Cacher {\n            calculation,\n            value: None\n        }\n    }\n\n    // when we first create our cacher, `value` will be none,\n    // so match expression allow us to handle the variant of the\n    // `Option` and set the value after calling `calculation` closure.\n    fn value(&amp;mut self, arg: u32) -&gt; u32 {\n        match self.value {\n            Some(v) =&gt; v,\n            None =&gt; {\n                let v = (self.calculation)(arg);\n                self.value = Some(v);\n                v\n            }\n        }\n    }\n}\n</code></pre> <p>Let's use our <code>Cacher</code> struct in <code>generate_workout()</code> function.</p> <p>We'll wrap the closure in <code>Cacher</code> struct:</p> <pre><code>fn generate_workout(intensity: u32, random_number: u32) {\n    let mut cached_result = Cacher::new(|num| {\n        println!(\"Calculating slowly..\");\n        thread::sleep(Duration::from_secs(2));\n        num\n    });\n\n    if intensity &lt; 25 {\n        println!(\"Today, do {} pushups!\",\n        cached_result.value(intensity));\n        println!(\"Next, do {} situps!\",\n        cached_result.value(intensity));\n    } else {\n        if random_number == 3 {\n            println(\"Take a break today! Rember to Hydrated!\");\n        } else {\n            println!(\"Today, run for {} minutes\",\n            cached_result.value(intensity)) );\n        }\n    }\n}\n</code></pre> <p>This works, but what we might want to do is instead of caching one value no matter what the <code>arg</code> passed in is;  we need to cache one value for each <code>arg</code> being passed because <code>arg</code> changes the value.</p> <p>This requires hashmap, where the keys will be argument passed into <code>value()</code> and value will be the result of calling the closure <code>calculation</code> if not already in the hashmap.</p> <p>We might also want to use generic function for our closure instead of hard coding the type to <code>u32</code>.</p> <pre><code>use std::collections::HashMap;\nuse std::hash::Hash;\n\nstruct Cacher&lt;F, K, V&gt; {\n    calculation: F,\n    cache: HashMap&lt;K, V&gt;,\n}\n\nimpl&lt;F, K, V&gt; Cacher&lt;F, K, V&gt;\nwhere\n    F: Fn(&amp;K) -&gt; V,\n    K: Hash + Eq,\n{\n    fn new(calculation: F) -&gt; Self {\n        Cacher {\n            calculation,\n            cache: HashMap::new(),\n        }\n    }\n\n    fn value(&amp;mut self, arg: K) -&gt; &amp;V {\n        use std::collections::hash_map::Entry;\n\n        match self.cache.entry(arg) {\n            Entry::Occupied(occupied) =&gt; occupied.into_mut(),\n            Entry::Vacant(vacant) =&gt; {\n                let value = (self.calculation)(vacant.key());\n                vacant.insert(value)\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/functional/closures/#capturing-the-environment-with-closure","title":"Capturing the environment with closure","text":"<p>Unlike functions, closure have access the variables that are defined within the scope in which the closure is defined. This are required extra memory overhead than regular functions.</p> <pre><code>fn main() {\n    let x = 4;\n\n    // closure saved in `equal_to_x` has access to `x`\n    let equal_to_x = |z| z == x;\n\n    let y = 4;\n\n    assert!(equal_to_x(y));\n}\n</code></pre> <p>Closures capture variables from there environment in three ways, encoded in function traits we talked earlier: 1. By borrowing immutably <code>Fn</code> 2. By borrowing mutably   <code>FnMut</code> 3. By taking ownership    <code>FnOnce</code></p> <p>When we create closures Rust automatically infers which traits to use based on how you use the values inside the closures environment.</p> <p>We can force the closure to take ownershup of the values it uses inside it's environemnt by using the <code>move</code> keyword in front of closure. (userful when passing closure from one thread to another)</p> <pre><code>fn main() {\n    let x = vec![1, 2, 3];\n\n    // this will work though, since it doesn't take ownership of `x`\n    // let equal_to_x = |z| z == x;\n    // this won't:\n    let equal_to_x = move |z| z == x;\n\n    println!(\"Can't use x here: {:?}\", x);\n    //                                 ^ error: borrow of moved value\n\n    let x = vec![1, 2, 3];\n\n    assert!(equal_to_x(y));\n}\n</code></pre>"},{"location":"cs/rust/tutorial/functional/iterators/","title":"Iterators in Rust","text":"<p>Info</p> <p>Iterator pattern allows us to iterate over a sequence of elements regardless of how the elements are stored.</p> <p>Processing a Series of Items with Iterators</p>"},{"location":"cs/rust/tutorial/functional/iterators/#processing-items-with-iterators","title":"Processing items with Iterators","text":"<p>Iterators can be implemented for any data structure.</p> <pre><code>fn main() {\n    let v1 = vec![1, 2, 3];\n\n    // iterators are lazy in Rust\n    let v1_iter = v1.iter();\n\n    // abstract away the logic of how to iterate over the sequence\n    // encapsulated by iterator\n    for value in v1_iter {\n        println!(\"Got: {}\", value);\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/functional/iterators/#iterator-trait-and-the-next-method","title":"Iterator Trait and the next Method","text":"<p>All iterators in Rust implements <code>Iterator</code> trait defined in standard library, something like this:</p> <p>The <code>next()</code> method returns the next Item in the sequence and requires a mutable reference to self because calling <code>next()</code> changes the internal state of iterator used to track where it is.</p> <pre><code>pub trait Iterator {\n    type Iterm;    // Associated types\n\n    fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt;;\n\n    // methods with default implementations elided\n}\n\n#[test]\nfn iterator_demonstration() {\n    let v1 = vec![1, 2, 3];\n\n    // an immutable reference\n    let mut v1_iter = v1.iter();\n\n    // for mutable references use `iter_mut()`\n\n    assert_eq!(v1_iter.next(), Some(&amp;1));\n    assert_eq!(v1_iter.next(), Some(&amp;2));\n    assert_eq!(v1_iter.next(), Some(&amp;3));\n    assert_eq!(v1_iter.next(), None);\n}\n</code></pre> <p>Associated Types</p>"},{"location":"cs/rust/tutorial/functional/iterators/#methods-that-consume-the-iterator","title":"Methods that Consume the iterator","text":"<p>Iterator trait has various methods with default implementations.</p> <p>There are two broad categories: 1. Adaptors: take in an and return another iterator 2. Consumers: take in an iterator and returns some other type.</p> <p>For example <code>sum()</code> is a type of consumer, which repeatedly calls <code>next()</code> method to get next element and add them up:</p> <pre><code>#[test]\nfn iterator_sum() {\n    let v1 = vec![1, 2, 3];\n    let total: i32  = v1.iter().sum();\n    assert_eq!(total, 6);\n}\n</code></pre>"},{"location":"cs/rust/tutorial/functional/iterators/#methods-that-produce-other-iterators","title":"Methods that Produce Other Iterators","text":"<p>Adapter methods produce other iterators, one of them is <code>map()</code> which takes in a closure and returns an iterator which calls the closure over each element in sequence:</p> <pre><code>fn main() {\n    let v1: Vec&lt;i32&gt; = vec![1, 2, 3];\n    v1.iter().map(|x| x+1);\n}\n</code></pre> <p>Since in Rust iterators are lazy, the compiler will warn about the iterator that is not used returned by <code>map()</code> and this won't actually do anything until a consumer method is called upon:</p> <pre><code>fn main() {\n    let v1: Vec&lt;i32&gt; = vec![1, 2, 3];\n    let v2: Vec&lt;_&gt; = v1.iter().map(|x| x+1).collect();\n\n    assert_eq!(v2, vec![2, 3, 4]);\n}\n</code></pre>"},{"location":"cs/rust/tutorial/functional/iterators/#closures-that-capture-their-environment","title":"Closures that capture their Environment","text":"<pre><code>#[derive(PartialEq, Debug)]\nstruct Shoe {\n    size: u32,\n    style: String,\n}\n\n// returns shoes vector that have `shoe_size`\n//\n// `shoe_size` in closure passed into the `filter()`\n// method can be accessed from the environment\nfn shoes_in_my_size(shoes: Vec&lt;Shoe&gt;, shoe_size: u32) -&gt; Vec&lt;Shoe&gt; {\n    // the iterator `into_iter()` take ownership of our vector\n    shoes.into_iter().filter(|s| s.size == shoe_size).collect()\n}\n\nfn main() {}\n\n#[cfg(test)]\nmod tests {\n    user super::*;\n\n    $[test]\n    fn filters_by_size() {\n        let shoes = vec![\n            Shoe {\n                size: 10,\n                style: String::from(\"sneaker\"),\n            },\n            Shoe {\n                size: 13,\n                style: String::from(\"sandal\")\n            },\n            Shoe{\n                size: 10,\n                style: String::from(\"book\"),\n            },\n        ];\n\n        let in_my_size = shoes_in_my_size(shoes, 10);\n\n        assert_eq!(\n            in_my_size,\n            vec![\n                Shoe {\n                size: 10,\n                style: String::from(\"sneaker\"),\n                },\n                Shoe{\n                size: 10,\n                style: String::from(\"book\"),\n                },\n            ]\n        )\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/functional/iterators/#creating-our-own-iterators","title":"Creating our own iterators","text":"<pre><code>struct Counter {\n    count: u32,\n}\n\nimpl Counter {\n    fn new() -&gt; Counter {\n        Counter { count: 0 }\n    }\n}\n\nimpl Iterator for Counter {\n    type Item = u32;\n\n    // the only method we need to implement\n    fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; {\n        if self.count &lt; 5 {\n            self.count += 1;\n            Some(self.count)\n        } else {\n            None\n        }\n    }\n}\n\n// a test for our `next()` method\n#[test]\nfn calling_next_directly() {\n    let mut counter = Counter::new();\n\n    assert_eq!(counter.next(), Some(1));\n    assert_eq!(counter.next(), Some(2));\n    assert_eq!(counter.next(), Some(3));\n    assert_eq!(counter.next(), Some(4));\n    assert_eq!(counter.next(), Some(5));\n    assert_eq!(counter.next(), None);\n}\n</code></pre> <p>Few other methods of <code>Iterator</code> traits: 1. <code>zip()</code>: 'Zips up' two iterators into a single iterator of pairs.     The first iterator is the one on which <code>zip()</code> is called on, the second is being passed into the method. 2. <code>skip()</code>: Is an adapter method returns iterator, skipping first <code>n</code> elements. 3. <code>map()</code>: Takes a closure and call it for each item in the iterator. 4. <code>filter()</code>: filter item by taking in a closure, requiring to return a <code>bool</code> value to be accepted into the generated iterator. 5.  <code>sum()</code>: cosumer method to sum up all values.</p> <pre><code>#[test]\nfn using_other_iterator_trait_methods() {\n    let sum: u32 = Counter::new()\n        .zip(Counter::new().skip(1))\n        .map(|(a, b)| a*b)      // a pair of value from previous `zip` iterator\n        .filter(|x| x%3 == 0)\n        .sum();\n    asser_eq!(18, sum);\n}\n</code></pre>"},{"location":"cs/rust/tutorial/functional/iterators/#iterators-in-practice","title":"Iterators in Practice","text":""},{"location":"cs/rust/tutorial/functional/iterators/#removing-a-clone-using-an-iterator","title":"Removing a clone Using an Iterator","text":"<pre><code>pub struct Config {\n    pub query: String,\n    pub filename: String,\n    pub case_sensitive: bool,\n}\n\nimpl Config {\n    pub fn new(args: &amp;[String]) -&gt; Result&lt;Config, &amp;str&gt; {\n        if args.len() &lt; 3 {\n            return Err(\"not enough arguments\");\n        }\n\n        let query = args[1].clone();\n        let filename = args[2].clone();\n\n        let case_sensitive = env::var(\"CASE_INSENTITIVE\").is_err();\n\n        Ok(Config {query, filename, case_senitive})\n    }\n}\n</code></pre> <p>Instead of taking ownership of array reference/slice, we can take in an iterator which means we'll have ownership over <code>args</code>, eliminating the need of <code>clone()</code>.</p> <p>Going back to <code>main.rs</code>, instead of calling the <code>collect()</code> method to convert it to a collection from an iterator, let's just pass the iterator itself: <pre><code>fn main() {\n    // let args = env::args().collect();\n\n    let config = Config::new(env::args()).unwrap_or_else(|err|  {\n        // ...\n    })\n    // ...\n}\n</code></pre></p> <p>and update the signature of <code>Config::new()</code>:</p> <pre><code>impl Config {\n    // `mut` since we'll be iterating over iterator\n    // static lifetime because now `args` is a owned type and we're\n    // returning a string slice, we do need to specify the lifetime\n    pub fn new(mut args: env::Args) -&gt; Result&lt;Config, &amp;'static str&gt; {\n        args.next();  // discard, the first cmd line argument is path to our program\n\n        // `query` is taking ownership of string inside `Some`\n        // which is owned string\n        let query = match args.next() {\n            Some(arg) =&gt; arg,\n            None =&gt; return Err(\"Didn't get a query string\"),\n        };\n\n        // filename is taking ownership of it's string\n        // no `clone()` method call required\n        let filename = match args.next() {\n            Some(arg) =&gt; arg,\n            None =&gt; return Err(\"Didn't get a file name\"),\n        };\n\n        let case_sensitive = env::var(\"CASE_INSENSITIVE\").is_err();\n\n        Ok(Config { query, filename, case_sensitive })\n    }\n}\n</code></pre> <p>Adapting <code>search()</code> function with iterator adaptor method</p> <pre><code>pub fn search&lt;'a&gt;(query: &amp;str, contents: &amp;'a str) -&gt; Vec&lt;&amp;'a str&gt; {\n    contents\n        .lines()\n        .filter(|line|  line.contains(query))\n        .collect()\n}\n</code></pre>"},{"location":"cs/rust/tutorial/functional/iterators/#loops-vs-iterators","title":"Loops v/s Iterators","text":"<p>Rust gives ability to perform zero cost abstration whch implies using higher level abstractions like iterators over loops doesn't have meaningful impact on performance. It's about the same speed, the thing is about of abstraction.</p>"},{"location":"cs/rust/tutorial/oops/object-oriented-programming/","title":"Object Oriented Programming in Rust","text":"<p>Object Oriented Programming in the context of Rust. Rust is inspired by many different programming languages, some Object oriented other functional.</p> <p>For most Object Oriented Programming languages out there, three things are prominent:</p> <ol> <li>Objects</li> <li>Encapsulation</li> <li>Inheritance</li> </ol> <p>So, lets revise them and learn if they are supported in Rust?</p> <p>Object-Oriented Programming Features of Rust</p>"},{"location":"cs/rust/tutorial/oops/object-oriented-programming/#objects","title":"Objects","text":"<p>Objects are made out of data and methods that operate on that data.</p> <p>In the context of Rust, Structs and Enums are responsible for holding the data and we use <code>impl</code> block for implementing methods for those type defined by either <code>struct</code> or <code>enum</code>.</p> <p>So, yes we get about the same functionality.</p>"},{"location":"cs/rust/tutorial/oops/object-oriented-programming/#encapsulation","title":"Encapsulation","text":"<p>Implementation details of an object are hidden from the code using that object.</p> <p>The objects interaction with outside code is limited by the methods/APIs it provide allowing programmer to change the internals of an object without changinging the code which uses that object.</p> <p>In Rust, we use <code>pub</code> keyword to decide which module, types, function, method are going to be public. Since otherwise everything is private by default.</p> <pre><code>pub struct AveragedCollection {\n    list: Vec&lt;i32&gt;,\n    average: f64,\n}\n</code></pre> <p>The struct <code>AveragedCollection</code> is public, i.e., anyone outside the library can use it but it's fields are private. This struct is going to be used to serve as a cache. We want when the <code>list</code> is updated the <code>average</code> gets updated as well. If some outside code is allowed to manipulate <code>list</code>, it might not update <code>average</code> leaving inconsistent data. We want to provide this functionality via methods.</p> <p>So something like this is necessary:</p> <pre><code>impl AveragedCollection {\n    pub fn add(&amp;mut self, value: i32) {\n        self.list.push(value);\n        self.update_average();\n    }\n\n    pub fn remove(&amp;mut self) -&gt; Option&lt;i32&gt; {\n        let result = self.list.pop();\n        match result {\n            Some(value) =&gt; {\n                self.update_average();\n                Some(value)\n            }\n            None =&gt; None,\n        }\n    }\n\n    pub fn average(&amp;self) -&gt; f64 {\n        self.average\n    }\n\n    // a private method by default\n    fn update_average(&amp;self) {\n         let total: i32 = self.list.iter().sum();\n         self.average = total as f64 / self.list.len() as f64;\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/oops/object-oriented-programming/#inheritance","title":"Inheritance","text":"<p>Inheritance is the ability for an object to inherit from another object's definition gaining the data and behavior of that other object without having to define the data and behavior itself.</p> <p>Rust doesn't support this feature of Object Oriented programming. So, we can't define structs/enums that derive it's fields and methods from other struct/enum.</p> <p>However, we do have other methods to accomplish something like that, depending on why do we want Inheritance?.</p> <p>There are two main uses of Inheritance:</p> <ol> <li>Code sharing: In Rust, we can use default trait method implementation, so that all types and hence object can get the same behvior/methods (not fields) for which they use the same trait.</li> <li> <p>Polymorphism: Polymorphism allows us to substitute multiple objects objects for each other at runtime for each other at runtime if they share certain characteristics. In traditional programming language, this is done via Parent class (say Vehicle) and child class (Scooter, truct, car) and at runtime for functions accepting Parent class, we can pass in child class.</p> <p>Rust uses different approach. You can use generics to abstract away concrete type and use trait bounds to restrict the characteristics of those types.</p> <p>Trait Bounds</p> <p>In addition to that Rust also provides Trait objects similar to generics except they use dynamic dispatch whereas generics use static dispatch.</p> </li> </ol>"},{"location":"cs/rust/tutorial/oops/state-design-pattern/","title":"State Design Pattern in Rust","text":"<p>Implementing an Object-Oriented Design Pattern</p>"},{"location":"cs/rust/tutorial/oops/state-design-pattern/#object-oriented-design-pattern","title":"Object Oriented Design Pattern","text":"<p>In the state pattern we have some value which has internal state represented by state objects.</p> <p>Each state object is reponsible for its own behavior and deciding when to transition into another state.</p> <p>The value that holds the state objects know nothing about the different behavior states or when to transition into different state.</p> <p>The benefit is that when the business requirement change, we don't need to change the code which uses the value but instead we need to change code inside one of the state objects or add new state objects</p> <p>To understand this pattern, we'll be implementing a blog post workflow in Rust.</p> <p>The workflow is kinda like this:</p> <ul> <li>A blog post start as empty draft</li> <li>After drafting a review is done</li> <li>Once it gets approved and then published post return content to be print</li> </ul> <p>Also some sequence is also required. Like, if somebody tries to review a post before a review is requested then that blog post should remain in a draft state.</p> <p>The workflow should look something like this in code, in a new library crate named <code>blog</code>:</p> <pre><code>// main.rs\nuse blog::Post;\n\nfn main() {\n    let mut post = Post::new();\n\n    post.add_text(\"I ate a salad for lunch today\");\n    assert_eq!(\"\", post.content());  // In draft state post will be empty\n\n    post.request_review();\n    assert_eq!(\"\", post.content()); // post will still be empty because it's not approved\n\n    // upon approve, `post.content()` should return actual content of the post\n    post.approve();\n    assert_eq!(\"I ate a salad for lunch today\", post.content());\n}\n</code></pre> <p>Interation here is done with <code>Post</code> type. The <code>Post</code> type will store a value representing the state of the post which is either:</p> <ul> <li>Draft</li> <li>Waiting for review or</li> <li>Published</li> </ul> <p>and this state transition is managed by the <code>Post</code> type upon invocation of methods.</p>"},{"location":"cs/rust/tutorial/oops/state-design-pattern/#defining-post","title":"Defining Post","text":"<p>Inside <code>src/lib.rs</code>:</p> <pre><code>// lib.rs\npub struct Post {\n    state: Option&lt;Box&lt;dyn State&gt;&gt;,\n    content: String,\n}\n\nimpl Post {\n    pub fn new() -&gt; Post {\n        // a constructor function with\n        // `state` set to `Draft` and `content` empty\n        Post {\n            state: Some(Box::new(Draft {})),\n            content: String::new(),\n        }\n    }\n}\n\n// `State` trait define shared behavior\n// between various state of a post\ntrait State {}\n\n// later we'll define other states as well\nstruct Draft {}\n\nimpl State for Draft {}\n</code></pre>"},{"location":"cs/rust/tutorial/oops/state-design-pattern/#storing-text","title":"Storing Text","text":"<p>We'll need to implement <code>add_text()</code> method that update the post <code>content</code> since the field is private.</p> <pre><code>// lib.rs\nimpl Post {\n    pub fn new() -&gt; Post {\n        Post {\n            state: Some(Box::new(Draft {})),\n            content: String::new(),\n        }\n    }\n\n    // a `mut` reference to `self` to mutate `content`\n    pub fn add_text(&amp;mut self, text: &amp;str) {\n        self.content.push_str(text)\n    }\n}\n</code></pre> <p><code>add_text()</code> is a  functionality we want but doens't depend on what state the post is in; and hence not part of state pattern.</p>"},{"location":"cs/rust/tutorial/oops/state-design-pattern/#content-method","title":"Content Method","text":"<pre><code>// lib.rs\nimpl Post {\n    pub fn new() -&gt; Post {\n        Post {\n            state: Some(Box::new(Draft {})),\n            content: String::new(),\n        }\n    }\n\n    pub fn add_text(&amp;mut self, text: &amp;str) {\n        self.content.push_str(text)\n    }\n\n    // this should only return if `Post` is published\n    // But since we have now only implemented `Draft` state.\n    // We simply return an empty String\n    pub fn content(&amp;self) -&gt; &amp;str {\n        \"\"\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/oops/state-design-pattern/#requesting-a-review","title":"Requesting a Review","text":"<p>First we'll add a new state called <code>PendingReview</code> at the end of the file <code>lib.rs</code>:</p> <pre><code>struct PendingReview {}\n\nimpl State for PendingReview {}\n</code></pre> <p>Then we'll define a new method on <code>State</code> trait called <code>request_review() which takes ownerhsip of</code>Self<code>and returns a</code>State` trait object:</p> <p>Remember why we want to return a trait object:</p> <p>Trait for common behavior</p> <pre><code>trait State {\n    fn request_review(self: Box&lt;Self&gt;) -&gt; Box&lt;dyn State&gt;;\n}\n</code></pre> <p>and then we'll add a custom implementation for both <code>Draft</code> and <code>PendingReview</code>:</p> <pre><code>struct Draft {}\n\nimpl State for Draft {\n    // this will take ownership of `Box&lt;T&gt;` containing `Self\n    // and there is no `Self` inside the function; we're invalidating &amp;\n    // returning a new state in it's place\n    fn request_review(self: Box&lt;Self&gt;) -&gt; Box&lt;dyn State&gt; {\n        Box::new(PendingReview {})\n    }\n}\n</code></pre> <pre><code>struct PendingReview {}\n\nimpl State for PendingReview {\n    // doens't require anything to do since it's already in PendingReview\n    fn request_review(self: Box&lt;Self&gt;) -&gt; Box&lt;dyn State&gt; {\n        self\n    }\n}\n</code></pre> <p>At last let's implement a public <code>request_review()</code> method on <code>Post</code> struct:</p> <pre><code>impl Post {\n    pub fn new() -&gt; Post {\n        Post {\n            state: Some(Box::new(Draft {})),\n            content: String::new(),\n        }\n    }\n\n    pub fn add_text(&amp;mut self, text: &amp;str) {\n        self.content.push_str(text)\n    }\n\n    pub fn content(&amp;self) -&gt; &amp;str {\n        \"\"\n    }\n\n    pub fn request_review(&amp;mut self) {\n        // `take()` the value out of the Option, leaving a `None` in its place\n        // and that what we are matching onto on the left side of if-let statement\n        if let Some(state) = self.state.take() {\n            self.state = Some(state.request_review());\n        }\n    }\n}\n</code></pre> <p>The <code>request_review()</code> method on <code>Post</code> struct is going to be same no matter what state we're in. Each state is responsible for it's own rules that govern what happend when we call <code>request_review()</code> on that particular state object.</p>"},{"location":"cs/rust/tutorial/oops/state-design-pattern/#adding-the-approve-method","title":"Adding the approve Method","text":"<p>Before we add <code>approve()</code> method to <code>Post</code> struct, we'll add a new <code>Published</code> state struct at the end of <code>lib.rs</code>.</p> <pre><code>struct Published {}\n\nimpl State for Published {\n    // if post is already Published just return this state\n    fn request_review(self: Box&lt;Self&gt;) -&gt; Box&lt;dyn State&gt; {\n        self\n    }\n}\n</code></pre> <p>Then we'll add new method to <code>State</code> trait:</p> <pre><code>trait State {\n    fn request_review(self: Box&lt;Self&gt;) -&gt; Box&lt;dyn State&gt;;\n    fn approve(self: Box&lt;Self&gt;) -&gt; Box&lt;dyn State&gt;;\n}\n</code></pre> <p>Then we'll implement this method for <code>Draft</code> stuct:</p> <pre><code>impl State for Draft {\n    fn request_review(self: Box&lt;Self&gt;) -&gt; Box&lt;dyn State&gt; {\n        // ...\n    }\n\n    // return self because approval won't work until review is requested\n    fn approve(self: Box&lt;Self&gt;) -&gt; Box&lt;dyn State&gt; {\n        self\n    }\n}\n</code></pre> <p>Same goes for <code>Published</code> state object:</p> <pre><code>impl State for Published {\n    fn request_review(self: Box&lt;Self&gt;) -&gt; Box&lt;dyn State&gt; {\n        // ...\n    }\n\n    // approval isn't required since Post is already published\n    fn approve(self: Box&lt;Self&gt;) -&gt; Box&lt;dyn State&gt; {\n        self\n    }\n}\n</code></pre> <p>But for <code>PendingReview</code> state, we want to transition to <code>Published</code> state:</p> <pre><code>impl State for PendingReview {\n    fn request_review(self: Box&lt;Self&gt;) -&gt; Box&lt;dyn State&gt; {\n        // ...\n    }\n\n    // approval isn't required since Post is already published\n    fn approve(self: Box&lt;Self&gt;) -&gt; Box&lt;dyn State&gt; {\n        Box::new(Published {})\n    }\n}\n</code></pre> <p>At last we're going to add <code>approve()</code> method to <code>Post</code> struct:</p>"},{"location":"cs/rust/tutorial/oops/state-design-pattern/#impl-post-pub-fn-approvemut-self-if-let-somestate-selfstatetake-selfstate-somestateapprove","title":"<pre><code>impl Post {\n    // ...\n\n    pub fn approve(&amp;mut self) {\n       if let Some(state) = self.state.take() {\n            self.state = Some(state.approve());\n        }\n    }\n}\n</code></pre>","text":"<p>We'll also want <code>Post</code> struct to return content of the post if it is in <code>Published</code> state.</p> <p>So let's update the <code>content()</code> method of <code>Post</code> struct:</p> <pre><code>impl Post {\n    // ...\n\n    pub fn content(&amp;self) -&gt; &amp;str {\n        self.state.as_ref().unwrap().content(self)\n    }\n\n    // ...\n}\n</code></pre> <p>We're calling <code>as_ref()</code> because <code>state</code> is going to be an <code>Option&lt;T&gt;</code> that owns the state object but instead we want reference to the state object. Because we know that there's always going to be a valid <code>State</code> object it's safe to call <code>unwrap()</code> to get the inside value of <code>Some</code> variant.</p> <p>Then we call the <code>content()</code> method (not yet implemented though) on state object, passing in <code>post</code> (which is <code>self</code>).</p> <p>The goal is to keep all these rules contained within the state objects sot the <code>content()</code> method takes a reference to the <code>Post</code> so that it has access to the <code>content</code> field on the <code>Post</code> and can return appropriate string depending upon state.</p> <p>Because of Deref coercion we were able to call <code>content()</code> method directly, even though <code>as_ref()</code> returns an <code>Option&lt;T&gt;</code> that contains a reference to <code>Box&lt;T&gt;</code> holding the state object: <code>Option&lt;&amp;Box&lt;dyn State, Global&gt;&gt;</code>.</p> <p>Now, let's implement the <code>content()</code> method for each State, but before that again we update our trait <code>State</code> with the new method with a default implementation.</p> <pre><code>trait State {\n    fn request_review(self: Box&lt;Self&gt;) -&gt; Box&lt;dyn State&gt;;\n    fn approve(self: Box&lt;Self&gt;) -&gt; Box&lt;dyn State&gt;;\n    fn content(&amp;self, post: &amp;Post) -&gt; &amp;str {\n        \"\"\n    }\n}\n</code></pre> <p>Now we'll only add custom implementation for <code>PublishedState</code> since it is the only one which should return <code>content</code>.</p> <pre><code>impl State for Published {\n    // ...\n    fn content(&amp;self, post: &amp;Post) -&gt; &amp;str {\n        &amp;post.content\n    //  ^^^^^^^^^^^^^ error: this parameter and the return type are declared with different lifetimes...\n    }\n}\n</code></pre> <p>So, we're taking in two references and returning another reference. We want to tell the compiler the relationship between input parameters's lifetimes and returned parameter lifetime. Essentially we want <code>&amp;content</code> to live as long as <code>post</code> argument.</p> <p>So using lifetimes:</p> <pre><code>impl State for Published {\n    // ...\n    fn content&lt;'a&gt;(&amp;self, post: &amp;'a Post) -&gt; &amp;'a str {\n        &amp;post.content\n    }\n}\n</code></pre> <p>and also update <code>State</code> trait signature:</p> <pre><code>trait State {\n    fn request_review(self: Box&lt;Self&gt;) -&gt; Box&lt;dyn State&gt;;\n    fn approve(self: Box&lt;Self&gt;) -&gt; Box&lt;dyn State&gt;;\n    fn content&lt;'a&gt;(&amp;self, post: &amp;'a Post) -&gt; &amp;'a str {\n        \"\"\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/oops/state-design-pattern/#trade-offs-of-the-state-pattern","title":"Trade-offs of the State Pattern","text":"<p>One downside here is that some state are coupled to each other.</p> <p>For example, imagine we want to add a state between <code>PendingReview</code> and <code>Published</code>, say <code>Scheduled</code>.</p> <p>After adding that state, we would need to update the <code>PendingReview</code> state such that when <code>approve()</code> is called it transitions to <code>Scheduled</code> state.</p> <p>The other downside is duplication. We've very similar implementation for <code>request_review()</code> and <code>approve()</code> in <code>impl</code> block for <code>Post</code> struct. We could use macros to reduce those repetition in case we had larger codebase.</p>"},{"location":"cs/rust/tutorial/oops/state-design-pattern/#final-librs","title":"Final lib.rs","text":"<p>At the end the <code>lib.rs</code> should look something like this:</p> <pre><code>// lib.rs\npub struct Post {\n    state: Option&lt;Box&lt;dyn State&gt;&gt;,\n    content: String,\n}\n\nimpl Post {\n    pub fn new() -&gt; Post {\n        Post {\n            state: Some(Box::new(Draft {})),\n            content: String::new(),\n        }\n    }\n\n    pub fn add_text(&amp;mut self, text: &amp;str) {\n        self.content.push_str(text)\n    }\n\n    pub fn content(&amp;self) -&gt; &amp;str {\n        self.state.as_ref().unwrap().content(self)\n    }\n\n    pub fn request_review(&amp;mut self) {\n       if let Some(state) = self.state.take() {\n            self.state = Some(state.request_review());\n        }\n    }\n\n    pub fn approve(&amp;mut self) {\n       if let Some(state) = self.state.take() {\n            self.state = Some(state.approve());\n        }\n    }\n}\n\ntrait State {\n    fn request_review(self: Box&lt;Self&gt;) -&gt; Box&lt;dyn State&gt;;\n    fn approve(self: Box&lt;Self&gt;) -&gt; Box&lt;dyn State&gt;;\n    fn content&lt;'a&gt;(&amp;self, post: &amp;'a Post) -&gt; &amp;'a str {\n        \"\"\n    }\n}\n\n// later we'll define other states as well\nstruct Draft {}\n\nimpl State for Draft {\n    fn request_review(self: Box&lt;Self&gt;) -&gt; Box&lt;dyn State&gt; {\n        Box::new(PendingReview {})\n    }\n\n    fn approve(self: Box&lt;Self&gt;) -&gt; Box&lt;dyn State&gt; {\n        self\n    }\n}\n\nstruct PendingReview {}\n\nimpl State for PendingReview {\n    fn request_review(self: Box&lt;Self&gt;) -&gt; Box&lt;dyn State&gt; {\n        self\n    }\n\n    fn approve(self: Box&lt;Self&gt;) -&gt; Box&lt;dyn State&gt; {\n        Box::new(Published {})\n    }\n}\n\nstruct Published {}\n\nimpl State for Published {\n    fn request_review(self: Box&lt;Self&gt;) -&gt; Box&lt;dyn State&gt; {\n        self\n    }\n\n    fn approve(self: Box&lt;Self&gt;) -&gt; Box&lt;dyn State&gt; {\n        self\n    }\n\n    fn content&lt;'a&gt;(&amp;self, post: &amp;'a Post) -&gt; &amp;'a str {\n        &amp;post.content\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/oops/state-design-pattern/#encoding-states-and-behavior-as-types","title":"Encoding States and Behavior as Types","text":"<p>By implementing the state Design pattern in Rust as we'd have done in traditional object oriented programming language, we are not taking the full advantage of Rust. We'd want to implement the library such that invalid states and transition are compile time error.</p> <p>Instead of encapsulating various states as different struct, hiding away the implementation, we'll encode different states as different types.</p> <p>Let's rewrite <code>lib.rs</code>:</p> <pre><code>pub struct Post {\n    content: String,\n}\n\npub struct DraftPost {\n    content: String,\n}\n</code></pre> <p>and then add <code>impl</code> block for both:</p> <pre><code>impl Post {\n    pub fn new() -&gt; DraftPost {\n        DraftPost {\n            content: String::new(),\n        }\n    }\n\n    pub fn content(&amp;self) -&gt; &amp;str {\n        &amp;self.content\n    }\n}\n\nimpl DraftPost {\n    pub fn add_text(&amp;mut self, text: &amp;str) {\n        self.content.push_str(text);\n    }\n}\n</code></pre> <p>Our structs don't need <code>state</code> field anymore because we're moving the encoding of state to the type of struct.</p> <p>The <code>DraftPost</code> struct also don't have content method, disabling any draft post to return their private <code>content</code>.</p> <p>Also only way to create a <code>DraftPost</code> is to call <code>new()</code> function of <code>Post</code> struct and no way to create an instance of <code>Post</code> struct.</p> <p>Next, we want ability to review a post, so we'll add <code>PendingReviewPost</code>:</p> <p>Notice we use <code>self</code> and not <code>&amp;self</code>.</p> <pre><code>pub struct PendingReviewPost {\n    content: String,\n}\n\nimpl PendingReviewPost {\n    pub fn approve(self) -&gt; Post {\n        Post {\n            content: self.content,\n        }\n    }\n}\n</code></pre> <p>Only <code>PendingReviewPost</code> will have <code>approve()</code> method, returning a <code>Post</code> instance.</p> <p>To get to pending review state, we'll add in a method on <code>DraftPost</code> to request for review:</p> <pre><code>impl DraftPost {\n    // ...\n\n    pub fn request_review(self) -&gt; PendingReviewPost {\n        PendingReviewPost {\n            content: self.content\n        }\n    }\n}\n</code></pre> <p><code>request_review()</code> is going to return a <code>PendingReviewPost</code>. <code>request_review()</code> and <code>approve()</code> take ownership of <code>self</code>,  meaning they'll consume and invalidate the old state and return a new state.</p> <p>Also, now only way to get <code>PendingReviewPost</code> is to <code>request_review()</code> on <code>DraftPost</code>.  We've now encoded the workflow into type system.</p> <p>Our final <code>lib.rs</code> should look like:</p> <pre><code>// lib.rs\npub struct Post {\n    content: String,\n}\n\npub struct DraftPost {\n    content: String,\n}\n\nimpl Post {\n    pub fn new() -&gt; DraftPost {\n        DraftPost {\n            content: String::new(),\n        }\n    }\n\n    pub fn content(&amp;self) -&gt; &amp;str {\n        &amp;self.content\n    }\n}\n\nimpl DraftPost {\n    pub fn add_text(&amp;mut self, text: &amp;str) {\n        self.content.push_str(text);\n    }\n\n    pub fn request_review(self) -&gt; PendingReviewPost {\n        PendingReviewPost {\n            content: self.content\n        }\n    }\n}\n\npub struct PendingReviewPost {\n    content: String,\n}\n\nimpl PendingReviewPost {\n    pub fn approve(self) -&gt; Post {\n        Post {\n            content: self.content,\n        }\n    }\n}\n</code></pre> <p>Latly, let's update <code>main.rs</code>:</p> <ul> <li>get rid of <code>post.content()</code> since this method cannot be called unless the method is published.</li> <li><code>request_review()</code> and <code>approve()</code> no longer change the internal state, instead they return a new post type.</li> </ul> <pre><code>// main.rs\nuse blog::Post;\n\nfn main() {\n    let mut post = Post::new();\n\n    post.add_text(\"I ate a salad for lunch today\");\n\n    let post = post.request_review();\n\n    let post = post.approve();  // the regular `Post` instance has `content()` method\n    assert_eq!(\"I ate a salad for lunch today\", post.content());\n}\n</code></pre> <p>Now invalid states are impossible to be represent using the type system.</p> <p>Using Object Oriented patterns won't always be the best approach because of features such as Ownership in Rust that traditional Object Oriented programming languages don't have.</p>"},{"location":"cs/rust/tutorial/oops/trait-objects/","title":"Using Trait Objects in Rust","text":"<p>Although Rust doesn't support classical inheritance, it does support polymorphism through generics and Trait Objects.</p> <p>Using Trait Objects That Allow for Values of Different Types</p>"},{"location":"cs/rust/tutorial/oops/trait-objects/#trait-for-common-behavior","title":"Trait for Common Behavior","text":"<p>Imagine we're building a GUI library to take a list of visual components (buttons, checkbox, etc) and draw them to screen. In addition to that we'd like our users to extend that library, i.e., they could create their own visual components and draw them on screen.</p> <p>All these visual components are going to have a method called <code>draw()</code>.</p> <p>In traditional programming concepts, we're going to have a base class like <code>VisualComponent</code> having a <code>draw()</code> method, other visual component will inherit from this class. They'll also be able to override the <code>draw()</code> method with their own implementation.</p> <p>In Rust, we define shared behavior using traits, so let's write a trait <code>draw</code> with a method <code>draw()</code>:</p> <p>We'll create a new project with structure like this, having <code>main.rs</code> and <code>lib.rs</code>:</p> <pre><code>.\n\u251c\u2500\u2500 Cargo.toml\n\u251c\u2500\u2500 .gitignore\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 lib.rs\n    \u2514\u2500\u2500 main.rs\n</code></pre> <p>In our <code>lib.rs</code>:</p> <pre><code>// lib.rs\npub trait Draw {\n    fn draw(&amp;self);\n}\n\npub struct Screen {\n    pub components: Vec&lt;Box&lt;dyn Draw&gt;&gt;,\n}\n</code></pre> <p>Here, <code>components</code> is vector of trait object.</p> <p>We define a trait object by first specifying some sort of pointer such as a reference or a <code>Box&lt;T&gt;</code> smart pointer then using <code>dyn</code> keyword followed by the trait name. <code>dyn</code> stands for dynamic dispatch.</p> <p>Why trait object need to use some sort of pointer? Here is the answer:</p> <p>Dynamically sized types</p> <p>Now, Rust will ensure at compile time that any object in this vector implements the <code>Draw</code> trait.</p> <p>Let's create another <code>impl</code> block for <code>Screen</code> that implements <code>run()</code> that iterate through components and run them.</p> <pre><code>// lib.rs\n// in continuation ...\nimpl Screen {\n    pub fn run(&amp;self) {\n        // `iter()` let's us iterate without taking ownership\n        for component in self.components.iter() {\n            component.draw()\n        }\n    }\n}\n</code></pre> <p>So, now a component is anything that implements <code>Draw</code> trait. These component can then be iterated and by calling their <code>run()</code> method will be drawn to the screen.</p> <p>But why not generics?</p> <p>Let's talk about that,</p> <pre><code>pub struct Screen&lt;T: Draw&gt; {  // a trait bound\n    pub components: Vec&lt;T&gt;,   // vec will store anything of type T\n}\n\nimpl&lt;T&gt; Screen&lt;T&gt;\nwhere\n    T: Draw,\n{\n    pub fn run(&amp;self) {\n        for component in self.components.iter() {\n            component.draw();\n        }\n    }\n}\n</code></pre> <p>So, isn't this the same functionality as our trait object implementation?</p> <p>But there is one main difference here.</p> <p>Our list <code>components</code> can only store a list of one type of component that implements <code>Draw</code> trait. So the list is homogeneous.</p> <pre><code>let components: Vec&lt;Slider&gt;;\nlet components: Vec&lt;CheckBox&gt;;\nlet components: Vec&lt;Button&gt;;\n</code></pre> <p>It won't be possible to store a mixture of different components. But using trait objects, do have a performance cost.</p>"},{"location":"cs/rust/tutorial/oops/trait-objects/#implementing-the-trait","title":"Implementing the Trait","text":"<p>Let's implement some components:</p> <pre><code>// lib.rs\npub trait Draw {\n    fn draw(&amp;self);\n}\n\npub struct Screen {\n    pub components: Vec&lt;Box&lt;dyn Draw&gt;&gt;,\n}\n\nimpl Screen {\n    pub fn run(&amp;self) {\n        // `iter()` let's us iterate without taking ownership\n        for component in self.components.iter() {\n            component.draw()\n        }\n    }\n}\n\n// A component can implement different method other than\n// what required by Trait such `on_click()` inside `impl Button {}`\npub struct Button {\n    // these fields are relevant to `Button` component\n    // another component might have different fields\n    pub width: u32,\n    pub height: u32,\n    pub label: String,\n}\n\nimpl Draw for Button {\n    fn draw(&amp;self) {\n        // draw button\n    }\n}\n</code></pre> <p>And this is how consumers are going to define there own drawable components:</p> <p>In <code>main.rs</code>:</p> <pre><code>use gui_lib::{Screen, Button, Draw};\n\n// custom component\nstruct SelectBox {\n    width: u32,\n    height: u32,\n    options: Vec&lt;String&gt;\n}\n\nimpl Draw for SelectBox {\n    fn draw(&amp;self) {\n        // draw select box\n    }\n}\n\nfn main() {\n    let screen = Screen {\n        components: vec![\n            Box::new(SelectBox {\n                width: 100,\n                height: 100,\n                options: vec![\n                    String::from(\"yes\"),\n                    String::from(\"no\"),\n                    String::from(\"maybe\"),\n                ]\n            }),\n            Box::new(Button {\n                width: 100,\n                height: 100,\n                label: String::from(\"ok\")\n            })\n        ]\n    };\n\n    screen.run();\n}\n</code></pre> <p>Because we're using trait object Rust will ensure at compile time that every component in the list <code>components</code> implements the <code>Draw</code> trait.</p> <p>If we add in an element that does not implement <code>Draw</code> trait, like so:</p> <pre><code>let screen = Screen {\n        components: vec![\n            Box::new(String::from(\"test\")),\n        // ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        // error: the trait bound `String: Draw` is not satisfied required for the\n        // cast to the object type `dyn Draw`.\n            Box::new(SelectBox {\n                width: 100,\n                height: 100,\n                options: vec![\n                    String::from(\"yes\"),\n                    String::from(\"no\"),\n                    String::from(\"maybe\"),\n                ]\n            }),\n            //...\n</code></pre>"},{"location":"cs/rust/tutorial/oops/trait-objects/#static-vs-dynamic-dispatch","title":"Static vs Dynamic Dispatch","text":"<p>Monomorphization is a process where the compiler will generate non-generic implementations of functions based on the concrete types used in place of generic types.</p> <p>For eg., we have a generic function called <code>add()</code> which takes two generic parameters and adds them. To use that function with floating pointer numbers or integers, the compiler will generate <code>integer_add()</code> and then a <code>float_add()</code>, and then find all invocation of <code>add()</code> method and replace them concrete function for individual types.</p> <p>So, we're taking a generic implementation and substituting it for concrete implementation. This is called as Static Dispatch.</p> <p>In dynamic dispatch, the compiler does not know the concrete methods you're calling at compile time so instead it figures that out at run time.</p> <p>When using Trait object the Rust compiler must use Dynamic dispatch because the compiler doens't know all the concrete objects that are going to be used at compile time. The compiler will add code to figure out the correct method to call at runtime adding up performance cost.</p>"},{"location":"cs/rust/tutorial/oops/trait-objects/#object-safety-for-trait-objects","title":"Object Safety for Trait Objects","text":"<p>We can only make object safe traits into trait bounds.</p> <p>To be object safe;</p> <p>A trait is object safe when all of the methods implemented on that trait have these two properties:</p> <ol> <li>The return type is not <code>Self</code>.</li> <li>There are no generic parameters</li> </ol>"},{"location":"cs/rust/tutorial/patterns/pattern-syntax/","title":"Pattern Syntax in Rust","text":"<p>All valid syntax to use with pattern matching.</p> <p>Pattern Syntax</p>"},{"location":"cs/rust/tutorial/patterns/pattern-syntax/#matching-literals","title":"Matching Literals","text":"<p>This pattern is useful when we want our code to take action when it receives a concrete value.</p> <pre><code>fn main() {\n    let x = 1;\n\n    match x {\n        1 =&gt; println!(\"one\"),\n        2 =&gt; println!(\"two\"),\n        3 =&gt; println!(\"three\"),\n        _ =&gt; println!(\"anything\"),\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/patterns/pattern-syntax/#matching-named-variables","title":"Matching Named Variables","text":"<p>Second branch is using named variable pattern to match other variable shadowing any variable outside of match block.</p> <pre><code>fn main() {\n    let x = Some(5);\n    let y = 10;\n\n    match x {\n        Some(50) =&gt; println!(\"Got 50\"),\n        Some(y) =&gt; println!(\"Matched, y = {:?}\", y),\n        _ =&gt; println!(\"Default case, x = {:?}\", x).\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/patterns/pattern-syntax/#multiple-patterns","title":"Multiple patterns","text":"<pre><code>fn main() {\n    let x = 1;\n\n    match x {\n        1 | 2 =&gt; println!(\"one or two\"), // using OR operator |\n        2 =&gt; println!(\"three\"),\n        3 | 4 | 5 =&gt; println!(\"greater than 2 or equal to 5\"),\n        _ =&gt; println!(\"anything\"),\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/patterns/pattern-syntax/#matching-ranges-of-values","title":"Matching Ranges of Values","text":"<p>The range operator only works on numeric values and characters.</p> <pre><code>fn main() {\n    let x = 5;\n\n    match x {\n        1..=5 =&gt; println!(\"one through five\"),\n        _ =&gt; println!(\"something else\"),\n    }\n\n    let x = 'c';\n\n    match x {\n        'a'..='j' =&gt; println!(\"early ASCII letter\"),\n        'k'..='z' =&gt; println!(\"late ASCII letter\"),\n        _ =&gt; println!(\"something else\"),\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/patterns/pattern-syntax/#destructuring-to-break-apart-values","title":"Destructuring to Break Apart Values","text":""},{"location":"cs/rust/tutorial/patterns/pattern-syntax/#destructuring-structs","title":"Destructuring structs","text":"<p>We deconstruct <code>p</code> a <code>Point</code> instance to create <code>a</code> and <code>b</code>.</p> <p><code>a</code> is going to be mapped to whatever values contained in <code>x</code> and <code>b</code> to <code>y</code>.</p> <pre><code>struct Point {\n    x: i32,\n    y: i32,\n}\n\nfn main() {\n    let p = Point { x: 0, y: 7};\n\n    let point { x: a, y: b} = p;\n    assert_eq!(0, a);\n    assert_eq!(7, b);\n}\n</code></pre> <p>However it's common to have variables inside of a pattern match the field names, so:</p> <pre><code>let point { x, y } = p;\nassert_eq!(0, x);\nassert_eq!(7, y);\n</code></pre> <p>When destructuring a struct we can use named variables and literals.</p> <pre><code>fn main() {\n    let p = Point { x: 0, y: 7};\n\n    match p {\n        Point { x, y: 0 } =&gt; {    // `y` must be zero\n            println!(\"On the x axis at {}\", x)\n        },\n        Point { x: 0, y} =&gt; {     // `x` must be zero\n            println!(\"On the y axis at {}\", y)\n        },\n        Point { x, y } =&gt; {\n            println!(\"On neither axis: ({}, {})\", x, y)\n        }\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/patterns/pattern-syntax/#destructuring-enums","title":"Destructuring Enums","text":"<pre><code>enum Message {\n    Quit,                          // unit like variant\n    Move { x: i32, y: i32},        // struct variant\n    Write(String),                 // tuple variant called `Write`\n    ChangeColor(i32, i32, i32),    // tuple variant with 3 values called `ChangeColor`\n}\n\nfn main() {\n    let msg = Message::ChangeColor(0, 160, 255);\n\n    match msg {\n        Message::Quit =&gt; {\n            // a semicolon isn't required if there is only one expression\n            println!(\"Quit\");\n        }\n        Message::Move { x, y } =&gt; {\n            println!(\"Move to x: {} y: {}\", x, y)\n        }\n        Message::Write(text) =&gt; {\n            println!(\"Text message: {}\", text)\n        }\n        Message::ChangeColor(r, g, b) =&gt; {\n            println!(\"Change color: red {}, green {}, and blue {}\", r, g, b)\n        }\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/patterns/pattern-syntax/#destructuring-nested-structs-and-enums","title":"Destructuring nested structs and Enums","text":"<pre><code>enum Color {\n    Rgb(i32, i32, i32),\n    Hsv(i32, i32, i32),\n}\n\nenum Message {\n    Quit,                          // unit like variant\n    Move { x: i32, y: i32},        // struct variant\n    Write(String),                 // tuple variant called `Write`\n    ChangeColor(Color),            // tuple variant with 3 values called `ChangeColor`\n}\n\nfn main() {\n    let msg = Message::ChangeColor(\n        Color::Hsv(0, 160, 255)\n    );\n\n    match msg {\n        Message::ChangeColor(Color::Rgb(r, g, b)) =&gt; {\n            println!(\"Change color: red {}, green {}, and blue {}\", r, g, b)\n        }\n        Message::ChangeColor(Color::Hsv(h, s, v)) =&gt; {\n            println!(\"Change color: hue {}, saturation {}, and value {}\", h, s, v)\n        }\n    }\n    _ =&gt; (),\n}\n</code></pre> <pre><code>struct Point {\n    x: i32,\n    y: i32,\n}\n\nfn main() {\n    // destruct 3 and 10 into `feet` and `inches`\n    let ((feet, inches), Point { x, y }) = ((3, 10), Point { x: 3, y: -10 });\n}\n</code></pre>"},{"location":"cs/rust/tutorial/patterns/pattern-syntax/#ignoring-values-in-a-pattern","title":"Ignoring Values in a Pattern","text":"<p><code>_</code> can be used anywhere to signify ingoring of the value.</p> <p>This could be useful when we need a function signature to be something specific but we're not gonna use all the arguments passed in. The <code>_</code> avoid compiler warning for unused parameters.</p> <pre><code>fn main() {\n    foo(3, y);\n}\n\nfn foo(_, i32, y: i32) {\n    println!(\"This code only uses the y parameter: {}\", y);\n}\n</code></pre>"},{"location":"cs/rust/tutorial/patterns/pattern-syntax/#to-ignore-part-of-a-value","title":"To ignore part of a value","text":"<p>If the setting has no value then we can set a value, however if the setting already has a value then we can't modify it. <pre><code>fn main() {\n    let mut setting_value = Some(5);\n    let new_setting_value = Some(10);\n\n    match (setting_value, new_setting_value) {\n        // if both are `Some&lt;T&gt;` variant then\n        (Some(_), Some(_)) =&gt; {\n            println!(\"Can't overwrite an existing customized value\")\n        }\n        _ =&gt; {\n            setting_value = new_setting_value\n        }\n    }\n\n    println!(\"setting is {:?}\", setting_value);\n}\n</code></pre></p>"},{"location":"cs/rust/tutorial/patterns/pattern-syntax/#to-ignore-values-at-multiple-places-withing-one-pattern","title":"To ignore values at multiple places withing one pattern","text":"<pre><code>fn main() {\n    let numbers = (2, 4, 8, 16, 32);\n\n    match numbers {\n        (first, _, third, _, fifth) =&gt; {\n            println!(\"Some numbers: {}, {}, {}\", first, third, fifth)\n        }\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/patterns/pattern-syntax/#unused-variables","title":"Unused variables","text":"<p>The compiler won't complain.</p> <pre><code>fn main() {\n    let _x = 5;\n    let y = 10;\n}\n</code></pre> <p>Prefixing a name with <code>_</code> is different than just using <code>_</code>.</p> <p>Prefixing a variable name with <code>_</code> still binds the value.</p> <p><code>_s</code> is still binding with <code>s</code>.</p> <pre><code>fn main() {\n    let s = Some(String::from(\"Hello!\"));\n\n    // to avoid compiler complain from not using `s` inside we prefix it with `_`\n    // but this moves value to `_s`\n    if let Some(_s) = s {\n        println!(\"found a string\")\n    }\n\n    println!(\"{:?}\", s);\n    //               ^ error: moved value\n}\n</code></pre> <p>Instead if we would have done this, then it would successfully compile:</p> <pre><code>fn main() {\n    let s = Some(String::from(\"Hello!\"));\n\n    if let Some(_) = s {\n        println!(\"found a string\")\n    }\n\n    println!(\"{:?}\", s);\n}\n</code></pre>"},{"location":"cs/rust/tutorial/patterns/pattern-syntax/#range-syntax-to-ignore-values","title":"Range syntax to ignore values","text":"<pre><code>fn main() {\n    struct Point {\n        x: i32,\n        y: i32,\n    }\n\n    let origin = Point { x: 0, y:0, z: 0 };\n\n    match origin {\n        // ignore all field except `x`\n        Point { x, .. } =&gt; println!(\"x is {}\", x),\n    }\n}\n</code></pre> <pre><code>fn main() {\n    let numbers = (2, 4, 8, 16, 32);\n\n    match numbers {\n        (first, ..., last) =&gt; {\n            println!(\"Some numbers: {}, {}\", first, last);\n        }\n    }\n}\n</code></pre> <p>But they should be unambigous. Something like this won't work:</p> <pre><code>(..., second, ...) =&gt; { }\n</code></pre>"},{"location":"cs/rust/tutorial/patterns/pattern-syntax/#match-guards","title":"Match Guards","text":"<p>A match guard is an additional if condition specified after the pattern in a match arm that must also match along with the pattern.</p> <p>Useful for expressing complex ideas that patterns themselves cannot express.</p> <pre><code>fn main() {\n    let num = Some(4);\n\n    match num {\n        Some(x) if x &lt; 5 =&gt; println!(\"less than five: {}\", x),\n        //      ^^^^^^^^ a match guard\n        Some(x) =&gt; println!(\"{}\", x),\n        None =&gt; (),\n    }\n}\n</code></pre> <p>Match guards also solve issues with shadowing inside the match block.</p> <p>Here in this example, we want to execute some code when <code>x</code> is equal to <code>y</code>.</p> <p>We can't use <code>y</code> inside the pattern because it will shadow the outside <code>y</code>. But we could use <code>y</code> inside of our match guard.</p> <pre><code>fn main() {\n    let x = Some(5);\n    let y = 10;\n\n    match num {\n        // match on any `Some&lt;T&gt;`; bind it to `n` and compare it to `y`.\n        Some(n) if n == y =&gt; println!(\"Matched: x = {}\", n),\n        _ =&gt; println!(\"Default case, x = {:?}\", x),\n    }\n}\n</code></pre> <p>Multiple patterns with match guards:</p> <pre><code>fn main() {\n    let x = 4;\n    let y = false;\n\n    match x {\n        // `x` has to either 4 or 5 or 6 AND `y` has to be true\n        4 | 5 | 6 if y =&gt; println!(\"yes\"),\n        _ =&gt; println!(\"no\"),\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/patterns/pattern-syntax/#bindings","title":"Bindings","text":"<p><code>@</code> operator let's us create a variable that holds a value at the same time we're testing that value to see whether it matches a pattern.</p> <pre><code>fn main() {\n    enum Message {\n        Hello { id: i32 },\n    }\n\n    let msg = Message::Hello { id: 5 };\n\n    match msg {\n        Message::Hello {\n            // match `id` between 3 and 7 and also store it in `id_variable`\n            // can be same as field name; instead of `id_variable`\n            id: id_variable @ 3..=7,\n        } =&gt; println!(\"Found and id in range: {}\", id_variable),\n        Message::Hello { id: 10..=12 } =&gt; { // otherwise simpy use range operator\n            println!(\"Found an id in another range\")\n        }\n        Message::Hello { id } =&gt; {          // to bind the variable\n            println!(\"Found some other id: {}\", id)\n        }\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/patterns/patterns-and-matching/","title":"Patterns and Matching in Rust","text":"<p>What are patterns?</p> <p>Patterns are special syntax in Rust for matching against the structure of types.</p> <p>Patterns consist of some combination of following: - Literals - Destructed     - Arrays     - Enums     - Structs     - Tuples - Variables - Wildcards - Placeholders</p> <p>These components describe the shape of the data we're working with which we can then match against values to determine whether out program has the correct data to continue running or not.</p> <p>Patterns and Matching</p>"},{"location":"cs/rust/tutorial/patterns/patterns-and-matching/#match-arms","title":"match Arms","text":"<p>We already know to use patterns inside <code>match</code> expressions:</p> <pre><code>fn main() {\n    enum Language {\n        English,\n        Spanish,\n        Russian,\n        Japanese,\n    }\n\n    let language = Language::English;\n\n    // a match arm\n    match language {\n        // patterns =&gt; expressions,\n        Language::English =&gt; println!(\"Hello World!\"),\n        Language::Spanish =&gt; println!(\"Hola Mundo!\"),\n        // _ =&gt; println!(\"Unsupported language!\")\n    }\n}\n</code></pre> <p><code>match</code> expressions should be exhaustive, i.e., every possible value for the variable has to be accounted for.</p> <p>That's why the above program does not compile due to non-exhaustive patterns, we've not added <code>Language::Russian</code> and <code>Language::Japanese</code>. We could add that but there is another way to fix this using a catch-all pattern:</p> <pre><code>_ =&gt; println!(\"Unsupported language\")\n</code></pre> <p>So, if our program goes through all of these arms and none of them matches, then we execute the catch-all arm (it doesn't need to be at the end always).</p> <p>The underscore (<code>_</code>) doensn't bind to the variable we're matching on. If we did want to bind to the variable, we could do:</p> <pre><code>lang =&gt; println!(\"Unsupported language! {:?}\", lang)\n</code></pre>"},{"location":"cs/rust/tutorial/patterns/patterns-and-matching/#conditional-if-let-expressions","title":"Conditional if-let Expressions","text":"<p>You use <code>if-let</code> expressions if you want to match on some variable but you only care about one case.</p> <p>Using if let Syntax</p> <p>This piece code determines users's authorization status.</p> <pre><code>fn main() {\n    let authorization_status: Option&lt;&amp;str&gt; = None; // Option containing \"string slice\"\n    let is_admin = false;\n    let group_id: Result&lt;u8, _&gt; = \"34\".parse(); // Result containing either `u8` or error\n\n    if let Some(status) = authorization_status {\n        println!(\"Authorization status: {}\", status);\n    } else if is_admin {\n        println!(\"Authorizatin status: admin\");\n    } else if let Ok(group_id) = group_id {\n        if group_id &gt; 30 {\n            println!(\"Authorization status: priviledge\");\n        } else {\n            println!(\"Authorization status: basic\");\n        }\n    } else {\n        println!(\"Authorization status: guest\");\n    }\n}\n</code></pre> <p>The downside of <code>if-let</code> expressions is that the compiler doesn't enfore that they are exhaustive, i.e., you can get away with not writing last <code>else</code> case.</p>"},{"location":"cs/rust/tutorial/patterns/patterns-and-matching/#while-let-conditional-loops","title":"While let Conditional Loops","text":"<p>while-let loop allows us to continue the loop as long as the pattern specified continues to match.</p> <pre><code>fn main() {\n    let mut stack = Vec::new();\n\n    stack.push(1);\n    stack.push(2);\n    stack.push(3);\n\n    while let Some(top) = stack.pop() {\n        println!(\"{}\", top);\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/patterns/patterns-and-matching/#for-loops","title":"for loops","text":"<p>Use pattern to destructure the tuple into two variables.</p> <pre><code>fn main() {\n    let v = vec!['a', 'b', 'c'];\n\n    // for PATTERN in variable { }\n    for (index, value) in v.iter().enumerate() {\n        println!(\"{} is at index {}\", value, index);\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/patterns/patterns-and-matching/#let-statements","title":"let Statements","text":"<pre><code>fn main() {\n    let x = 5;\n\n    // let PATTERN = EXPRESSION\n\n    let (x, y, z) = (1, 2, 3); // to destruct; since pattern match 3 variables\n\n    // But...\n    let (x, y) = (1, 2, 3);\n    //  ^^^^^^ error: mismatched types\n    //  expected tuple `({integer}, {integer}, {integer})` found tuple `(_, _)`\n\n    // use _ to ignore values\n    let (x, y, _) = (1, 2, 3);\n}\n</code></pre>"},{"location":"cs/rust/tutorial/patterns/patterns-and-matching/#function-parameters","title":"Function Parameters","text":"<pre><code>fn main() {\n    let point = (3, 5); // you do require brackets to identify as tuple\n    print_coordinates(&amp;point);\n}\n\n// decontruct tuple into `x` and `y`.\nfn print_coordinates(&amp;(x, y): &amp;(i32, i32)) {\n    println!(\"Current location: ({}, {})\", x, y);\n}\n</code></pre>"},{"location":"cs/rust/tutorial/patterns/patterns-and-matching/#irrefutable-and-refutable-patterns","title":"Irrefutable and Refutable patterns","text":"<p>Irrefutable patterns are those patterns that will always match</p> <p>Refutable patterns might not match.</p> <p>Only accept irrefutable patterns: - function parameters - let statements - for loops</p> <p>if-let and while-let expressions accepts both refutable and irrefutable patterns but we'll get a compiler warning if we use an irrefutable pattern because those expressions are meant to handle matching failures.</p> <pre><code>fn main() {\n    // Irrefutable\n    let x = 5;\n\n    // Refutable\n    let x: Option&lt;&amp;str&gt; = None;\n    if let Some(x) = x {\n        println!(\"{}\", x);\n    };\n}\n</code></pre>"},{"location":"cs/rust/tutorial/smart-pointers/","title":"Index","text":"<p>title: \"Smart Pointers\" description: \"\" lead: \"\" date: 2022-09-20T07:58:13+01:00 lastmod: 2022-09-20T07:58:13+01:00 draft: false images: [] weight: 19</p>"},{"location":"cs/rust/tutorial/smart-pointers/box-smart-pointer/","title":"Box Smart Pointer in Rust","text":"<p>To understand Box smart pointer, we'll need to revise our concept of pointers.</p> <p>What is pointer?</p> <p>A pointer is a variable that stores the memory address that points to some other data in memory.</p> <p>The most common pointer in Rust is a Reference. References simply borrows the value they point to without getting the ownership of the data. The simplicity means they don't have much overhead, unlike smart pointer which we are going to discuss.</p> <p>What are smart pointers?</p> <p>Smart pointers are data structures that act like a pointer but have metadata and extra capabilities.</p> <ul> <li>In many cases the smart pointers own the data they point to.</li> <li>Strings and Vectors is one such example of smart pointer.</li> <li>The implementation involves structs combined with <code>Deref</code> and <code>Drop</code> traits.</li> </ul> <p><code>Deref</code> trait allows instances of our smart pointer struct to be treated like references.</p> <p><code>Drop</code> trait allows us to customize the code that is run when an instance of your smart pointer goes out of scope.</p> <p>In this section, we'll covering most commonly seen smart pointers in Rust. Many third-party libraries implement their own custom smart pointers.</p> <p>Using Box to Point to Data on the Heap"},{"location":"cs/rust/tutorial/smart-pointers/box-smart-pointer/#using-a-box-to-store-data","title":"Using a Box to Store Data","text":"<p>Box smart pointer</p> <p><code>Box</code> smart pointer allows us to store data on the Heap. They don't add much overhead except storing data on heap, but then you also don't provide you much capabilities.</p> <p>Useful when:</p> <ul> <li>When we have a type whose exact size cannot be known at compile time and we want to use a value of that type in a context which requires knowing the exact size.</li> <li>When we have large amount of data and we want to transfer ownership without being copied.</li> <li>When we own a value and we only care that the value implements a specific trait rather than it being a specific type(trait object).</li> </ul> <p>The code below shows a simple example, but it isn't really viable. Storing the value on stack will make a lot more sense. <pre><code>fn main() {\n    // `b` stores the pointer on stack to that memory location on heap\n    let b = Box::new(5); // store 5 on the heap\n    println!(\"b = {}\", b); // the boxed value can be used as if it was stored on stack\n}\n</code></pre></p>"},{"location":"cs/rust/tutorial/smart-pointers/box-smart-pointer/#enabling-recursive-types-with-boxes","title":"Enabling Recursive Types with Boxes","text":"<p>Rust needs to know how much space a type takes up at compile type.</p> <p>But in the given below example (and with recursive enums), we can recurse forever and we won't know how much space an enum can take.</p> <p>Here we implement Cons List that comes from Lisp Programming language.</p> <pre><code>// A recursive enum with two variants\nenum List {\n    Cons(i32, List),\n    Nil,\n}\n\nuse List::{Cons, Nil};\n\nfn main() {\n    // A Con cell that stores 1, then a Con cell that stores 2\n    // and finally another cell that stores 3 and at last Nil\n    let list = Cons(1, Cons(2, Cons(3, Nil)));\n}\n</code></pre> <p>This fails with error:</p> <pre><code>error[E0072]: recursive type `List` has infinite size\n --&gt; src/main.rs:2:1\n  |\n2 | enum List {\n  | ^^^^^^^^^ recursive type has infinite size\n3 |     Cons(i32, List),\n  |               ---- recursive without indirection\n  |\nhelp: insert some indirection (e.g., a `Box`, `Rc`, or `&amp;`) to make `List` representable\n  |\n3 |     Cons(i32, Box&lt;List&gt;),\n  |               ++++    +\n</code></pre> <p>How Rust computes the size of non-recursive enums</p> <p>It's going to go through each variant and see how much size a variant needs.</p> <p>So for example, <pre><code>enum Message {\n    Quit,                       // no space required\n    Move { x: i32, y: i32 },    // 2-integers\n    Write(String),              // a string\n    ChangeColor(i32, i32, i32)  // 3-integers\n}\n</code></pre></p> <p>Since the <code>enum</code> can take the maximum space that a variant with maximum space required is gonna use. The same can be said for the <code>List</code> enum we defined above.</p> <pre><code>enum List {\n    Cons(i32, List),  // this does require some space. But that's recursive! What is exact space required?\n    Nil,              // no spae required\n}\n</code></pre> <p>To fix this wrap the <code>List</code> inside a <code>Box</code> smart pointer:</p> <pre><code>enum List {\n    Cons(i32, Box&lt;List&gt;),\n    Nil,\n}\n</code></pre> <p>Let's try to determine the size of <code>List</code> again:</p> <pre><code>enum List {\n    Cons(i32, Box&lt;List&gt;), // fixed space, an integer and a Box pointer pointing to memory in Heap\n    Nil,             // no space required\n}\n</code></pre> <p>So now with the help of <code>Box</code> smart pointer, the <code>List</code> instead of being stored on stack, is getting stored on Heap memory. On stack only a pointer of fixed size takes into account the size of our enum <code>List</code></p> <p>To finish it up:</p> <pre><code>// This compiles successfully \u2705\nenum List {\n    Cons(i32, Box&lt;List&gt;),\n    Nil\n}\n\nuse List::{Cons, Nil};\n\nfn main() {\n    let list = Cons(1, Box::new(Cons(2, Box::new(Cons(3, Box::new(Nil))))));\n}\n</code></pre>"},{"location":"cs/rust/tutorial/smart-pointers/deref-trait/","title":"Smart Pointers in Rust - The Deref Trait","text":"<p>The <code>Deref</code> trait allows you to customize the behavior of the derefernece operator (<code>*</code> before the pointer, e.g. <code>*y</code>).</p> <p>Treating Smart Pointers Like Regular References with the Deref Trait</p> <pre><code>fn main() {\n    let x = 5;\n    let y = &amp;x;    // `y` is a memory address pointing to memory location where 5 is stored\n\n    assert_eq!(5, x);  // assert 5 is equal to `x`\n    assert_eq!(5, *y); // assert derefercing `y` is also equal to 5\n\n    assert_eq!(5, y);   // does `y` is equal to 5?\n}\n</code></pre> <p>Well no, the compiler tells us that:</p> <pre><code>error[E0277]: can't compare `{integer}` with `&amp;{integer}`\n --&gt; src/main.rs:8:5\n  |\n8 |     assert_eq!(5, y);   // does `y` is equal to 5?\n  |     ^^^^^^^^^^^^^^^^ no implementation for `{integer} == &amp;{integer}`\n  |\n  = help: the trait `PartialEq&lt;&amp;{integer}&gt;` is not implemented for `{integer}`\n  = help: the following other types implement trait `PartialEq&lt;Rhs&gt;`:\n            f32\n            f64\n            i128\n            i16\n            i32\n            i64\n            i8\n            isize\n          and 6 others\n  = note: this error originates in the macro `assert_eq` (in Nightly builds, run with -Z macro-backtrace for more info)\n</code></pre> <p>which indicates we can't compare an integer to a reference to an integer.</p> <p>Now if we update the above example to use the <code>Box</code> smart pointer, the error vanishes:</p> <pre><code>fn main() {\n    let x = 5;\n    // Box is pointing to value stored somewhere in memory, here 5\n    // `y` is pointing to a copy of 5 since it's primitive type\n    // since Box type owns the data, instead of transfering the ownership\n    let y = Box::new(x);\n\n    assert_eq!(5, x);\n    // Since `Box` implements `Deref` trait\n    // it allows the derefernce trait to work the same as if it were a reference\n    assert_eq!(5, *y);\n}\n</code></pre>"},{"location":"cs/rust/tutorial/smart-pointers/deref-trait/#defining-our-own-smart-pointer","title":"Defining Our own Smart Pointer","text":"<pre><code>struct MyBox&lt;T&gt;(T);\n\nimpl&lt;T&gt; MyBox&lt;T&gt; {\n    fn new(x: T) -&gt; MyBox&lt;T&gt; {\n        // x is still not stored on Heap though.\n        // our focus here is on `Deref` trait\n        MyBox(x)\n    }\n}\n\nfn main() {\n    let x = 5;\n    let y = MyBox::new(5);\n\n    assert_eq!(5, x);\n    assert_eq!(5, *y);  // error: type `MyBox&lt;{integer}&gt;` cannot be dereferenced\n}\n</code></pre> <p>Since we haven't implemented <code>Deref</code> trait yet, we can't use derefernece operator. Continuing the above the code, let add some more pieces:</p> <pre><code>use std::ops::Deref;\n\nimpl&lt;T&gt; Deref for MyBox&lt;T&gt; {\n    type Target = T;\n\n    fn deref(&amp;self) -&gt; &amp;Self::Target {\n        &amp;self.0\n    }\n}\n</code></pre> <p>Recall that <code>MyBox</code> is a tuple struct, so <code>deref()</code> method is returning a reference to first item in the tuple (and that's the only item).</p> <p>Under the hood, Rust actually calls something like this:</p> <pre><code>assert_eq(5, *(y.deref()));\n</code></pre> <p>Why does the <code>deref()</code> returns a reference instead of returning the value itself?</p> <p>If <code>deref()</code> returned value directly, Rust will move the ownership of the value outside of the smart pointer. Something which we don't really want.</p>"},{"location":"cs/rust/tutorial/smart-pointers/deref-trait/#implicit-deref-coercions","title":"Implicit Deref Coercions","text":"<p>Deref Coercion is a convenience feature in Rust that happens automatically for type that implements <code>Deref</code> trait which allows a reference to convert from one type to a reference of different type.</p> <pre><code>fn main() {\n    let x = 5;\n    let y = MyBox::new(x);\n\n    assert_eq!(5, x);\n    assert_eq!(5, *(y.deref()));\n\n    // perfectly fine, even though `hello()` expects a `&amp;str`\n    // and we are passing `&amp;MyBox&lt;String&gt;`\n    // Dereferncing results -&gt; &amp;String which also implements `Deref` trait -&gt; &amp;str\n    let m = MyBox::new(String::from(\"Rust\"));\n    hello(&amp;m);\n}\n\nfn hello(name: &amp;str) {\n    println!(\"Hello, {}!\", name);\n}\n</code></pre>"},{"location":"cs/rust/tutorial/smart-pointers/deref-trait/#deref-coercion-and-mutability","title":"Deref Coercion and Mutability","text":"<p>Rust can automatically perform these chained deref calls at compile time to get the correct type.</p> <p>Rust does deref coercion when it finds types and trait implementations in three cases:</p> <ul> <li>From <code>&amp;T</code> to <code>&amp;u</code> when <code>T: Deref&lt;Target=U&gt;</code></li> <li>From <code>&amp;mut T</code> to <code>&amp;mut U</code> when <code>T: DerefMut&lt;Target=U&gt;</code></li> <li>From <code>&amp;mut T</code> to <code>&amp;U</code> when <code>T: Deref&lt;Target=U&gt;</code></li> </ul> <p>Warning</p> <p>Rust cannot perform Deref coercion when going from an immutable reference to mutable reference due to borrowing rules:</p> <ul> <li>We can only have one mutable reference to a specific piece of data withing a specific scope.</li> </ul>"},{"location":"cs/rust/tutorial/smart-pointers/drop-trait/","title":"Smart Pointers in Rust - The Drop Trait","text":"<p>The <code>Drop</code> trait can be implemeted on any type and allows us to customize when a value goes out of scope.</p> <p>In some languages, we have to manually deallocate the data stored on the heap when we're done using the smart pointer, but with <code>Drop</code> trait this clean up happens automatically when a value goes out of scope.</p> <p>Running Code on Cleanup with the Drop Trait</p> <pre><code>struct CustomSmartPointer {\n    data: String\n}\n\n// the drop trait is already included in prelude; already in scope\nimpl Drop for CustomSmartPointer {\n    fn drop(&amp;mut self) {\n        println!(\"Dropping CustomSmartPointer with data `{}`!\", self.data);\n    }\n}\n\nfn main() {\n    let c = CustomSmartPointer {\n        data: String::from(\"my stuff\"),\n    };\n\n    let d = CustomSmartPointer {\n        data: String::from(\"other stuff\"),\n    };\n\n    println!(\"CustomSmartPointers created.\");\n\n    // variables will be dropped in reverse order of their creation\n    // first d then c\n}\n</code></pre> <p>Running this produces:</p> <pre><code>CustomSmartPointers created.\nDropping CustomSmartPointer with data `other stuff`!\nDropping CustomSmartPointer with data `my stuff`!\n</code></pre> <p>In most cases customizing this cleanup behavior isn't necessary per say, but in some cases to cleanup value early such as when using smart pointers to manage locks.</p> <p>We might want to call the drop method to release a lock so other code in the same scope</p> <p>Info</p> <p>It's worth noting that the <code>drop()</code> in std is just the empty function. It simply takes ownership of the value and make it go out of scope.</p>"},{"location":"cs/rust/tutorial/smart-pointers/drop-trait/#drop-method-with-stdmemdrop","title":"Drop method with std::mem::drop","text":"<p>Rust doesn't allows us to call the drop method directly. For example something like:</p> <pre><code>fn main() {\n    let c = CustomSmartPointer {\n        data: String::from(\"some data\"),\n    };\n    println!(\"CustomSmartPointer created.\");\n    c.drop();\n    println!(\"CustomSmartPointer dropped before the end of main.\");\n}\n</code></pre> <p>when calling <code>cargo check</code> from terminal:</p> <pre><code>error[E0040]: explicit use of destructor method\n  --&gt; src/main.rs:17:7\n   |\n17 |     c.drop();\n   |     --^^^^--\n   |     | |\n   |     | explicit destructor calls not allowed\n   |     help: consider using `drop` function: `drop(c)`\n</code></pre> <p>Rust doesn't allow us to call the drop method manually because when our variable goes out of scope, Rust will still automatically call the drop method.</p> <p>In other to clean up a value early, we can call the drop fucntion provided by Rust standard library and passing the value</p> <pre><code>// c.drop();\ndrop(c);\n</code></pre>"},{"location":"cs/rust/tutorial/smart-pointers/interior-mutability/","title":"Smart Pointers in Rust - Interior Mutability","text":"<p>Info</p> <p>Interior mutability is a design pattern in Rust that allows you to mutate data in Rust even when there are immutable references to that data, typically disallowd by the borrowing rules.</p> <p>This pattern uses unsafe code inside a data structure to bypass the typical rules around mutation and borrowing. The unsafe code is wrapped around <code>unsafe</code> block and is not checked by borrow cheker at compile time for memory safety.</p> <p>Although the borrow rules are not enforced at compile time, we may enforce it at run time.</p> <p>\"RefCell and the Interior Mutability Pattern"},{"location":"cs/rust/tutorial/smart-pointers/interior-mutability/#enforcing-borrowing-rules-at-runtime","title":"Enforcing Borrowing Rules at Runtime","text":"<p>Refcell smart pointer represents single ownership over the data it holds kind of like <code>Box</code> smart pointer. The difference being the <code>Box</code> smart pointer enforces borrowing rules at compile time wheread <code>RefCell</code> enforces these rules at run time.</p> <p>This means if we break the borrowing rules at run time, the program will panic at exit.</p> <p>Compile time borrow checks means we can catch error sooner in the development cycle with no runtime performance cost.</p> <p>The advantage of checking borrowing rules at runtime is that certain memory safe scenarios are allowed whereas they would be disallowed at compile time.</p> <p>This is because certain properties of a program are impossible to detect using static analysis. The most famous example of this is the Halting problem:</p> <p><code>RefCell</code> smart pointer is useful when you're sure that your code is following the borrowing rules but the compiler can't understand or gurantee that. **You can only use <code>RefCell</code> smart pointer is single threaded programs.</p> <p>Here is a recap of the reasons to choose <code>Box&lt;T&gt;</code>, <code>Rc&lt;T&gt;</code>, or <code>RefCell&lt;T&gt;</code>: - <code>Rc&lt;T&gt;</code> enables multiple owners of the same data; <code>Box&lt;T&gt;</code> and <code>RefCell&lt;T&gt;</code> have single owners. - <code>Box&lt;T&gt;</code> allows immutable and mutable borrows checked at compile time; <code>Rc&lt;T&gt;</code> allows only immutable borrows checked at compile time; <code>RefCell&lt;T&gt;</code> allows immutable and mutable borrows checked at runtime. - Because <code>RefCell&lt;T&gt;</code> allows mutable borrows checked at runtime, you can mutate the value inside the <code>RefCell&lt;T&gt;</code> even when the <code>RefCell&lt;T&gt;</code> is immutable. <code>Box&lt;T&gt;</code> doesn't allows that, it would require <code>Box&lt;T&gt;</code> to mutable as well.</p> <p>Mutating the value inside an immutable value is the interior mutability pattern.</p>"},{"location":"cs/rust/tutorial/smart-pointers/interior-mutability/#interior-mutability-pattern","title":"Interior Mutability Pattern","text":"<p>The borrowing rules checked at compile time doesn't allow us to mutate a value using an immutable reference to a mutable data.</p> <pre><code>fn main() {\n    let a = 5;\n    let b = &amp;mut a; // mutable borrow to `a`\n    //      ^^^^^^ error: cannot borrow `a` as mutable, as it is not declared mutable\n    let mut c = 10;\n    let d = &amp;c;     // immutable borrow to `c`\n    *d = 20;\n//  ^^^^^^^^ error: cannot assign to `*` which is behind a `&amp;` reference `d` is a `&amp;`\n//                  reference, so the data it refers to cannot be written\n}\n</code></pre> <p>Though we could solve this with some indirection. Let's say we have a data structure that stores some value and inside that data structure the value is mutable but when we get reference to that data structure the reference itself is immutable. Code outside of the data structure would not be able to mutate the data but it's methods can.</p> <p><code>RefCell</code> does exactly that, using which we can call methods to get an immutable or mutable reference to the data.</p>"},{"location":"cs/rust/tutorial/smart-pointers/interior-mutability/#interior-mutability-mock-objects","title":"Interior Mutability: Mock Objects","text":"<p>Let's take an example where we're trying to build a library that tracks a value against a maximum value and sends messages depending on how close the value is to the maximum value. This can be useful in scenario where we want to trak how much API calls a user is able to make.</p> <p>This library will only provide the functionality to track how close the value is to maximum and what messages is to send at what time. The application depending on this library will be going to implement how the message is actually supposed to be send.</p> <pre><code>pub trait Messenger {\n    fn send(&amp;self, msg: &amp;str);\n}\n\npub struct LimitTracker&lt;'a, T: Messenger&gt; {\n    // Since we're borrowing `T` we must use lifetimes\n    messenger: &amp;'a T,   // a referece to generic type T that must implement `Messenger`\n    // usize is a pointer-sized unsigned integer type\n    // adapts to u32 or u64, depending on the architecture of the computer (32/64 bits)\n    value: usize,\n    max: usize\n}\n\nimpl&lt;'a, T&gt; LimitTracker&lt;'a, T&gt;\nwhere\n    T: Messenger,\n{\n    pub fn new(messenger: &amp;T, max: usize) -&gt; LimitTracker&lt;T&gt; {\n        LimitTracker {\n            messenger,\n            value: 0,\n            max\n        }\n    }\n\n    pub fn set_value(&amp;mut self, value: usize) {\n        self.value = value;\n\n        let percentage_of_max = self.value as f64 / self.max as f64;\n\n        if percentage_of_max &gt;= 1.0 {\n            self.messenger.send(\"Error: You are over your quota!\")\n        } else if percentage_of_max &gt;= 0.9 {\n            self.messenger\n                .send(\"Urgent warning: You'ver used up over 90% of your quota!\");\n        } else if percentage_of_max &gt;= 0.75 {\n            self.messenger\n                .send(\"Warning: You've used up over 75% of your quota!\");\n        }\n    }\n}\n</code></pre> <p>Say we want to test our library at certain points, like 75%, 90% and 100% as quota runs out. We could test this code using a mock object.</p> <p>When we send a message we call <code>self.messenger.send()</code>. So imagine if we pass in a mock messenger object to our limit tracker struct. Our mock messenger object could keep track of how many times the send method was called.</p> <p>Let's implement the test:</p> <pre><code>#[cfg(test)]\nmod tests {\n    use super::*;\n\n    struct MockMessenger {\n        sent_messages: Vec&lt;String&gt;,\n    }\n\n    impl MockMessenger {\n        fn new() -&gt; MockMessenger {\n            MockMessenger {\n                sent_messages: vec![],\n            }\n        }\n    }\n\n    impl Messenger for MockMessenger {\n        fn send(&amp;self, message: &amp;str) {\n            // instead of sending messages; just push into the `sent_messages` vector\n            self.sent_messages.push(String::from(message));\n        //  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        //  error: cannot borrow `self.sent_messages` as mutable, as it is behind a `&amp;` reference\n        //  `self` is a `&amp;` reference, so the data it refers to cannot be borrowed as mutable\n        }\n    }\n\n    #[test]\n    fn it_sends_an_over_75_percent_warning_message() {\n        let mock_messenger = MockMessenger::new();\n        let mut limit_tracker = LimitTracker::new(&amp;mock_messenger, 100);\n\n        limit_tracker.set_value(80);\n\n        assert_eq!(mock_messenger.sent_messages.len(), 1);\n    }\n}\n</code></pre> <p>The error above indicates that we are using an immutable reference of <code>self</code> which is an instance <code>MockMessenger</code> that means inside that struct any field inside that struct should be immutable as well. But we cannot make <code>&amp;self</code> mutable , i.e., <code>&amp;mut self</code> since the trait defines the function signature requires a immutable reference.</p> <p>What we need here is Interior Mutability.</p> <p><pre><code>#[cfg(test)]\nmod tests {\n    use super::*;\n    use std::cell::RefCell;\n\n    struct MockMessenger {\n        sent_messages: RefCell&lt;Vec&lt;String&gt;&gt;,\n    }\n\n    impl MockMessenger {\n        fn new() -&gt; MockMessenger {\n            MockMessenger {\n                // wrap the empty vector in `RefCell`\n                sent_messages: RefCell::new(vec![]),\n            }\n        }\n    }\n\n    impl Messenger for MockMessenger {\n        fn send(&amp;self, message: &amp;str) {\n            // call `borrow_mut()` on `RefCell` smart pointer\n            self.sent_messages.borrow_mut().push(String::from(message));\n        }\n    }\n\n    #[test]\n    fn it_sends_an_over_75_percent_warning_message() {\n        let mock_messenger = MockMessenger::new();\n        let mut limit_tracker = LimitTracker::new(&amp;mock_messenger, 100);\n\n        limit_tracker.set_value(80);\n\n        // call `borrow()` to get an immutable reference to struct field\n        assert_eq!(mock_messenger.sent_messages.borrow().len(), 1);\n    }\n}\n</code></pre> The tests passes in this case.</p>"},{"location":"cs/rust/tutorial/smart-pointers/interior-mutability/#borrowing-rules-with-refcell","title":"Borrowing Rules with RefCell","text":"<p>We know that <code>RefCell</code> check borrow rules at runtime, we cannot have two mutable reference for a value at same time. Let's see what happens incase we do have:</p> <pre><code>mod tests {\n    // ...\n\n    impl Messenger for MockMessenger {\n        fn send(&amp;self, message: &amp;str) {\n            let mut one_borrow = self.sent_messages.borrow_mut();\n            let mut two_borrow = self.sent_messages.borrow_mut();\n\n            one_borrow.push(String::from(message));\n            two_borrow.push(String::from(message));\n        }\n    }\n\n    // ...\n}\n</code></pre> <p>If we run <code>cargo test</code> for this update test, it fails:</p> <pre><code>running 1 test\ntest tests::it_sends_an_over_75_percent_warning_message ... FAILED\n\nfailures:\n\n---- tests::it_sends_an_over_75_percent_warning_message stdout ----\nthread 'tests::it_sends_an_over_75_percent_warning_message' panicked at 'already borrowed: BorrowMutError', src/lib.rs:64:53\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n\n\nfailures:\n    tests::it_sends_an_over_75_percent_warning_message\n\ntest result: FAILED. 0 passed; 1 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n</code></pre> <p>Also note that since we are checking borrowing rules at runtime, this does cost a small overhead on runtime performace of the program.</p>"},{"location":"cs/rust/tutorial/smart-pointers/interior-mutability/#combining-rc-and-refcell","title":"Combining Rc and RefCell","text":"<p>Combining them we want to achieve multiple owners of mutable data. In our original <code>Cons</code> list we used <code>Rc</code> to create two list that shared a Third list. But the values inside them were immutable.</p> <p>Now, with <code>RefCell</code> we can make them mutable.</p> <pre><code>#[derive(Debug)]\nenum List {\n    // wrapped inside Rc to have multiple owners.\n    // i32 is wrapped inside `RefCell` to make it mutable\n    Cons(Rc&lt;RefCell&lt;i32&gt;&gt;, Rc&lt;List&gt;),\n    Nil,\n}\n\nuse crate::List::{Cons, Nil};\nuse std::cell::RefCell;\nuse std::rc::Rc;\n\nfn main() {\n    let value = Rc::new(RefCell::new(5));\n\n    let a = Rc::new(Cons(Rc::clone(&amp;value), Rc::new(Nil)));\n\n    let b = Cons(Rc::new(RefCell::new(3)), Rc::clone(&amp;a));\n    let c = Cons(Rc::new(RefCell::new(4)), Rc::clone(&amp;a));\n\n    *value.borrow_mut() += 10;\n\n    println!(\"a after = {:?}\", a);\n    println!(\"b after = {:?}\", b);\n    println!(\"c after = {:?}\", c);\n}\n</code></pre> <p>Running this, the update done once appears in all three list:</p> <pre><code>a after = Cons(RefCell { value: 15 }, Nil)\nb after = Cons(RefCell { value: 3 }, Cons(RefCell { value: 15 }, Nil))\nc after = Cons(RefCell { value: 4 }, Cons(RefCell { value: 15 }, Nil))\n</code></pre>"},{"location":"cs/rust/tutorial/smart-pointers/reference-counting/","title":"Smart Pointers in Rust - Reference Counting","text":"<p>There are some cases where a single value has multiple owners, for e.g., a graph with multiple edges that point to same node, the node being owned by those edges. So the node should not be cleaned up until it doesn't have any edges pointing to it.</p> <p>To enable multiple ownership of a value we can use a reference counting smart pointer which keeps tracks of number of references to a value and when there are no more references the value will get cleaned up.</p> <p>Reference counting smart pointers we'll discuss here are only useful for single threaded applications. For multi-threaded we'll discuss later on.</p> <p>Rc, the Reference Counted Smart Pointer"},{"location":"cs/rust/tutorial/smart-pointers/reference-counting/#using-rc-to-share-data","title":"Using Rc to Share Data","text":"<p>We'll be demonstrating the use of <code>Rc</code> using Cons list as we discussed earlier.</p> <pre><code>use crate::List::{Cons, Nil};\n\n\nenum List {\n    Cons(i32, Box&lt;List&gt;),\n    Nil,\n}\n\nfn main() {\n    let a = Cons(5, Box::new(Cons(10, Box::new(Nil))));\n    let b = Cons(3, Box::new(a));\n    let c = Cons(4, Box::new(a));  // error: use of moved value: `a`\n}\n</code></pre> <p>The <code>Cons</code> variants holds the data they hold, on line 10 when we created <code>b</code> it owned <code>a</code>, hence the value now cannot be owned by <code>c</code> on line 11.</p> <p>We can though change the definition of <code>Cons</code> variant to hold references instead of owned value, but that would require the use of lifetimes. Using lifetimes we would specify that every element in the list as to live at least as long as long as the entire list.</p> <p>This is because, the borrow checker would'nt allow the code to compile because a temporary <code>&amp;Nil</code> would be dropped before <code>a</code> could take a reference to it.</p> <p>We can adapt the <code>Cons</code> variant to use a reference counting pointer, instead of a <code>Box</code> pointer:</p> <pre><code>use std::rc::Rc;\nuse crate::List::{Cons, Nil};\n\nenum List {\n    Cons(i32, Rc&lt;List&gt;),\n    Nil,\n}\n\nfn main() {\n    // we'll also need to wrap the list inside `a` with Rc because\n    // we're passing it into `b` and `c`\n    let a = Rc::new(Cons(5, Rc::new(Cons(10, Rc::new(Nil)))));\n\n    // to pass `a` to `b` and `c`, we'll use `Rc::clone()`\n    let b = Cons(3, Rc::clone(&amp;a));\n    let c = Cons(4, Rc::clone(&amp;a));\n}\n</code></pre> <p><code>Rc::clone()</code> doesn't create deep copies of the data. It only increments the reference count..</p> <p>Another way to do this is to:</p> <pre><code>let b = Cons(3, a.clone());\n</code></pre> <p>although in our case the convention is to use the first syntax: <code>Rc::clone(&amp;a)</code>.</p> <p>We can't pass in a reference to <code>a</code> here since we expect it to be a owned type:</p> <pre><code>let b = Cons(3, &amp;a);\n</code></pre> <p>We also can't pass <code>a</code> directly, since that would mean move of ownership:</p> <pre><code>let b = Cons(3, a);\n</code></pre>"},{"location":"cs/rust/tutorial/smart-pointers/reference-counting/#increasing-the-reference-count","title":"Increasing the Reference Count","text":"<p>Let's see how the reference count changes as we create new <code>List</code> and updated Reference count. <pre><code>use std::rc::Rc;\nuse crate::List::{Cons, Nil};\n\nenum List {\n    Cons(i32, Rc&lt;List&gt;),\n    Nil,\n}\n\nfn main() {\n    let a = Rc::new(Cons(5, Rc::new(Cons(10, Rc::new(Nil)))));\n    println!(\"count after creating a = {}\", Rc::strong_count(&amp;a));  // not weak count\n\n    let b = Cons(3, Rc::clone(&amp;a));\n    println!(\"count after creating b = {}\", Rc::strong_count(&amp;a));\n    {\n        // inner scope\n        let c = Cons(4, Rc::clone(&amp;a));\n        println!(\"count after creating c = {}\", Rc::strong_count(&amp;a));\n    }\n\n    println!(\"count after c goes out of scope = {}\", Rc::strong_count(&amp;a));\n}\n</code></pre></p> <p>results in:</p> <pre><code>count after creating a = 1\ncount after creating b = 2\ncount after creating c = 3\ncount after c goes out of scope = 2\n</code></pre> <p>Warning</p> <p>Note that the reference counting smart pointer only allows multiple parts of our program to read the same data not modify it.</p> <p>This is due to multiple mutable references violates borrowing rules. There could only be one mutable reference to a value.</p>"},{"location":"cs/rust/tutorial/smart-pointers/reference-cycles/","title":"Smart Pointers in Rust - References Cycles","text":"<p>Danger</p> <p>Rust is known for memory safety so it guarantees you can't have data races, but it doesn't provide you the same guarantee for memory leaks.</p> <p>We can have memory leak issues with <code>Rc&lt;T&gt;</code> smart pointer or <code>RefCell&lt;T&gt;</code> smart pointer. In both we can have items that reference each other in a cycle which leads to memory leaks.</p> <p>Reference Cycles Can Leak Memory</p> <p>Let's learn by an example:</p>"},{"location":"cs/rust/tutorial/smart-pointers/reference-cycles/#creating-a-reference-cycle","title":"Creating a Reference Cycle","text":"<pre><code>use crate::List::{Cons, Nil};\nuse std::cell::RefCell;\nuse std::rc::Rc;\n\n#[derive(Debug)]\nenum List {\n    Cons(i32, RefCell&lt;Rc&lt;List&gt;&gt;),\n    Nil,\n}\n\nimpl List {\n    fn tail(&amp;self) -&gt; Option&lt;&amp;RefCell&lt;Rc&lt;List&gt;&gt;&gt; {\n        match self {\n            Cons(_, item) =&gt; Some(item),  // get list from `Cons` variant if it is `Cons`\n            Nil =&gt; None,                  // otherwise Nil\n        }\n    }\n}\n\nfn main() {\n    // Create Reference Cycle\n    // Wrapped in `Rc&lt;T&gt;` to have multiple owners\n    let a = Rc::new(Cons(5, RefCell::new(Rc::new(Nil))));\n\n    println!(\"a initial rc count = {}\", Rc::strong_count(&amp;a)); // should be 1\n    println!(\"a next item = {:?}\", a.tail());   // next will be `Nil`\n\n    // create b which stores 10 next item being `a`\n    let b = Rc::new(Cons(10, RefCell::new(Rc::clone(&amp;a))));\n\n    println!(\"a rc count after b creation = {}\", Rc::strong_count(&amp;a)); // should be 2\n    println!(\"b initial rc count = {}\", Rc::strong_count(&amp;b));  // should be 1\n    println!(\"b next item = {:?}\", b.tail());\n\n    // modifying list `a` to store list `b`\n    if let Some(link) = a.tail() {\n        // `link` will be wrapped in `Rc&lt;T&gt;`\n        // so get mutable reference to `link`'s data\n        // and change it to Reference to `b`.\n        *link.borrow_mut() = Rc::clone(&amp;b);\n    }\n\n    // print reference counts of `b` and `a`.\n    println!(\"b rc count after changing a = {}\", Rc::strong_count(&amp;b));  // should be 2\n    println!(\"a rc count after changing a = {}\", Rc::strong_count(&amp;a));  // should be 2\n\n    // Uncomment the next line to see that we have a cycle;\n    // it will overflow the stack\n    // println!(\"a next item = {:?}\", a.tail());\n}\n</code></pre> <p>Running this outputs:</p> <pre><code>a initial rc count = 1\na next item = Some(RefCell { value: Nil })\na rc count after b creation = 2\nb initial rc count = 1\nb next item = Some(RefCell { value: Cons(5, RefCell { value: Nil }) })\nb rc count after changing a = 2\na rc count after changing a = 2\n</code></pre> <p>We just created a Reference cycle.</p> <p></p> <p>On stack there are two pointers <code>a</code> and <code>b</code> which points to some memory stored on Heap called <code>'a</code> &amp; <code>'b</code>. <code>'a</code> holds integer 5 and value <code>Nil</code>. Similarly <code>'b</code> stores integer 10 and then a reference to <code>'a</code>. So, the reference count for <code>a</code> is 2.</p> <p>Then we modified list <code>a</code> to store a reference of <code>b</code> (line number 40):</p> <p></p> <p>That means for <code>b</code> as well we'll have reference count of 2.</p> <p>If we uncomment the last <code>println!()</code> line (line 49) we'll get stack overflow. If we print next item of <code>a</code> which will be <code>b</code> then we print next item of <code>b</code> which is <code>a</code> and this goes on infinitely.</p> <p>This circular dependency causes memory leaks, because at the end of <code>main()</code>, <code>a</code> and <code>b</code> should be cleaned up. First <code>b</code> will be cleaned up (remember, variables goes out of scope in reverse order) from stack, but the memory location it point to on heap will still exist because that is still being referenced inside <code>a</code>. Then <code>a</code> will be cleaned up from stack but memory on heap will not be cleaned for the same reason.</p> <p></p> <p>So, to list exists on heap but no variables on stack references them on stack. Leading to memory leak.</p> <p>So, Reference cycles are difficult to create but they are not impossible to create.</p> <p>So far we only had to deal with the pointers that own the data they point, but if we are in the situation where it's okay to get away with pointers that don't own the data they point to, i.e., Weak pointers, then we can also prevent Reference cycles.</p>"},{"location":"cs/rust/tutorial/smart-pointers/reference-cycles/#creating-a-tree-data-structure","title":"Creating a Tree Data Structure","text":"<pre><code>use std::cell::RefCell;\nuse std::rc::Rc;\n\n#[derive(Debug)]\nstruct Node {\n    value: i32,\n    // children of a Node is a Vector of Node wrapped\n    // in `Rc&lt;T&gt;` so that variables outside of this tree to be able to point\n    // to node so we can traverse\n    // Wrapped in `RefCell&lt;T&gt;` to modify a node's children\n    children: RefCell&lt;Vec&lt;Rc&lt;Node&gt;&gt;&gt;,\n}\n\nfn main() {\n    let leaf = Rc::new(Node {\n        value: 3,\n        children: RefCell::new(vec![]), // empty vec\n    });\n\n    let branch = Rc::new(Node {\n        value: 5,\n        // branch stores the `leaf` as children\n        children: RefCell::new(vec![Rc::clone(&amp;leaf)]),\n    });\n}\n</code></pre> <p>We can't get to <code>branch</code> node from <code>leaf</code> node because children don't know about parent; so we'll need parent references.</p>"},{"location":"cs/rust/tutorial/smart-pointers/reference-cycles/#weak-smart-pointer-adding-reference-from-child-to-its-parent","title":"Weak Smart pointer | Adding Reference from Child to it's Parent","text":"<p>Info</p> <p>The <code>Weak</code> smart pointer is a version of <code>Rc</code> that hold a non-owning reference to the managed allocation.</p> <pre><code>use std::cell::RefCell;\nuse std::rc::{Rc, Weak};  // import `Weak&lt;T&gt;`\n\n#[derive(Debug)]\nstruct Node {\n    value: i32,\n    children: RefCell&lt;Vec&lt;Rc&lt;Node&gt;&gt;&gt;,\n    // `RefCell&lt;T&gt;` to modify the parent node\n    // variables outside of tree to be able to reference the parent,\n    // so we want parent node to have multiple ownership.\n    // BUT, `Rc&lt;Node&gt;` will create Reference Cycle.\n    // Luckily we don't need that; childrens don't own the parent.\n    // when children goes out of scope, parent remains.\n    // This is where `Weak&lt;T&gt;` smart pointer come into play\n    parent: RefCell&lt;Weak&lt;Node&gt;&gt;\n}\n\n\nfn main() {\n    let leaf = Rc::new(Node {\n        value: 3,\n        children: RefCell::new(vec![]),\n        // construct new `Weak&lt;T&gt;` without allocating any memory\n        parent: RefCell::new(Weak::new()),\n    });\n\n    // `upgrade()` attempts to upgrade `Weak&lt;T&gt;` pointer to `Rc&lt;T&gt;`\n    // returns a option, because underlying valye may have dropped.\n    // We do this because `Weak&lt;T&gt;` has no idea if the inner valye is dropped or not.\n    println!(\"leaf parent = {:?}\", leaf.parent.borrow().upgrade());\n\n    let branch = Rc::new(Node {\n        value: 5,\n        children: RefCell::new(vec![Rc::clone(&amp;leaf)]),\n        parent: RefCell::new(Weak::new()),\n    });\n\n    // modify `leaf`'s node to store branch node's reference as parent\n    // get mutable reference using `borrow_mut()` and then using dereference operator\n    // `*` to change the value to `branch` node\n    // `branch` is a reference counting smart pointer; while our `parent` field expects\n    // `Weak&lt;T&gt;`. To make the convertion happen we call `Rc::downgrade()`\n    *leaf.parent.borrow_mut()= Rc::downgrade(&amp;branch);\n\n    println!(\"leaf parent = {:?}\", leaf.parent.borrow().upgrade());\n}\n</code></pre> <p>Running this outputs:</p> <pre><code>leaf parent = None\nleaf parent = Some(Node { value: 5, children: RefCell { value: [Node { value: 3, children: RefCell { value: [] }, parent: RefCell { value: (Weak) } }] }, parent: RefCell { value: (Weak) } })\n</code></pre>"},{"location":"cs/rust/tutorial/smart-pointers/reference-cycles/#strong_count-vs-weak_count","title":"strong_count vs weak_count","text":"<p>Internally <code>Rc&lt;T&gt;</code> stores two counts, <code>weak_count</code> and a <code>strong_count</code>.</p> <ol> <li>Strong Count: the number of references which have ownership of the data.</li> <li>Weak Count: the number of references which don't have the ownership of the data.</li> </ol> <pre><code>use std::cell::RefCell;\nuse std::rc::{Rc, Weak};  // import `Weak&lt;T&gt;`\n\n#[derive(Debug)]\nstruct Node {\n    value: i32,\n    children: RefCell&lt;Vec&lt;Rc&lt;Node&gt;&gt;&gt;,\n    parent: RefCell&lt;Weak&lt;Node&gt;&gt;\n}\n\nfn main() {\n    let leaf = Rc::new(Node {\n        value: 3,\n        parent: RefCell::new(Weak::new()),\n        children: RefCell::new(vec![]),\n    });\n\n    println!(\n        \"leaf strong = {}, weak = {}\",\n        Rc::strong_count(&amp;leaf),\n        Rc::weak_count(&amp;leaf)\n    );\n\n    {\n        let branch = Rc::new(Node {\n            value: 5,\n            parent: RefCell::new(Weak::new()),\n            children: RefCell::new(vec![Rc::clone(&amp;leaf)]),\n        });\n\n        // modify `leaf` node's parent field to be a weak reference to branch node\n        *leaf.parent.borrow_mut() = Rc::downgrade(&amp;branch);\n\n        println!(\"\\nInside inner scope\");\n        println!(\n            \"branch strong = {}, weak = {}\",\n            Rc::strong_count(&amp;branch),\n            Rc::weak_count(&amp;branch)\n        );\n\n        println!(\n            \"leaf strong = {}, weak = {}\",\n            Rc::strong_count(&amp;leaf),\n            Rc::weak_count(&amp;leaf)\n        );\n    }\n\n    // after the inner scope ends the branch node will have\n    // strong count of 0 and will be dropped\n    // Branch node still has weak count of 1 because\n    // leaf has weak reference to branch however that doesn't affect\n    // if the underlying value is dropped or not\n\n    println!(\"\\nOutside inner scope\");\n    println!(\"leaf parent = {:?}\", leaf.parent.borrow().upgrade());\n    println!(\n        \"leaf strong = {}, weak = {}\",\n        Rc::strong_count(&amp;leaf),\n        Rc::weak_count(&amp;leaf)\n    );\n}\n</code></pre> <p>Running this outputs:</p> <pre><code>leaf strong = 1, weak = 0\n\nInside inner scope\nbranch strong = 1, weak = 1\nleaf strong = 2, weak = 0\n\nOutside inner scope\nleaf parent = None\nleaf strong = 1, weak = 0\n</code></pre>"},{"location":"cs/rust/tutorial/testing/testing-in-rust-2/","title":"Testing in Rust - Part 2","text":"<p>Test Organization</p> <p>We'll learn how to run tests in different ways and how to organize them into integration tests and unit tests.</p>"},{"location":"cs/rust/tutorial/testing/testing-in-rust-2/#controlling-how-tests-run","title":"Controlling How Tests Run","text":"<p>We'll continue with the <code>adder</code> crate we built earlier with <code>it_works()</code> test plus one more added <code>it_works2()</code> test function:</p> <pre><code>#[cfg(test)]\nmod tests {\n    #[test]\n    fn it_works() -&gt; Result&lt;(), String&gt; {\n        if 2 + 2 == 4 {\n            Ok(())  // Success case is unit type\n        } else {\n            Err(String::from(\"two plus two does not equal four\"))\n        }\n    }\n\n    #[test]\n    fn it_works2() {\n      assert_eq!(2 + 2, 4);\n    }\n}\n</code></pre> <p>This test passes successfully. Cargo compiles your code in test mode and runs the resulting test binary.</p> <p>We can change the default using command line arguments. By default all tests get run in parallel in a separate thread &amp; all generated ouptut is captures and not printed to the screen.</p> <p>There are two sets of command line options separated by <code>--</code>, - one set is for <code>cargo test</code> command - other set is for the resulting test binary.</p> <p>So this print help page for <code>cargo test</code>,</p> <pre><code>cargo test --help\n</code></pre> <p>If we want to figure out which commands we could pass to the resulting test binary:</p> <pre><code>cargo test -- --help\n</code></pre> <p>Here we have an option, <code>--test-threads</code> which can set the number of threads used for running tests in parallel:</p> <p><pre><code>cargo test -- --test-threads=1\n</code></pre> Generally you don't want to do this because then you're test will run slower but in some cases this might be useful. For example in case we might have some tests that modify a file. Running a test parallely will corrupt the file or cause race condition. In that case it is suitable to use single thread for running test.</p>"},{"location":"cs/rust/tutorial/testing/testing-in-rust-2/#showing-output","title":"Showing Output","text":"<pre><code>fn prints_and_returns_10(a: i32) -&gt; i32 {\n    println!(\"I got the value {}\", a);\n    10\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn this_test_will_pass() {\n        let value = prints_and_returns_10(4);\n        assert_eq!(10, value);\n    }\n\n    #[test]\n    fn this_test_will_fail() {\n        let value = prints_and_returns_10(8);\n        assert_eq!(5, value);\n    }\n}\n</code></pre> <p>Running this test fails (obviously):</p> <pre><code>    Finished test [unoptimized + debuginfo] target(s) in 0.30s\n     Running unittests src/lib.rs (target/debug/deps/adder-43e1c9247f5d477d)\n\nrunning 2 tests\ntest tests::this_test_will_pass ... ok\ntest tests::this_test_will_fail ... FAILED\n\nfailures:\n\n---- tests::this_test_will_fail stdout ----\nI got the value 8\nthread 'tests::this_test_will_fail' panicked at 'assertion failed: `(left == right)`\n  left: `5`,\n right: `10`', src/lib.rs:19:9\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n\n\nfailures:\n    tests::this_test_will_fail\n\ntest result: FAILED. 1 passed; 1 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n</code></pre> <p>which also have print statement, <code>I got the value 8</code> but for the test that succeded, we don't see any print statement.</p> <p>This is because by default standard output is captured for passing tests and we don't see it on the screen. But this behavior can be changed:</p> <pre><code>cargo test -- --show-output\n</code></pre> <p>which successfully prints our both print statement:</p> <pre><code>    Finished test [unoptimized + debuginfo] target(s) in 0.00s\n     Running unittests src/lib.rs (target/debug/deps/adder-43e1c9247f5d477d)\n\nrunning 2 tests\ntest tests::this_test_will_pass ... ok\ntest tests::this_test_will_fail ... FAILED\n\nsuccesses:\n\n---- tests::this_test_will_pass stdout ----\nI got the value 4\n\n\nsuccesses:\n    tests::this_test_will_pass\n\nfailures:\n\n---- tests::this_test_will_fail stdout ----\nI got the value 8\nthread 'tests::this_test_will_fail' panicked at 'assertion failed: `(left == right)`\n  left: `5`,\n right: `10`', src/lib.rs:19:9\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n\n\nfailures:\n    tests::this_test_will_fail\n\ntest result: FAILED. 1 passed; 1 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n</code></pre>"},{"location":"cs/rust/tutorial/testing/testing-in-rust-2/#running-a-subset-of-tests","title":"Running a Subset of Tests","text":"<p>How can we run a subset of tests using test name.</p> <pre><code>pub fn add_two(a: i32) -&gt; i32 {\n    a + 2\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn add_two_and_two() {\n        assert_eq!(4, add_two(2));\n    }\n\n    #[test]\n    fn add_three_and_two() {\n        assert_eq!(5, add_two(3));\n    }\n\n    #[test]\n    fn one_hundered() {\n        assert_eq!(102, add_two(100));\n    }\n}\n</code></pre> <p>Say, we wanted to run only one test, specifically the test called <code>one_hundered()</code>, here is how we do that:</p> <pre><code>cargo test one_hundered\n</code></pre> <p>then runs only that specific test:</p> <pre><code>   Compiling adder v0.1.0 (/home/adhadse/Downloads/adder)\n    Finished test [unoptimized + debuginfo] target(s) in 0.24s\n     Running unittests src/lib.rs (target/debug/deps/adder-43e1c9247f5d477d)\n\nrunning 1 test\ntest tests::one_hundered ... ok\n\ntest result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 2 filtered out; finished in 0.00s\n</code></pre> <p>Or run subset of tests by mentioning part of the name:</p> <pre><code>cargo test add\n</code></pre> <p>runs tests starting with <code>add</code>:</p> <pre><code>   Compiling adder v0.1.0 (/home/adhadse/Downloads/adder)\n    Finished test [unoptimized + debuginfo] target(s) in 0.24s\n     Running unittests src/lib.rs (target/debug/deps/adder-43e1c9247f5d477d)\n\nrunning 2 tests\ntest tests::add_three_and_two ... ok\ntest tests::add_two_and_two ... ok\n\ntest result: ok. 2 passed; 0 failed; 0 ignored; 0 measured; 1 filtered out; finished in 0.00s\n</code></pre> <p>Notice we have module name <code>tests</code> in the tests that were run. So, we can also run tests based on the module it belongs to:</p> <pre><code>cargo test tests::\n</code></pre> <p>runs all test in <code>tests</code> module:</p> <pre><code>    Finished test [unoptimized + debuginfo] target(s) in 0.00s\n     Running unittests src/lib.rs (target/debug/deps/adder-43e1c9247f5d477d)\n\nrunning 3 tests\ntest tests::add_three_and_two ... ok\ntest tests::add_two_and_two ... ok\ntest tests::one_hundered ... ok\n\ntest result: ok. 3 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n</code></pre>"},{"location":"cs/rust/tutorial/testing/testing-in-rust-2/#ignoring-tests","title":"Ignoring Tests","text":"<p>Ignore tests with <code>#[ignore]</code> attribute.</p> <pre><code>#[cfg(test)]\nmod tests {\n    #[test]\n    fn it_works() {\n        assert_eq!(2 + 2, 4);\n    }\n\n    #[test]\n    #[ignore]\n    fn expensive_test() {\n        // code that takes an hour to run\n    }\n}\n</code></pre> <p>Under normal circumstances, running <code>cargo test</code> ignore that test case.</p> <p>But if we do want to run that ignored test case (only the ignored tests), we can do as follows:</p> <pre><code>cargo test -- --ignore\n</code></pre>"},{"location":"cs/rust/tutorial/testing/testing-in-rust-2/#test-organization","title":"Test Organization","text":"<p>The Rust community think about test into 2 main categories: 1. Unit tests: Unit tests are small, focused test one module in isolation and could test private interfaces. 2. Integration tests: Integration tests are completely external to your library and thus test the public interface of your library. <p>Up until this point, we've writing unit tests and in Rust units tests live in the same file as our product code.</p> <p>It's convention that in the same file as your product code, you have a module called <code>tests</code> which hold your tests.</p> <ul> <li><code>cfg</code> stands for configurtion. With <code>#[cfg(test)]</code> means that cargo will only compile this code when we run <code>cargo test</code>.</li> <li>Even thought the <code>internal_adder()</code> is private but we're able to call it inside our test module because of the relationship between the parent and child modules in Rust. Child modules are able to access anything in their parent module, even private fields.</li> <li>Some people in Rust community thinks that it's not right to test private functions, but the functionality is there if the need arises.</li> <li>It might look weird to have your test code in the exact same file as your product code. There is a way to keep the tests in separate folder/file but Rust doesn't make this super easy.Putting the tests inside the same file where the product code resides is the convention.</li> </ul> <pre><code>// public function\npub fn add_two(a: i32) -&gt; i32 {\n    internal_adder(a, 2)\n}\n\n// inner function\nfn internal_adder(a: i32, b: i32) -&gt; i32 {\n    a + b\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn internal() {\n        assert_eq!(4, internal_adder(2, 2));\n    }\n}\n</code></pre>"},{"location":"cs/rust/tutorial/testing/testing-in-rust-2/#integratinon-tests","title":"Integratinon tests","text":"<p>Integration tests live in a folder/directory called <code>tests</code> at the root of your project.</p> <p>Each file in the directory <code>tests</code> will be converted to a crate by cargo.</p> <pre><code>// tests/integration_test.rs\nuse adder;\n\n#[test]\nfn it_adds_two() {\n    assert_eq!(4, adder::add_two(2));\n}\n</code></pre> <ul> <li>Notice at the top of the file we have to bring our <code>adder</code> library into scope, because every file in the <code>tests</code> directory is going to be a new crate.</li> <li>Then we write our tests, but with no module with <code>cfg</code> annotation because Cargo knows that all the files in the test directory are tests.</li> <li>Here, we can't call the <code>inner_adder()</code>, only public API can be called.</li> </ul> <p>When we test using <code>cargo test</code>:</p> <pre><code>   Compiling adder v0.1.0 (/home/adhadse/Downloads/adder)\n    Finished test [unoptimized + debuginfo] target(s) in 0.32s\n     Running unittests src/lib.rs (target/debug/deps/adder-43e1c9247f5d477d)\n\nrunning 1 test\ntest tests::internal ... ok\n\ntest result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n\n     Running tests/integration_test.rs (target/debug/deps/integration_test-f3950c12fad8757d)\n\nrunning 1 test\ntest it_adds_two ... ok\n\ntest result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n\n   Doc-tests adder\n\nrunning 0 tests\n\ntest result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n</code></pre> <p>This time we have three sections: - First one for unit tests - Second one for Integration tests - Third one for doc tests</p> <p>To run just our integration tests:</p>"},{"location":"cs/rust/tutorial/testing/testing-in-rust-2/#cargo-test-test-integration_test","title":"<pre><code>cargo test --test integration_test\n</code></pre>","text":"<p>Because every file in the <code>tests</code> directory is treated as a separate crate this could lead to unexpected behavior.</p> <ul> <li>For example, we have multiple integration test files and we want to share some code between those files. You might do something like this, creating a new file called <code>tests/common.rs</code>:</li> </ul> <pre><code>pub fn setup() {\n    // setup code\n}\n</code></pre> <p>running test produces something like this:</p> <pre><code>   Compiling adder v0.1.0 (/home/adhadse/Downloads/adder)\n    Finished test [unoptimized + debuginfo] target(s) in 0.23s\n     Running unittests src/lib.rs (target/debug/deps/adder-43e1c9247f5d477d)\n\nrunning 1 test\ntest tests::internal ... ok\n\ntest result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n\n     Running tests/common.rs (target/debug/deps/common-b5b22cd1b601f16d)\n\nrunning 0 tests\n\ntest result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n\n     Running tests/integration_test.rs (target/debug/deps/integration_test-f3950c12fad8757d)\n\nrunning 1 test\ntest it_adds_two ... ok\n\ntest result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n\n   Doc-tests adder\n\nrunning 0 tests\n\ntest result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n</code></pre> <p>This time we have 4 sections: - first for unit tests - next two for integration tests - last for doc test</p> <p>So Cargo is treating our <code>common.rs</code> file as an integration test file but this is not what we actually wanted to do.</p> <p>Instead to get the desired behavior create a new folder inside our <code>tests</code> directory called <code>common</code> and with new file inside that named <code>mod.rs</code>; and move our previor <code>common.rs</code> file code to this file:</p> <ul> <li>Also delte the previously created <code>tests/common.rs</code></li> </ul> <pre><code>// tests/common/mod.rs\npub fn setup() {\n    // setup code specific to your library\n}\n</code></pre> <p>Running our test suite again creates expected 3 sections instead of 4 with common code shared between integration tests stored in <code>tests/common/mod.rs</code> module:</p> <p>This is because file in the subdirectory of the <code>tests</code> folder do not compiled as crates.</p> <pre><code>    Finished test [unoptimized + debuginfo] target(s) in 0.00s\n     Running unittests src/lib.rs (target/debug/deps/adder-43e1c9247f5d477d)\n\nrunning 1 test\ntest tests::internal ... ok\n\ntest result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n\n     Running tests/integration_test.rs (target/debug/deps/integration_test-f3950c12fad8757d)\n\nrunning 1 test\ntest it_adds_two ... ok\n\ntest result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n\n   Doc-tests adder\n\nrunning 0 tests\n\ntest result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n</code></pre> <p>So when we want to use our new module of shared code in <code>integration_test.rs</code>:</p> <pre><code>use adder;\n\nmod common;\n\n#[test]\nfn it_adds_two() {\n    common::setup();\n    assert_eq!(4, adder::add_two(2));\n}\n</code></pre> <p><code>mod common;</code> is module decalration and it will look for the contents of the module in either a file called <code>common.rs</code> or a directory named <code>common</code> with file named <code>mod.rs</code></p> <p>One thing to note here is we have a library crate because of <code>lib.rs</code>. If we had <code>main.rs</code> file we would have a binary crate and we can't directly test binary crate with integration tests. This is why it's common to see a binary crate that's a thin wrapper around a library crate, so that we can test the library crate with integration tests.</p>"},{"location":"cs/rust/tutorial/testing/testing-in-rust/","title":"Testing in Rust","text":"<p>Part 1 talk about writing test while the part 2 talk about running, organizing tests into integration tests &amp; unit tests.</p> <p>Why we do even want to write tests?</p> <p>Rust already does a great job of making sure our program is correct with the help of it's type system and borrow checker.</p> <p>But these checks can't really test if our functions are really doing the right thing. Rust just test validity not logic verification. Our tests does the logic verification.</p> <p>Writing Automated Tests</p>"},{"location":"cs/rust/tutorial/testing/testing-in-rust/#test-example","title":"Test Example","text":"<p>To write tests as an example we're going to create a new library called <code>adder</code>:</p> <pre><code>cargo new adder --lib\ncd adder\ncode .\n</code></pre> <p>This will create a template library crate with a <code>lib.rs</code> file inside <code>src</code> directory with an example test function <code>it_works()</code> insdie <code>tests</code> module:</p> <pre><code>#[cfg(test)] // `cfg` means config\nmod tests {\n  #[test]\n  fn it_works() {\n    assert_eq!(2 + 2, 4);\n  }\n}\n</code></pre> <p>In Rust, functions are tests if they have <code>#[test]</code> attribute defined on top. Inside our <code>tests</code> module there can be other functions, helper functions not annotated with <code>#[test]</code> attribute and hence they'll not be test function.</p> <p>To run our test, type in terminal:</p> <pre><code>cargo test\n</code></pre> <p>which should give to result something like this:</p> <pre><code>  Compiling adder v0.1.0 (/home/adhadse/Downloads/adder)\n    Finished test [unoptimized + debuginfo] target(s) in 0.63s\n     Running unittests src/lib.rs (target/debug/deps/adder-43e1c9247f5d477d)\n\nrunning 1 test\ntest tests::it_works ... ok\n\ntest result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n\n   Doc-tests adder\n\nrunning 0 tests\n\ntest result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n</code></pre> <p>Where we see what tests function where called afte <code>running 1 test</code>, theres status, passed or not. In our case it said <code>ok</code>. Then we see the summary, if all tests passed then <code>ok</code>, how many tests passed, how many failed and so on. In next part we'll also see how we can ignore and filter out tests. The section below that is for document tests. In Rust we can write tests in our documentation and even test them.</p>"},{"location":"cs/rust/tutorial/testing/testing-in-rust/#writing-a-failing-test","title":"Writing a Failing Test","text":"<p>In Rust a test fails when something inside the test function panics. Each test is ran in a new thread and if the main thread sees that the test thread has died then it fails the test.</p> <p>Let's add a <code>failing_test()</code> right below <code>it_works()</code> test:</p> <pre><code>#[cfg(test)]\nmod tests {\n    //...\n\n    #[test]\n    fn failing_test() {\n        panic!(\"Make this test fail\");\n    }\n}\n</code></pre> <p>running our updated <code>lib.rs</code> with <code>cargo test</code>:</p> <pre><code>  Compiling adder v0.1.0 (/home/adhadse/Downloads/adder)\n    Finished test [unoptimized + debuginfo] target(s) in 0.33s\n     Running unittests src/lib.rs (target/debug/deps/adder-43e1c9247f5d477d)\n\nrunning 2 tests\ntest tests::it_works ... ok\ntest tests::failing_test ... FAILED\n\nfailures:\n\n---- tests::failing_test stdout ----\nthread 'tests::failing_test' panicked at 'Make this test fail', src/lib.rs:17:9\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n\n\nfailures:\n    tests::failing_test\n\ntest result: FAILED. 1 passed; 1 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n</code></pre> <p>This has 2 sections, first shows which tells what tests were ran, then <code>failures:</code> section which tells exactly why a test failed, then it lists out failing test and at last summary.</p>"},{"location":"cs/rust/tutorial/testing/testing-in-rust/#testing-product-code","title":"Testing Product Code","text":"<p>Let's test some actual code. We'll use the code that we wrote in chapter 5 at the top of our <code>lib.rs</code> file:</p> <pre><code>#[derive(Debug)]\nstruct Rectangle {\n    width: u32,\n    height: u32\n}\n\nimpl Rectangle {\n    // can this rectanlge hold another recatangle?\n    fn can_hold(&amp;self, other: &amp;Rectangle) -&gt; bool {\n        self.width &gt; other.width &amp;&amp; self.height &gt; other.height\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    // ...\n}\n</code></pre> <p>Now, let's remove the previous tests and rewrite <code>tests</code> module:</p> <ul> <li>Since our tests are in <code>tests</code> module and produce code in default module, we'll bring everything in parent module into scope using <code>use</code> keyword.</li> <li><code>assert!</code> macro expects <code>true/false</code> to assert that a test pass.</li> </ul> <pre><code>#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn larger_can_hold_smaller() {\n        let larger = Rectangle {\n            width: 8,\n            height: 7,\n        };\n        let smaller = Rectangle {\n            width: 5,\n            height: 1,\n        };\n\n        assert!(large.can_hold(&amp;smaller));\n    }\n}\n</code></pre> <p>This time our test passes:</p> <pre><code>    Finished test [unoptimized + debuginfo] target(s) in 0.26s\n     Running unittests src/lib.rs (target/debug/deps/adder-43e1c9247f5d477d)\n\nrunning 0 tests\n\ntest result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n\n   Doc-tests adder\n\nrunning 0 tests\n\ntest result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n</code></pre> <p>Let's add another test which test a smaller rectangle cannot hold a larger rectangle:</p> <pre><code>#[cfg(test)]\nmod tests {\n    // ...\n\n    #[test]\n    fn smaller_cannot_hold_larger() {\n        let larger = Rectangle {\n            width: 8,\n            height: 7,\n        };\n        let smaller = Rectangle {\n            width: 5,\n            height: 1,\n        };\n\n        assert!(!smaller.can_hold(&amp;larger));\n    }\n}\n</code></pre> <p>and this also passes successfully.</p> <p>Now, let's introduce a bug in our code.</p> <pre><code>impl Rectangle {\n    fn can_hold(&amp;self, other: &amp;Rectangle) -&gt; bool {\n        self.width &lt; other.width &amp;&amp; self.height &gt; other.height\n        // change first &gt; with &lt; sign\n    }\n}\n</code></pre> <p>running our test suite again, fails:</p> <pre><code>   Compiling adder v0.1.0 (/home/adhadse/Downloads/adder)\nerror[E0425]: cannot find value `large` in this scope\n  --&gt; src/lib.rs:29:17\n   |\n29 |         assert!(large.can_hold(&amp;smaller));\n   |                 ^^^^^ help: a local variable with a similar name exists: `larger`\n\nFor more information about this error, try `rustc --explain E0425`.\n</code></pre>"},{"location":"cs/rust/tutorial/testing/testing-in-rust/#asert_eq-macro","title":"asert_eq! macro","text":"<p>Let's change our <code>lib.rs</code> to:</p> <pre><code>pub fn add_two(a: i32) -&gt; i32 {\n    a + 2\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn it_adds_two() {\n        assert_eq!(4, add_two(2));\n    }\n}\n</code></pre> <p>The <code>assert_eq!</code> macro allow us to compare two value</p> <p>This test passes but, if we introduce a bug in our code, let's say add 3 instead of 2:</p> <pre><code>pub fn add_two(a: i32) -&gt; i32 {\n    a + 3\n}\n</code></pre> <p>fails like this, with <code>left != right</code>:</p> <ul> <li>In Rust we can have the expected value as the right side or left side.</li> </ul> <pre><code>   Compiling adder v0.1.0 (/home/adhadse/Downloads/adder)\n    Finished test [unoptimized + debuginfo] target(s) in 0.58s\n     Running unittests src/lib.rs (target/debug/deps/adder-43e1c9247f5d477d)\n\nrunning 1 test\ntest tests::it_adds_two ... FAILED\n\nfailures:\n\n---- tests::it_adds_two stdout ----\nthread 'tests::it_adds_two' panicked at 'assertion failed: `(left == right)`\n  left: `4`,\n right: `5`', src/lib.rs:11:9\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n\n\nfailures:\n    tests::it_adds_two\n\ntest result: FAILED. 0 passed; 1 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n</code></pre> <p>Counter to <code>assert_eq!</code> macro is <code>assert_ne!</code> macro which asserts that the two parameter passed in is not equal..</p> <p>One thing to note here both parameters passed into <code>assert_eq!</code> or <code>assert_ne!</code> has to implement <code>PartialEq</code> and <code>Debug</code> traits.</p>"},{"location":"cs/rust/tutorial/testing/testing-in-rust/#custom-failure-messages","title":"Custom Failure Messages","text":"<p>Changing our <code>lib.rs</code> to:</p> <pre><code>pub fn greeting(name: &amp;str) -&gt; String {\n    format!(\"Hello {}!\", name)\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn greeting_contains_name() {\n        let result = greeting(\"Carol\");\n        assert!(result.contains(\"Carol\")); // assert result contains \"Carol\"\n    }\n}\n</code></pre> <p>This test passes. If we modify our <code>greeting()</code> function like this:</p> <pre><code>pub fn greeting(name: &amp;str) -&gt; String {\n    format!(\"Hello!\")\n}\n</code></pre> <p>then the test fails:</p> <pre><code>    Finished test [unoptimized + debuginfo] target(s) in 0.27s\n     Running unittests src/lib.rs (target/debug/deps/adder-43e1c9247f5d477d)\n\nrunning 1 test\ntest tests::greeting_contains_name ... FAILED\n\nfailures:\n\n---- tests::greeting_contains_name stdout ----\nthread 'tests::greeting_contains_name' panicked at 'assertion failed: result.contains(\\\"Carol\\\")', src/lib.rs:12:9\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n\n\nfailures:\n    tests::greeting_contains_name\n\ntest result: FAILED. 0 passed; 1 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n</code></pre> <p>This output might be okay, but it's not the most useful failure, so let's write our own custom failure message:</p> <pre><code>#[test]\nfn greeting_contains_name() {\n    let result = greeting(\"Carol\");\n    assert!(\n        result.contains(\"Carol\"),\n        \"Greeting did not contain name, value was `{}`\",\n        result\n    );\n}\n</code></pre> <p><code>assert!</code> takes a custom failure message as the second parameter, and parameters after that are for the placeholders value in our custom failure message.</p> <p>Running this update test fails like this:</p> <pre><code>    Finished test [unoptimized + debuginfo] target(s) in 0.29s\n     Running unittests src/lib.rs (target/debug/deps/adder-43e1c9247f5d477d)\n\nrunning 1 test\ntest tests::greeting_contains_name ... FAILED\n\nfailures:\n\n---- tests::greeting_contains_name stdout ----\nthread 'tests::greeting_contains_name' panicked at 'Greeting did not contain name, value was `Hello!`', src/lib.rs:12:9\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n\n\nfailures:\n    tests::greeting_contains_name\n\ntest result: FAILED. 0 passed; 1 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n</code></pre>"},{"location":"cs/rust/tutorial/testing/testing-in-rust/#asserting-that-a-function-panics","title":"Asserting that a Function Panics","text":"<p>We'll learn to write tests that assert a function fails.</p> <p>In order to our guessing game to work, user needs to provide a value between 1 and 100. If it doesn't, then we need to panic and exit the program. We'll test panic functionality here:</p> <pre><code>pub struct Guess {\n    value: i32,\n}\n\nimpl Guess {\n    pub fn new(value: i32) -&gt; Guess {\n        if value &lt; 1 || value &gt; 100 {\n            panic!(\"Guess value must be between 1 and 100, got {}\", value);\n        }\n\n        Guess { value }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    #[should_panic]\n    fn greater_than_100() {\n        Guess::new(200);\n    }\n}\n</code></pre> <p>As we can see <code>greater_than_100()</code> function is decorated with <code>#[should_panic]</code> attribute which asserts that the code inside the function body should panic.</p> <p>This test also passes sweetly. But if change our <code>new()</code> associated function such that it does not panic given the condition we want it to:</p> <pre><code>impl Guess {\n    pub fn new(value: i32) -&gt; Guess {\n        if value &lt; 1 {\n            panic!(\"Guess value must be between 1 and 100, got {}\", value);\n        }\n\n        Guess { value }\n    }\n}\n</code></pre> <p>which fails with message: \"test did not panic as expected\"</p> <pre><code>    Finished test [unoptimized + debuginfo] target(s) in 0.24s\n     Running unittests src/lib.rs (target/debug/deps/adder-43e1c9247f5d477d)\n\nrunning 1 test\ntest tests::greater_than_100 - should panic ... FAILED\n\nfailures:\n\n---- tests::greater_than_100 stdout ----\nnote: test did not panic as expected\n\nfailures:\n    tests::greater_than_100\n\ntest result: FAILED. 0 passed; 1 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n\nerror: test failed, to rerun pass `--lib`\n</code></pre> <p>But can't our function panic for any reason, say other than value being between the range 0 and 100 some other <code>unwrap()</code> method calls <code>panic!</code>. Our test is just imprecise.</p> <p>To make assertion a little more precise, we'll make some changes. First let's modify our <code>new()</code> function:</p> <pre><code>impl Guess {\n    pub fn new(value: i32) -&gt; Guess {\n        if value &lt; 1 {\n            panic!(\"Guess value must be greater than or equal to 1, got {}\", value);\n        } else if value &gt; 100 {\n            panic!(\"Guess value must be less than or equal to 100, got {}\", value);\n        }\n\n        Guess { value }\n    }\n}\n</code></pre> <p>and modify <code>should_panic</code> attribute to only panic for specific failure message:</p> <pre><code>#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    #[should_panic(expected = \"Guess value must be less than or equal to 100\")]\n    fn greater_than_100() {\n        Guess::new(-2); // Notice we use -2\n    }\n}\n</code></pre> <p>This says that assert that the code in this test function panics and the failure message is expected to be somethign like that the <code>expected</code> argument.</p> <p>Running our test, this successfully fails:</p> <pre><code>    Finished test [unoptimized + debuginfo] target(s) in 0.24s\n     Running unittests src/lib.rs (target/debug/deps/adder-43e1c9247f5d477d)\n\nrunning 1 test\ntest tests::greater_than_100 - should panic ... FAILED\n\nfailures:\n\n---- tests::greater_than_100 stdout ----\nthread 'tests::greater_than_100' panicked at 'Guess value must be greater than or equal to 1, got -2', src/lib.rs:8:13\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\nnote: panic did not contain expected string\n      panic message: `\"Guess value must be greater than or equal to 1, got -2\"`,\n expected substring: `\"Guess value must be less than or equal to 100\"`\n\nfailures:\n    tests::greater_than_100\n\ntest result: FAILED. 0 passed; 1 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s\n</code></pre>"},{"location":"cs/rust/tutorial/testing/testing-in-rust/#returning-a-result-type","title":"Returning a Result Type","text":"<p>Tests that return a <code>Result&lt;T&gt;</code> type.</p> <pre><code>#[cfg(test)]\nmod tests {\n    #[test]\n    fn it_works() -&gt; Result&lt;(), String&gt; {\n        if 2 + 2 == 4 {\n            Ok(())  // Success case is unit type\n        } else {\n            Err(String::from(\"two plus two does not equal four\"))\n        }\n    }\n}\n</code></pre> <p>Tests that return a <code>Result&lt;T&gt;</code> type allows you to use the question mark operator which can be convenient if you have multiple operations withing the test that could return an <code>Err</code> type and we want the test to fail if any of those return an error type.</p>"},{"location":"etc/","title":"Etc","text":""},{"location":"etc/regular-expressions/","title":"Regular Expressions","text":"<p>Use either of these to formulate your regex expression. Both are Open Source.</p> <p> Regexr  Regex101</p> Description <code>^</code> Matches the beginning of line <code>$</code> Matches the end of line <code>.</code> Mathes any character <code>\\s</code> Matches whitespace; equivalent to <code>[\\t\\n\\r\\f\\v]</code> <code>\\S</code> Matches any NON-Whitespace; equivalent to <code>[^\\t\\n\\r\\f\\v]</code> <code>*</code> Zero or more time; Repeats a character <code>*?</code> Zero or more times; NON-GREEDY; Repeats a character <code>+</code> One or More times; Repeats a character <code>+?</code> One or More times; NON-GREEDY; Repeats a character <code>[aeiou]</code> Matches a single character in the listed set <code>[^XYZ]</code> Matches a single character NOT in the listed set <code>[a-z0-9]</code> A set of character or an include range <code>(</code> Indicates where the string extraction starts <code>)</code> Indicates where the string extraction ends <code>a\\|b</code> matches either a or b, a and b are string matching pattern <code>\\</code> Escape character for special characters (<code>\\t</code>, <code>\\n</code>, <code>\\b</code>) <code>\\b</code> Matches word boundary <code>\\d</code> Matches single digit; equivalent to <code>[0-9]</code> <code>\\w</code> Alphanumeric character; <code>[a-zA-Z0-9_]</code> <code>\\W</code> NON-Alphanumeric character; <code>[^a-zA-Z0-9_]</code> <code>?</code> Matches zero or One occurances <code>{n}</code> Exactly <code>n</code> repetitions, <code>n&gt;=0</code> <code>{n,}</code> Atleast <code>n</code> repeatitions <code>{,n}</code> Atmost <code>n</code> repeatitions <code>{m,n}</code> Atleast <code>m</code> times and atmost <code>n</code> repeatitions"},{"location":"etc/regular-expressions/#regex-lookaround","title":"Regex Lookaround","text":"<p>Regex Lookaround</p> <ul> <li> <p>Look ahead positive <code>(?=)</code></p> <p><code>A(?=B)</code> find expression A where expression B follows)</p> </li> <li> <p>Look ahead negative <code>(?!)</code></p> <p><code>A(?!B)</code> find expression A where expression B does not follow</p> </li> <li> <p>Look behind positive <code>(?&lt;=)</code></p> <p><code>(?&lt;=B)A</code> find expression A where expression B precesed</p> </li> <li> <p>Look behind negative <code>(?&lt;!)</code></p> <p><code>(?&lt;!B)A</code> find expression A where expression B does not precede</p> </li> </ul>"},{"location":"etc/mathematics/maths-formula/","title":"Maths Formula Compendium","text":""},{"location":"etc/mathematics/maths-formula/#trigonometry","title":"Trigonometry","text":"Name $0\\degree$ $30\\degree (\\frac{\\pi}{6})$ $45\\degree (\\frac{\\pi}{4})$ $60\\degree (\\frac{\\pi}{3})$ $90\\degree (\\frac{\\pi}{2})$ $120\\degree (\\frac{2\\pi}{3})$ $135\\degree (\\frac{3\\pi}{4})$ $150\\degree (\\frac{5\\pi}{6})$ $180\\degree (\\pi)$ $\\sin$ $0$ $\\frac{1}{2}$ $\\frac{1}{\\sqrt{2}}$ $\\frac{\\sqrt{3}}{2}$ $1$ $\\frac{\\sqrt{3}}{2}$ $\\frac{1}{\\sqrt{2}}$ $\\frac{1}{2}$ $0$ $\\cos$ $1$ $\\frac{\\sqrt{3}}{2}$ $\\frac{1}{\\sqrt{2}}$ $\\frac{1}{2}$ $0$ $-\\frac{1}{2}$ $-\\frac{1}{\\sqrt{2}}$ $-\\frac{\\sqrt{3}}{2}$ $-1$ $\\tan$ $0$ $\\frac{1}{\\sqrt{3}}$ $1$ $\\sqrt{3}$ $\\infin$ $-\\sqrt{3}$ $-1$ $-\\frac{1}{\\sqrt{3}}$ $0$ $\\cot$ $\\infin$ $\\sqrt{3}$ $1$ $\\frac{1}{\\sqrt{3}}$ $0$ $-\\frac{1}{\\sqrt{3}}$ $-1$ $\\sqrt{3}$ $-\\infin$ $\\sec$ $1$ $\\frac{2}{\\sqrt{3}}$ $\\sqrt{2}$ $ 2 $ $\\infin$ $-2$ $-\\sqrt{2}$ $-\\frac{2}{\\sqrt{3}}$ $-1$ $\\cosec$ $\\infin$ $2$ $\\sqrt{2}$ $\\frac{2}{\\sqrt{3}}$ $1$ $\\frac{2}{\\sqrt{3}}$ $\\sqrt{2}$ $2$ $\\infin$"},{"location":"etc/mathematics/maths-formula/#trigonometric-identities","title":"Trigonometric Identities","text":"$\\sin(A + B) = \\sin A \\cos B + \\cos A \\sin B$               $\\sin(A - B) = \\sin A \\cos B - \\cos A \\sin B$               $\\cos(A + B) = \\cos A \\cos B - \\sin A \\sin B$               $\\cos(A - B) = \\cos A \\cos B + \\sin A \\sin B$               $\\sin C + \\sin D = 2 \\sin \\frac{C + D}{2} \\cos \\frac{C - D}{2}$               $\\sin C - \\sin D = 2 \\cos \\frac{C + D}{2} \\sin \\frac{C - D}{2}$               $\\cos C + \\cos D = 2 \\cos \\frac{C + D}{2} \\cos \\frac{C - D}{2}$               $\\cos C - \\cos D = -2 \\sin \\frac{C + D}{2} \\sin \\frac{C - D}{2}$               $\\sin(A + B) \\sin(A - B) = \\sin^2 A - \\sin^2 B$               $\\cos(A + B) \\cos(A - B) = \\cos^2 A - \\sin^2 B$               $\\tan A - \\tan B = \\frac{\\sin(A - B)}{\\cos A \\cos B}$               $\\cot A - \\cot B = \\frac{-\\sin(A - B)}{\\sin A \\sin B}$      <p>$$ \\sin(A + B + C) = \\sin A \\cos B \\cos C + \\cos A \\sin B \\cos C + \\cos A \\cos B \\sin C - \\sin A \\sin B \\sin C $$</p> <p>$$ \\cos(A + B + C) = \\cos A \\cos B \\cos C - \\cos A \\sin B \\sin C - \\sin A \\cos B \\sin C - \\sin A \\sin B \\cos C $$</p> <p>$$ \\tan(A + B + C) = \\frac{\\tan A + \\tan B + \\tan C - \\tan A \\tan B \\tan C}{1 - \\tan A \\tan B - \\tan B \\tan C - \\tan C \\tan A} $$</p>"},{"location":"etc/mathematics/maths-formula/#double-angle-formulas","title":"Double Angle Formulas","text":"$$ \\begin{split} \\sin 2A &amp;= 2 \\sin A \\cos A \\\\ &amp;= \\frac{2 \\tan x}{1 + \\tan^2 x} \\\\ &amp;= \\frac{1 - \\tan^2 A}{1 + \\tan^2 A} \\end{split}  \\newline\\quad  \\begin{split}\\cos 2A &amp;= \\cos^2 A - \\sin^2 A \\\\&amp; = 2\\cos^2 A - 1 \\\\ &amp;= 1 - 2\\sin^2 \\end{split}  $$   $$  \\sin 3A = 3 \\sin A - 4 \\sin^3 A  \\newline\\quad  \\cos 3A = 4 \\cos^3 A - 3 \\cos A  $$   $$  \\tan 2A = \\frac{2 \\tan A}{1 - \\tan^2 A}  \\newline\\quad  \\tan 3A = \\frac{3 \\tan A - \\tan^3 A}{1 - 3 \\tan^2 A}  $$"},{"location":"etc/mathematics/maths-formula/#product-to-sum-formulas","title":"Product-to-Sum Formulas","text":"<ul> <li>$2 \\sin x \\sin y = \\cos(x - y) - \\cos(x + y)$</li> <li>$2 \\cos x \\cos y = \\cos(x + y) + \\cos(x - y)$</li> <li>$2 \\sin x \\cos y = \\sin(x + y) + \\sin(x - y)$</li> <li>$2 \\cos x \\sin y = \\sin(x + y) - \\sin(x - y)$</li> </ul>"},{"location":"etc/mathematics/maths-formula/#pythagorean-identities","title":"Pythagorean Identities","text":"<ul> <li>$\\sin^2 \\theta + \\cos^2 \\theta = 1$</li> <li>$1 + \\tan^2 \\theta = \\sec^2 \\theta$</li> <li>$1 + \\cot^2 \\theta = \\csc^2 \\theta$</li> </ul>          $\\tan(x + y) = \\frac{\\tan x + \\tan y}{1 - \\tan x \\tan y}$               $\\tan(x - y) = \\frac{\\tan x - \\tan y}{1 + \\tan x \\tan y}$               $\\cot(x + y) = \\frac{\\cot x \\cot y - 1}{\\cot x + \\cot y}$               $\\cot(x - y) = \\frac{\\cot x \\cot y + 1}{\\cot y - \\cot x}$      <ul> <li>$a^3 + b^3 = (a + b)(a^2 - ab + b^2)$</li> <li>$a^3 - b^3 = (a - b)(a^2 + ab + b^2)$</li> </ul>"},{"location":"etc/mathematics/maths-formula/#general-solutions","title":"General Solutions","text":""},{"location":"etc/mathematics/maths-formula/#basic-cases","title":"Basic Cases","text":"<ul> <li>$\\sin \\theta = 0 \\implies \\theta = n\\pi$</li> <li>$\\cos \\theta = 0 \\implies \\theta = (2n + 1)\\frac{\\pi}{2}$</li> <li>$\\tan \\theta = 0 \\implies \\theta = n\\pi$</li> </ul>"},{"location":"etc/mathematics/maths-formula/#equality-cases","title":"Equality Cases","text":"<ul> <li>$\\sin \\theta = \\sin \\beta \\implies \\theta = n\\pi + (-1)^n \\beta$</li> <li>$\\cos \\theta = \\cos \\beta \\implies \\theta = 2n\\pi \\pm \\beta$</li> <li>$\\tan \\theta = \\tan \\beta \\implies \\theta = n\\pi + \\beta$</li> </ul>"},{"location":"etc/mathematics/maths-formula/#derived-cases","title":"Derived Cases","text":"<ul> <li>$\\sin^2 \\theta = \\sin^2 \\beta \\implies \\theta = n\\pi \\pm \\beta$</li> <li>$\\cos^2 \\theta = \\cos^2 \\beta \\implies \\theta = 2n\\pi \\pm \\beta$</li> <li>$\\tan^2 \\theta = \\tan^2 \\beta \\implies \\theta = n\\pi \\pm \\beta$</li> </ul>"},{"location":"etc/mathematics/maths-formula/#secant-and-cosine-properties","title":"Secant and Cosine Properties","text":"<ul> <li>$-\\cos \\theta = \\cos(\\pi - \\theta)$</li> <li>$-\\sec \\theta = \\sec(\\pi - \\theta)$</li> <li>$\\cos(- \\theta) = \\cos\\theta$</li> <li>$\\sec(-\\theta) = \\sec\\theta$</li> </ul>"},{"location":"etc/mathematics/maths-formula/#transformation-rule-tr","title":"Transformation Rule (TR)","text":"<p>$(\\frac{n\\pi}{2} + \\theta)$</p> <ul> <li>If $n$ is odd $\\Rightarrow$ TR will change.</li> </ul> <p>$(n\\pi + \\theta)$</p> <ul> <li>If $n$ is odd/even $\\Rightarrow$ TR will not change.</li> </ul>"},{"location":"etc/mathematics/maths-formula/#inverse-trigonometry","title":"Inverse Trigonometry","text":"Domain (value) ($x$) Range (Angle) ($\\theta$) $sin^{-1}x$ $[-1,1]$ $[-\\pi/2, \\pi/2]$ $\\cos^{-1}x$ $[-1. 1]$ $[0, \\pi]$ $\\tan^{-1}x$ $R$ $(-\\pi/2, \\pi/2)$ $\\cot^{-1}x$ $R$ $(0, \\pi)$ $\\sec^{-1}x$ $R- (-1, 1)$ $[0, \\pi-{\\pi/2}$ $\\cosec^{-1}x$ $R-(-1, 1)$ $[-\\pi/2, \\pi/2] - {0}$ Property 1 Property 2 $\\sin^{-1}(\\sin\\theta) = \\theta$ $\\sin(\\sin^{-1}x) = x$ $\\cos^{-1}(\\cos\\theta) = \\theta$ $\\cos(\\cos^{-1}x) = x$ $\\tan^{-1}(\\tan\\theta) = \\theta$ $\\tan(\\tan^{-1}x) = x$ $\\cosec^{-1}(\\cosec\\theta) = \\theta$ $\\cosec(\\cosec^{-1}x) = x$ $\\sec^{-1}(\\sec\\theta) = \\theta$ $\\sec(\\sec^{-1}x) = x$ $\\cot^{-1}(\\cot\\theta) = \\theta$ $\\cot(\\cot^{-1}x) = x$ Property 3 $sin^{-1}(-x) = -\\sin^{-1}{x}$ $cos^{-1}(-x) = \\pi -\\cos^{-1}{x}$ $tan^{-1}(-x) = -\\tan^{-1}{x}$ $cot^{-1}(-x) = \\pi -\\cot^{-1}{x}$ $cosec^{-1}(-x) = -\\cosec^{-1}{x}$ $sec^{-1}(-x) = \\pi -\\sec^{-1}{x}$ Property 4 $sin^{-1}(\\frac{1}{x}) = \\cosec^{-1}x$ $cos^{-1}(\\frac{1}{x}) = \\sec^{-1}x$ $tan^{-1}(\\frac{1}{x}) = \\cot^{-1}x$ Property 5 $sin^{-1}x + \\cos ^{-1}x = \\frac{\\pi}{2}$ $tan^{-1}x + \\cot ^{-1}x = \\frac{\\pi}{2}$ $sec^{-1}x + \\cosec ^{-1}x = \\frac{\\pi}{2}$ Property 6 $\\sin^{-1}x + \\sin^{-1}y = \\sin^{-1} \\big(x\\sqrt{1-y^2} + y\\sqrt{1-x^2}\\big)$ $\\sin^{-1}x - \\sin^{-1}y = \\sin^{-1} \\big(x\\sqrt{1-y^2} - y\\sqrt{1-x^2}\\big)$ Property 7 $\\cos^{-1}x + \\cos^{-1}y = \\cos^{-1} \\big(xy - \\sqrt{1-x^2} \\sqrt{1-y^2}\\big)$ $\\cos^{-1}x - \\cos^{-1}y = \\cos^{-1} \\big(xy + \\sqrt{1-x^2} \\sqrt{1-y^2}\\big)$ Property 8 $\\tan^{-1}x - \\tan^{-1}{y} = \\tan^{-1}\\Big(\\frac{x+y}{1-xy}\\Big)$ $\\tan^{-1}x + \\tan^{-1}{y} = \\tan^{-1}\\Big(\\frac{x-y}{1+xy}\\Big)$ <p>Property 9</p>          $2\\sin^{-1}x = 2 \\sin^{-1}(\\theta/2)$               $3\\sin^{-1}x = \\sin^{-1}(3x-4x^2)$               $2\\cos^{-1}x = \\cos^{-1}(2x^2-1)$               $3\\cos^{-1}x = \\cos^{-1}(4x^3-3x)$       $$  2 \\tan^{-1}x = \\tan^{-1}\\Big(\\frac{2x}{1-x^2}\\Big) \\\\ 3\\tan^{-1}x = \\tan^{-1}\\Big(\\frac{3x-x^2}{1-3x^2}\\Big)  $$  <p>Property 10</p> <p>$$</p> <p>2\\tan^{-1}x = \\begin{cases}\\sin^{-1}\\Big(\\frac{2x}{1+x^2}\\Big)\\\\cos^{-1}\\Big(\\frac{1-x^2}{1+x^2}\\Big)\\end{cases}</p> <p>$$</p> <p>$1-\\cos\\theta = 2 \\sin^2(\\theta/2)$</p> <p>$1+ \\cos\\theta = 2\\,\\cos^2(\\theta/2)$</p> <p>$\\sin\\theta = 2\\sin(\\theta/2)\\cos(\\theta/2)$</p> <p>$2\\sin^{-1}x + \\sin^{-1}(-x) = \\cos^{-1}x$</p>  $$  \\begin{split}\\cos2\\theta &amp;= \\cos^2\\theta - \\sin^2\\theta \\\\&amp; = 2\\cos^2\\theta -1 \\\\ &amp;= 1 - 2\\sin^2\\theta \\end{split}  $$  Expression and Substitution Expression Substitution Substitution $a^2 + x^2$ $x = a\\tan\\theta$ $x = a\\cot\\theta$ $a^2-x^2$ $x=a\\sin\\theta$ $x=a\\cos\\theta$ $x^2-a^2$ $x=a\\sec\\theta$ $x=a\\cosec\\theta$ $\\sqrt{\\frac{a-x}{a+x}}$ $x=a\\cos2\\theta$ $\\sqrt{\\frac{a^2+x^2}{a^2-x^2}}$ $x=a^2\\cos2\\theta$"},{"location":"etc/mathematics/maths-formula/#differentiation","title":"Differentiation","text":"$\\frac{d}{dx} x^n = n x^{n-1}$               $\\frac{d}{dx} a^x = a^x \\ln a$               $\\frac{d}{dx} \\log_e x = \\frac{1}{x}$               $\\frac{d}{dx} \\log_a x = \\frac{1}{x \\ln a}$               $\\frac{d}{dx} e^x = e^x$               $\\frac{d}{dx} \\sin x = \\cos x$               $\\frac{d}{dx} \\cos x = -\\sin x$               $\\frac{d}{dx} \\tan x = \\sec^2 x$               $\\frac{d}{dx} \\sec x = \\sec x \\tan x$               $\\frac{d}{dx} \\cot x = -\\csc^2 x$               $\\frac{d}{dx} \\cosec x = -\\cosec x \\cot x$               $\\log m^n = n \\log m$               $\\log m + \\log n = \\log (mn)$               $\\log m - \\log n = \\log \\left(\\frac{m}{n}\\right)$               $\\log e = 1$, $\\log_a a = 1$               $\\log_a b = \\frac{1}{\\log_b a}$               $\\frac{d}{dx} \\sin^{-1} x = \\frac{1}{\\sqrt{1 - x^2}}$               $\\frac{d}{dx} \\cos^{-1} x = \\frac{-1}{\\sqrt{1 - x^2}}$               $\\frac{d}{dx} \\tan^{-1} x = \\frac{1}{1 + x^2}$               $\\frac{d}{dx} \\cot^{-1} x = \\frac{-1}{1 + x^2}$               $\\frac{d}{dx} \\sec^{-1} x = \\frac{1}{x \\sqrt{x^2 - 1}}$               $\\frac{d}{dx} \\cosec^{-1} x = \\frac{-1}{x \\sqrt{x^2 - 1}}$      <ul> <li>$a^{\\log_a x} = x$</li> <li>$x^{\\log_a y} = y^{\\log_a x}$</li> </ul>"},{"location":"etc/mathematics/maths-formula/#chain-rule","title":"Chain Rule","text":"<p>If a variable $z$ depends on the variable $y$, which itself depends on the variable $x$ (that is, $y$ and $z$ are dependent variables), then $z$ depends on $x$ as well, via the intermediate variable $y$. In this case, the chain rule is expressed as:</p> <p>$$   \\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dz}   $$</p>"},{"location":"etc/mathematics/maths-formula/#integration-formulas","title":"Integration Formulas","text":""},{"location":"etc/mathematics/maths-formula/#basic-integration-rules","title":"Basic Integration Rules","text":"<ol> <li>$\\int x^n \\, dx = \\frac{x^{n+1}}{n+1} + C$</li> <li>$\\int \\frac{1}{x} \\, dx = \\ln x + C$</li> <li>$\\int e^x \\, dx = e^x + C$</li> <li>$\\int a^x \\, dx = \\frac{a^x}{\\ln a} + C$</li> <li>$\\int \\sin x \\, dx = -\\cos x + C$</li> <li>$\\int \\cos x \\, dx = \\sin x + C$</li> <li>$\\int \\sec^2 x \\, dx = \\tan x + C$</li> <li>$\\int \\csc^2 x \\, dx = -\\cot x + C$</li> <li>$\\int \\sec x \\tan x \\, dx = \\sec x + C$</li> <li>$\\int \\csc x \\cot x \\, dx = -\\csc x + C$</li> <li>$\\int \\cot x \\, dx = \\ln |\\sin x| + C$</li> <li>$\\int \\tan x \\, dx = -\\ln |\\cos x| + C = \\ln |\\sec x| + C$</li> <li>$\\int \\sec x \\, dx = \\ln |\\sec x + \\tan x| + C$</li> <li>$\\int \\csc x \\, dx = \\ln |\\csc x - \\cot x| + C$</li> </ol>"},{"location":"etc/mathematics/maths-formula/#inverse-trigonometric-integrals","title":"Inverse Trigonometric Integrals","text":"<ol> <li>$\\int \\frac{1}{\\sqrt{a^2 - x^2}} \\, dx = \\sin^{-1} \\left(\\frac{x}{a}\\right) + C$</li> <li>$\\int \\frac{-1}{\\sqrt{a^2 - x^2}} \\, dx = \\cos^{-1} \\left(\\frac{x}{a}\\right) + C$</li> <li>$\\int \\frac{1}{x^2 + a^2} \\, dx = \\frac{1}{a} \\tan^{-1} \\left(\\frac{x}{a}\\right) + C$</li> <li>$\\int \\frac{-1}{x^2 + a^2} \\, dx = \\frac{1}{a} \\cot^{-1} \\left(\\frac{x}{a}\\right) + C$</li> <li>$\\int \\frac{1}{x \\sqrt{x^2 + a^2}} \\, dx = \\frac{1}{a} \\sec^{-1} \\left|\\frac{x}{a}\\right| + C$</li> <li>$\\int \\frac{-1}{x \\sqrt{x^2 - a^2}} \\, dx = \\frac{-1}{a} \\csc^{-1} \\left|\\frac{x}{a}\\right| + C$</li> </ol>"},{"location":"etc/mathematics/maths-formula/#logarithmic-and-advanced-integrals","title":"Logarithmic and Advanced Integrals","text":"<ol> <li>$\\int e^{x} f(x) + f'(x) \\, dx = e^x f(x) + C$</li> <li>$\\int \\frac{dx}{x^2 + a^2} = \\frac{1}{a} \\ln |x + \\sqrt{x^2 + a^2}| + C$</li> <li>$\\int \\frac{dx}{x^2 - a^2} = \\frac{1}{2a} \\ln \\left|\\frac{x - a}{x + a}\\right| + C$</li> <li>$\\int \\frac{dx}{a^2 - x^2} = \\frac{1}{2a} \\ln \\left|\\frac{a + x}{a - x}\\right| + C$</li> <li>$\\int \\frac{dx}{a^2 - x^2} = \\frac{1}{2a} \\ln \\left|\\frac{a + x}{x - a}\\right| + C$</li> </ol>"},{"location":"etc/mathematics/maths-formula/#additional-properties","title":"Additional Properties","text":"<ul> <li>$\\int k f(x) \\, dx = k \\int f(x) \\, dx$</li> <li>$\\int [f(x) \\pm g(x)] \\, dx = \\int f(x) \\, dx \\pm \\int g(x) \\, dx$</li> <li>$\\int c \\, dx = c x + C$</li> </ul>"},{"location":"etc/mathematics/maths-formula/#line-equations","title":"Line Equations","text":"<ol> <li> <p>$ax + by + c=0$</p> <p>$m = -a/b$ (Slope of line)</p> </li> <li> <p>One Point form of line</p> <p>$$ y - y_1 = m (x-x_1) $$</p> </li> <li> <p>Two point form of line</p> </li> </ol> <p>$$    y-y_1 = \\frac{y_2-y_1}{x_2-x_1}{x-x_1}    $$</p> <ol> <li>Intercept Form of line</li> </ol> <p>$$    \\frac{x}{a} + \\frac{y}{b} = 1    $$</p> <p></p> <ol> <li>Normal form of line</li> </ol> <p>$$    x\\cos \\theta + y\\sin\\theta = P    $$</p> <p></p> <ol> <li>Point Slope form</li> </ol> <p>$$    y = mx+ c    \\\\    \\text{Where $m$ is slope of line defined as  } m = \\frac{y_2 - y_1}{x_2 - x_1}    $$</p> <p></p>"},{"location":"etc/mathematics/maths-formula/#distance-of-a-point-from-a-line","title":"Distance of a point from a line","text":"<p>$$ \\text{Dist}_{PA}= \\Bigg|{\\frac{ax_1 + by_1 + c}{\\sqrt{a^2+b^2}}}\\Bigg| $$</p> <p></p>"},{"location":"etc/mathematics/maths-formula/#distance-between-two-lines","title":"Distance between two lines","text":"<p>$y = mx + c_1 \\qquad y= mx+c_2$</p> <p>$$ \\text{d} = \\Bigg| \\frac{c_1 - c_2}{\\sqrt{1+m^2}}\\Bigg| = \\Bigg|\\frac{c_1 - c_2}{\\sqrt{a^2+b^2}}\\Bigg| $$</p>"},{"location":"etc/mathematics/maths-formula/#angle-between-two-lines","title":"Angle between two lines","text":"<p>Where $m_1$ and $m_2$ are slopes of two lines.</p> <p>$$ \\tan\\theta = \\frac{m_2 - m_1}{1+m_2m_1} $$</p> <ul> <li>If line $l_1$ and $l_2$ are orthogonal to each other, then. $m_1m_2 = -1$</li> <li>Collinearity of points</li> </ul> <p>Slope of $AB$ = Slope of $AC$</p> <p></p>"},{"location":"etc/mathematics/maths-formula/#shapes-csacurved-surface-area-ttotalsa-and-volume","title":"Shapes CSA(Curved Surface Area), T(Total)SA, and volume","text":""},{"location":"etc/mathematics/maths-formula/#frustum","title":"Frustum","text":"<p>$\\text{CSA} = \\pi l (r_1+ r_2)$</p> <p>$\\text{TSA} = \\pi r_1^2 + \\pi r_2^2 + \\pi l(r_1+r_2)$</p> <p>$\\text{Volume} = \\frac{1}{3}\\pi h (r_1^2 + r_2^2 + r_1r_2)$</p> <p>where $l= \\sqrt{h^2+(r_1-r_2)^2}$</p> <p></p>"},{"location":"etc/mathematics/maths-formula/#adjoint-and-inverse","title":"Adjoint and Inverse","text":"$A(\\text{adj} A) = \\|A\\| I_n = (\\text{adj}) A$               $A^{-1} = \\frac{1}{\\|A\\|} (\\text{adj} A)$               $(A^\\top)^{-1} = (A^{-1})^\\top$               $\\|A \\enspace adj A\\| = \\|A\\|^{n}$               $\\|A^\\top\\| = \\|A\\| $               $AA^{-1} = I_n$               $(A^{-1})^{-1} = A$               $(AB)^{-1} = B^{-1}A^{-1}$               $\\text{adj}\\space AB = (\\text{adj} B)(\\text{adj} A)$               $\\|AB\\| = \\|A\\| \\|B\\|$               $\\text{adj}A^\\top = (\\text{adj} A)\\top$               $\\|KA\\| = K^n \\|A\\|$               $AA^{-1}=I$               $A^{-1}I = A^{-1}$               $\\|\\text{adj} A\\| = \\|A\\|^{n-1}$               $\\text{adj} \\space(adj A) = \\|A\\|^{n-2} A$               $\\|\\text{adj} \\space(adj A)\\| = \\|A\\|^{(n-1)^{2}}$"},{"location":"etc/mathematics/maths-formula/#binomial-theorem","title":"Binomial Theorem","text":"<p>Initial conditions:</p>          $^nc_0 = 1$               $^nc_1 = n$               $^nc_n = 1$               $^nc_{n-1} = n$      <p>Basic expansion:</p> <p>$$(x+a)^n = {^nc_0}x^na^0 + {^nc_1}x^{n-1}a^1 + {^nc_2}x^{n-2}a^2 + ... + {^nc_n}x^0a^n$$</p> <p>Sum and difference formulas:</p> <p>$$(x+a)^n + (x-a)^n = 2[{^nc_0}x^na^0 + {^nc_2}x^{n-2}a^2 + {^nc_4}x^{n-4}a^4 + ...]$$ $$(x+a)^n - (x-a)^n = 2[{^nc_1}x^{n-1}a^1 + {^nc_3}x^{n-3}a^3 + {^nc_5}x^{n-5}a^5 + ...]$$</p> <p>Number of terms:</p> When n is odd When n is even (x+a)^n + (x-a)^n $(\\frac{n+1}{2})$ terms $(\\frac{n}{2})$ terms (x+a)^n - (x-a)^n $(\\frac{n+1}{2})$ terms $(\\frac{n}{2})$ terms"},{"location":"etc/mathematics/maths-formula/#general-term-and-middle-term","title":"General Term and Middle Term","text":"<p>General term: $t_{r+1} = {^nc_r}x^{n-r}a^r$</p> <p>Middle term occurs at:</p> <ul> <li> <p>If n is odd: $(\\frac{n+1}{2})$ &amp; $(\\frac{n+3}{2})$ terms</p> </li> <li> <p>If n is even: $(\\frac{n}{2}+1)$ terms</p> </li> </ul>"},{"location":"etc/mathematics/maths-formula/#coefficient-tables","title":"Coefficient Tables","text":"Coefficient of Binomial Expression is $(r+1)^{\\text{th}}$ $(1+x)^n$ ${^nc_r}$ $x^r$ $(1+x)^n$ ${^nc_r}$ $x^r$ $(1-x)^n$ $(-1)^r\\enspace{^nc_r}$ $(r+1)^{\\text{th}}$ $(1-x)^n$ $(-1)^r\\enspace{^nc_r}$"},{"location":"etc/mathematics/maths-formula/#relations","title":"Relations","text":"$\\frac{^nc_r}{^nc_{r-1}} = \\frac{n-r+1}{r}$               $^nc_r = (\\frac{n}{r})\\enspace{^{n-1}c_{r-1}}$               $\\frac{^nc_{r+1}}{^nc_r} = \\frac{n-r}{r+1}$               $\\frac{t_{r+1}}{t_r} = \\frac{n-r+1}{r} \\cdot \\frac{a}{x}$"},{"location":"etc/mathematics/maths-formula/#probability","title":"Probability","text":"$P_r = \\frac{n!}{(n-r)!}$               $C_r = \\frac{n!}{(n-r)!r!}$      Operation Notation A or B $A \\cup B$ A and B $A \\cap B$ A but not B $A \\cap \\bar{B}$ B but not A $\\bar{A} \\cap B$ Neither A nor B $\\bar{A} \\cap \\bar{B}$ At least one of A,B,&amp; C $A \\cup B \\cup C$ Exactly one of A,B $(A \\cap \\bar{B}) \\cup (\\bar{A} \\cap B)$ All Three of A,B,&amp; C $A \\cap B \\cap C$ Exactly Two of A,B,&amp; C $(A \\cap B \\cap \\bar{C}) \\cup (A \\cap \\bar{B} \\cap C) \\cup (\\bar{A} \\cap B \\cap C)$ Exactly One of A,B,&amp; C $(A \\cap \\bar{B} \\cap \\bar{C}) \\cup (\\bar{A} \\cap B \\cap \\bar{C}) \\cup (\\bar{A} \\cap \\bar{B} \\cap C)$ <p>The probability formulas:</p>  $$ \\begin{split} P(A \\cup B) &amp;= P(A) + P(B) - P(A \\cap B) \\\\ &amp;= 1 - P(\\overline{A \\cup B}) \\\\ &amp;= 1 - P(\\overline{A} \\cap \\overline{B}) \\\\ &amp;= 1 - P(\\bar{A})P(\\bar{B}) \\end{split} $$   $$ \\tag{A and B\\\\ are mutually exclusive} P(A \\cup B) = P(A) + P(B) $$   $$ \\begin{split} P(A \\cup B \\cup C) &amp;= P(A) + P(B) + P(C) - P(A \\cap B) - P(B \\cap C) - P(A \\cap C) + P(A \\cap B \\cap C) \\end{split} $$   $$ \\tag{A,B and C\\\\ are mutually exclusive} P(A \\cup B \\cup C) = P(A) + P(B) + P(C) $$  <ul> <li> <p>Probability of occurrence of (A) only   $$P(A \\cap \\bar{B}) = P(A) - P(A \\cap B)$$</p> </li> <li> <p>Probability of occurrence of (B) only   $$P(\\bar{A} \\cap B) = P(B) - P(A \\cap B)$$</p> </li> <li> <p>P of occurrence of exactly one of A and B   $$P(A \\cap \\bar{B}) \\cup P(\\bar{A} \\cap B) \\Rightarrow P(A \\cup B) - P(A \\cap B)$$</p> </li> <li> <p>Three Events Occurring Simultaneously</p> </li> </ul> <p>$$P(A \\cup B \\cup C)$$</p> <ul> <li> <p>At Least Two Events:   $$P(A \\cap B) + P(B \\cap C) + P(C \\cap A) - 2P(A \\cap B \\cap C)$$</p> </li> <li> <p>Exactly Two Out of Three Events:   $$P(A \\cap B) + P(B \\cap C) + P(C \\cap A) - 3P(A \\cap B \\cap C)$$</p> </li> <li> <p>Exactly One Out of Three Events:   $$P(A) + P(B) + P(C) - 2P(A \\cap B) - 2P(B \\cap C) - 2P(C \\cap A) + 3P(A \\cap B \\cap C)$$</p> </li> <li> <p>Exactly One Out of Two Events   $$P(A \\cap \\overline{B}) + P(\\overline{A} \\cap B)$$</p> <ul> <li>Simplified Form:   $$P(A) + P(B) - 2P(A \\cap B) \\implies P(A \\cup B) - P(A \\cap B)$$</li> </ul> </li> <li> <p>Conditional probability P of E given A is true</p> </li> </ul>    $$   \\begin{split}   P(E_i|A) &amp;= P(E_i)P(A|E_i) \\\\ &amp;= \\sum_{i=1}^n P(E_i)P(A|E_i)   \\end{split}   $$    <p>$P(A \\cap B) = P(A)P(B|A) = P(B)P(A|B)$</p> <p>$P(A \\cap B \\cap C \\cap D) = P(A)P(B|A)P(C|A \\cap B)P(D|A \\cap B \\cap C)$</p> <p>$$P(E_i | A) = \\frac{P(E_i) P(A|E_i)}{\\sum^n_{i=1}P(E_i) P(A|E_i)}$$</p> <p>Also noting the card suit groupings shown in the image:</p> <ul> <li>Hearts and Diamonds are grouped as Red</li> <li>Spades and Clubs are grouped as Black</li> </ul>"},{"location":"etc/mathematics/maths-formula/#mean-variance-standard-deviation","title":"Mean Variance &amp; Standard Deviation","text":"<ul> <li>$p$ means success event</li> <li>$q$ means not a sucess event</li> </ul>          $P(X = r) = {}_nC_r p^r q^{n-r}$               $Variance = npq$               $Mean = np$      <p>Probability Relations: $$P(X \\leq x_i) = P(X = x_1) + P(X = x_2) + ... + P(X = x_i) = p_1 + p_2 + ... + p_i$$</p> <p>$$P(X &lt; x_i) = P(X = x_1) + P(X = x_2) + ... + P(X = x_{i-1}) = p_1 + p_2 + ... + p_{i-1}$$</p> <p>$$P(X \\geq x_i) = P(X = x_i) + P(X = x_{i+1}) + ... + P(X = x_n) = p_i + p_{i+1} + ... + p_n$$</p> <p>$$P(X &gt; x_i) = P(X = x_{i+1}) + P(X = x_{i+2}) + ... + P(X = x_n) = p_{i+1} + p_{i+2} + ... + p_n$$</p>          $P(X \\geq x_i) = 1 - P(X &lt; x_i)$               $P(X \\leq x_i) = 1 - P(X &gt; x_i)$               $P(X &gt; x_i) = 1 - P(X \\leq x_i)$               $P(X &lt; x_i) = 1 - P(X \\geq x_i)$      <p>$$P(x_i \\leq X \\leq x_j) = P(X = x_i) + P(X = x_{i+1}) + ... + P(X = x_j)$$</p> <p>$$P(x_i &lt; X &lt; x_j) = P(X = x_{i+1}) + P(X = x_{i+2}) + ... + P(X = x_{j-1})$$</p>"},{"location":"etc/mathematics/maths-formula/#mean-formulasmatehmatical-expectation","title":"Mean Formulas(Matehmatical expectation):","text":"<ul> <li>where $f$ = frequency</li> </ul>  $$ \\begin{split} \\overline{X} &amp;= p_1x_1 + p_2x_2 + ... + p_nx_n \\\\ &amp;= E(X) \\\\ &amp;= \\sum_{i=1}^n p_ix_i \\\\ &amp;= \\frac{f_1x_1}{N} + \\frac{f_2x_2}{N} + ... + \\frac{f_nx_n}{N} \\end{split} $$  <p>$$p_i = \\frac{f_i}{N}$$</p>"},{"location":"etc/mathematics/maths-formula/#variance-formula","title":"Variance Formula:","text":"$$ \\begin{split} Var(X) &amp;= E(X^2) - \\lbrace E(X)\\rbrace^2 \\\\ &amp;= \\sum_{i=1}^n p_ix_i^2 - (\\sum_{i=1}^n p_ix_i)^2 \\end{split} $$  <p>$$\\frac{\\sum x^2}{n} - (\\frac{\\sum x}{n})^2$$</p> <p>Random Variable Property:</p> <p>If $aX + b$ is a random variable with mean $aX + B$ and variance $a^2Var(X)$</p>"},{"location":"etc/mathematics/maths-formula/#standard-deviation","title":"Standard Deviation:","text":"$$ \\sigma = \\sqrt{Var(X)}  \\newline\\qquad  \\sigma^2 = \\frac{1}{N^2} \\lbrack N\\sum*{i=1}^n fx_i^2 - (\\sum*{i=1}^n fx_i)^2 \\rbrack  $$"},{"location":"etc/mathematics/maths-formula/#arithmetic-progression-ap","title":"Arithmetic Progression (AP)","text":"<ul> <li>General Form: $A, B, C$</li> </ul> <p>$$2B = A + C$$</p>"},{"location":"etc/mathematics/maths-formula/#formulas","title":"Formulas:","text":"<ul> <li>$t_n = a + (n-1)d$</li> <li>$S_n = \\frac{n}{2} \\left[2a + (n-1)d \\right]$</li> <li>$S_n = \\frac{n}{2} [a + a_n]$</li> <li>$t_n = S_n - S_{n-1}$</li> </ul>"},{"location":"etc/mathematics/maths-formula/#arithmetic-mean-am","title":"Arithmetic Mean (AM):","text":"<p>$G = \\frac{a+b}{2}$</p>"},{"location":"etc/mathematics/maths-formula/#geometric-progression-gp","title":"Geometric Progression (GP)","text":"<ul> <li>General Form: $A, B, C$</li> </ul> <p>$$B^2 = A \\cdot C$$</p>"},{"location":"etc/mathematics/maths-formula/#formulas_1","title":"Formulas:","text":"<ul> <li>$t_n = a \\cdot r^{n-1}$</li> <li>$S_n = a \\frac{r^n - 1}{r - 1}, \\, r \\neq 1$</li> <li> <p>$S_\\infty = \\frac{a}{r-1}, \\, |r| &lt; 1$</p> </li> <li> <p>where $r$ is common ratio</p> </li> </ul>"},{"location":"etc/mathematics/maths-formula/#geometric-mean-gm","title":"Geometric Mean (GM):","text":"<ul> <li>Single Mean Between $a$ and $b$: $G = \\sqrt{ab}$</li> <li>Multiple Means:</li> <li>$G_1 = ar^1$</li> <li>$G_2 = ar^2$</li> <li>$G_3 = ar^3$</li> <li>$r = \\sqrt[n+1]{\\frac{b}{a}}$</li> </ul>"},{"location":"etc/mathematics/maths-formula/#ap-gp-hp-relation","title":"AP, GP, HP Relation:","text":"<p>$$AP \\geq GP \\geq HP$$</p>"},{"location":"etc/mathematics/maths-formula/#special-sequences","title":"Special Sequences:","text":"<ol> <li>Sum of Natural Numbers:</li> </ol>  $$ 1 + 2 + 3 + \\dots + n = \\frac{n(n+1)}{2} $$  <ol> <li>Sum of Squares of Natural Numbers:</li> </ol>  $$ 1^2 + 2^2 + 3^2 + \\dots + n^2 = \\frac{n(n+1)(2n+1)}{6} $$  <ol> <li>Sum of Cubes of Natural Numbers:</li> </ol>  $$ 1^3 + 2^3 + 3^3 + \\dots + n^3 = \\left[\\frac{n(n+1)}{2}\\right]^2 $$"},{"location":"etc/mathematics/maths-formula/#homogeneous-and-non-homogeneous-equations-solutions","title":"Homogeneous and Non homogeneous equations solutions","text":"NON-HOMOGENEOUS               $AX = B$               Unique Solution      <ol> <li>$\\text{adj}(A)B = 0 \\implies \\text{Infinite Solutions}$</li> <li>$\\text{adj}(A)B \\neq 0 \\implies \\text{No Solution}$</li> </ol> When $|A| \\neq 0$ When $|A| = 0$          HOMOGENEOUS               $AX = 0$               Trivial solution $x = y = z = 0 \\enspace $:      <ol> <li>$\\text{Infinite Solutions}$ put $z = k$ and solve</li> </ol>"},{"location":"etc/mathematics/maths-formula/#finding-log","title":"Finding Log","text":"<ol> <li> <p>Given we need to find $\\log$ of $\\log 15.27$</p> <ul> <li>Move the decimal after 1st digit and introduce power of 10.</li> </ul> <p>$$ \\log \\textcolor{#f56c42}1.\\textcolor{#f56c42}5\\textcolor{#ad42f5}2\\textcolor{#f542bc}7 \\times 10^{\\textcolor{#42f5f2}1} $$</p> <p>When the decimal is moved in left/right: $$ (-)\\medspace \\overrightarrow{\\text{introduce negative powers}} \\qquad \\overleftarrow{\\text{introduce positive powers}} \\medspace(+) $$</p> </li> <li> <p>Look for $\\textcolor{#f56c42}{15}$<sup>th</sup> row and column with label $\\textcolor{#ad42f5}2$. which is $\\bold{\\textcolor{#07fc03}{1818}}$.</p> </li> <li> <p>Add Mean difference from column $\\textcolor{#f542bc}7$ in the corresponding row. which is $\\textcolor{#07fc03}{20}$</p> <p>$$ 1818 + 20 = \\bold{\\textcolor{#07fc03}{1838}} $$</p> </li> <li> <p>Write the exponent, insert decimal and write the value calculated in Step 3.</p> <p>$$ \\textcolor{#42f5f2}1.\\textcolor{#07fc03}{1838} $$</p> </li> <li> <p>So $\\log 15.27 = \\bold{\\textcolor{#07fc03}{1.1838}}$</p> </li> </ol>"},{"location":"etc/mathematics/maths-formula/#finding-antilog","title":"Finding AntiLog","text":"<ol> <li> <p>Given we need to find Antilog of $15.5932$     $$     \\log k = 15.5932 \\\\     k = Antilog (\\medspace \\textcolor{#42f5f2}{15}\\textcolor{#f56c42}{.59}\\textcolor{#ad42f5}3\\textcolor{#f542bc}2\\medspace)     $$</p> </li> <li> <p>Look for $0\\textcolor{#f56c42}{.59}$<sup>th</sup> row and column with label $\\textcolor{#ad42f5}3$. Which is $\\bold{\\textcolor{#07fc03}{3917}}$.</p> </li> <li> <p>Add Mean difference from column $\\textcolor{#f542bc}2$ to previous result. Which is $\\textcolor{#07fc03}2$</p> <p>$$ 3917 + 2 = \\bold{\\textcolor{#07fc03}{3919}} $$</p> </li> <li> <p>Add $1$ to characteristic $\\textcolor{#42f5f2}{15} = \\textcolor{#07fc03}{16}$ and add  insert the decimal from left calculated in step 2. That means we need to add decimal after $\\textcolor{#07fc03}{16}$<sup>th</sup> position</p> <p>$$ 3919\\enspace0000\\enspace0000\\enspace000 .\\\\ \\implies k = \\bold{\\textcolor{#07fc03}{3.919\\times10^{15}}} $$</p> </li> </ol> <p>Quickly write the exponential form</p> <p>Since, we want to put decimal just after 1<sup>st</sup> digit. We need to move decimal from 16<sup>th</sup> position to right after 1<sup>st</sup> digit; which will introduce +ve powers. ($16 -1 = \\textcolor{#07fc03}{15}$). Since we moved 15 positions left.</p> <p>$3.919 \\times 10^{\\textcolor{#07fc03}{15}}$</p>"},{"location":"etc/mathematics/discrete-mathematics/propositional-logic/","title":"Propositional Logic","text":"<p>Content inspired from:</p> <p>Computer Scientists use formal logical systems for expressing ideas about statements and whether they are true.</p> <p>Propositional logic stems from the word \"propositions\",  sentences that can be either true or false.</p> <p>We'll often use a variable to stand for the proposition, like:</p> <p>$$ P: \\text{The robot is blue} $$</p> <p>Now this proposition can be true/false depending on the current state of the world.</p> <ul> <li>$P$ holds true if robot is blue.</li> <li>$P$ is false if robot is not blue.</li> </ul>"},{"location":"etc/mathematics/discrete-mathematics/propositional-logic/#truth-table","title":"Truth Table","text":"<p>Truth table can display all of the possible combinations of the values for variables in a certain propositional logic and show if the formula holds true or false.</p>"},{"location":"etc/mathematics/discrete-mathematics/propositional-logic/#modifying-a-logical-formula","title":"Modifying a logical formula","text":""},{"location":"etc/mathematics/discrete-mathematics/propositional-logic/#negation-neg","title":"Negation ($\\neg$)","text":"<p>So, to convey the statement, $P$ is not true, i.e., \"The robot is blue\" does not hold true we use this symbol $\\neg$</p> <p>$$ \\neg P: \\text{The robot is not blue} $$</p>"},{"location":"etc/mathematics/discrete-mathematics/propositional-logic/#conjunction-land","title":"Conjunction ($\\land$)","text":"<p>This expresses, both statement holds true.</p> <p>$$ P: \\text{The robot is blue}\\newline Q: \\text{The robot has antenna} $$</p> <p>Then conjunction ($P \\land Q$) holds true when the robot is both blue and has antenna as well.</p> <p>So, we can visualize this with truth table as:</p> $P$ $Q$ $P \\land Q$ $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#07fc03}{\\text{true}}$ $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#07fc03}{\\text{true}}$ $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#07fc03}{\\text{true}}$ $\\textcolor{#07fc03}{\\text{true}}$ $\\textcolor{#07fc03}{\\text{true}}$"},{"location":"etc/mathematics/discrete-mathematics/propositional-logic/#disjunction-lor","title":"Disjunction ($\\lor$)","text":"<p>This expresses, at least one of them is true.</p> <p>So, disjunction ($P \\lor Q$) holds true when either the robot is blue or has antenna or both.</p> <p>We can visualize this with truth table as:</p> $P$ $Q$ $P \\lor Q$ $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#07fc03}{\\text{true}}$ $\\textcolor{#07fc03}{\\text{true}}$ $\\textcolor{#07fc03}{\\text{true}}$ $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#07fc03}{\\text{true}}$ $\\textcolor{#07fc03}{\\text{true}}$ $\\textcolor{#07fc03}{\\text{true}}$ $\\textcolor{#07fc03}{\\text{true}}$"},{"location":"etc/mathematics/discrete-mathematics/propositional-logic/#exclusive-or-oplus","title":"Exclusive Or ($\\oplus$)","text":"<p>$P \\oplus Q$ expresses, $P$ is true or $Q$ is true but not both.</p> $P$ $Q$ $P \\oplus Q$ $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#07fc03}{\\text{true}}$ $\\textcolor{#07fc03}{\\text{true}}$ $\\textcolor{#07fc03}{\\text{true}}$ $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#07fc03}{\\text{true}}$ $\\textcolor{#07fc03}{\\text{true}}$ $\\textcolor{#07fc03}{\\text{true}}$ $\\textcolor{#f56c42}{\\text{false}}$"},{"location":"etc/mathematics/discrete-mathematics/propositional-logic/#implication-implies","title":"Implication ($\\implies$)","text":"<p>If propositions are:</p> <p>$$ P: \\text{The robot is blue}\\newline Q: \\text{The robot has antenna} $$</p> <p>then, $P \\implies Q$ (1), tells if the robot is blue, then it also must be true that the robot has antenna. </p> <ol> <li>Read as $P$ implies $Q$ or if $P$ then $Q$*.</li> </ol> <p>$$ P \\implies Q: $$</p> $P$ $Q$ $P \\implies Q$ $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#07fc03}{\\text{true}}$ (1) $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#07fc03}{\\text{true}}$ $\\textcolor{#07fc03}{\\text{true}}$ (2) $\\textcolor{#07fc03}{\\text{true}}$ $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#07fc03}{\\text{true}}$ $\\textcolor{#07fc03}{\\text{true}}$ $\\textcolor{#07fc03}{\\text{true}}$ <ol> <li>The formula doesn't state when $P$ is not true, it's assumed maybe the robot has antenna, maybe it doesn't.</li> <li>The formula doesn't state when $P$ is not true, it's assumed maybe the robot has antenna, maybe it doesn't.</li> </ol> <p>Info</p> <p>Imagine you said something like, \"If it's my birthday, then I'll eat cake\".</p> <p>If it's your birthday, then as per statemtn you'll eat a cake, but it's doesn't mean that you may or may not eat a cake, if it's not your birthday.</p> <p>This logic can be also be said as $\\neg P \\lor Q$. i.e., robot is not blue or robot has an antenna.</p>"},{"location":"etc/mathematics/discrete-mathematics/propositional-logic/#biconditional-iff","title":"Biconditional ($\\iff$)","text":"<p>This expresses, read as \"if and only if\".</p> <ul> <li>If $P$ is true, then $Q$ is true</li> <li>If $P$ is false, then $Q$ is false.</li> </ul> $P$ $Q$ $P \\iff Q$ $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#07fc03}{\\text{true}}$ $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#07fc03}{\\text{true}}$ $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#07fc03}{\\text{true}}$ $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#f56c42}{\\text{false}}$ $\\textcolor{#07fc03}{\\text{true}}$ $\\textcolor{#07fc03}{\\text{true}}$ $\\textcolor{#07fc03}{\\text{true}}$"},{"location":"ml/intro-dl/","title":"Introduction to Neural Network","text":""},{"location":"ml/intro-dl/neural-networks/","title":"Index","text":""},{"location":"ml/intro-dl/neural-networks/#neural-networks-and-deep-learning","title":"Neural Networks and Deep Learning","text":""},{"location":"ml/intro-dl/neural-networks/week1/","title":"Week 1 \u2014 Neural Networks and Deep Learning","text":"<p>Notations (source) :</p> <p></p> <p></p>"},{"location":"ml/intro-dl/neural-networks/week1/#week-1-introduction-to-deep-learning","title":"Week 1 \u2014 Introduction to Deep Learning","text":"<p>May 1, 2021</p>"},{"location":"ml/intro-dl/neural-networks/week1/#welcome","title":"Welcome","text":"<ul> <li>AI (Artificial Intelligence) is the new Electricity to bring about a change and a new era</li> <li>Parts of this course:</li> <li>Neural Networks and Deep Learning. \u2192 Recognizing cats</li> <li>Improving Deep Neural Networks: Hyperparmeter tuning, Regularization and Optimization</li> <li>Structuring your Machine Learning Project \u2192 Best Practices for project</li> <li>Convolutional Neural Networks</li> <li>Natural Language Processing: Building sequence models</li> </ul>"},{"location":"ml/intro-dl/neural-networks/week1/#introduction-to-deep-learning","title":"Introduction to Deep Learning","text":"<p>What Deep Learning/ML is good for</p> <ol> <li> <p>Problems with long lists of rules: When the traditional approach fails, machine learning/deep leanring may help</p> </li> <li> <p>Continually changing environments: Deep Learning can adapt ('lear') to new scenarios</p> </li> <li> <p>Discovering insights withing large collections of data: Imaging trying to hand-craft rules for what 101 different kinds of food look like?</p> </li> </ol> <p>Where Deep Learning is (typically) not good?</p> <ol> <li>When you need explainability: the patterns learned by a deep learnign model are typically uninterpretable by human</li> <li>When the traditional approach is a better option: if you can accomplish what you need with a simple rule-based system.</li> <li>When errors are unacceptable: since the outputs of deep learning aren't always predictable. The outputs are probabilistic and not deterministic.</li> <li>When you don't have much data: deep learning models usually require a fairly large amount of data to produce great results.</li> </ol>"},{"location":"ml/intro-dl/neural-networks/week1/#what-is-neural-networks","title":"What is neural networks?","text":"<p>Neural networks are created by structuring layers, with each layer consisting of \"neurons\" which get activated depending on its activation function and the input.</p>"},{"location":"ml/intro-dl/neural-networks/week1/#supervised-learning-with-neural-networks","title":"Supervised Learning with Neural Networks","text":"<p>Supervised learning refers to problems when we have inputs as well as labels (outputs) to predict using machine learning techniques. Then the models figure out the mapping from inputs to output.</p> <p>The data can be:</p> <ul> <li>Structured Data (tables)</li> <li>Unstructured Data (images, audio, etc.)</li> </ul>"},{"location":"ml/intro-dl/neural-networks/week1/#why-is-deep-learning-taking-off","title":"Why is Deep Learning taking off?","text":"<p>Three reasons:</p> <ul> <li>Backpropagation algorithm</li> <li>Glorot and He initialization</li> <li>ReLU (Rectified Linear Unit) activation function</li> </ul> <p>Other reasons:</p> <ul> <li>Data</li> <li>Computation power</li> <li>Algorithms</li> </ul> <p>The scale at which we creating data is also important, as neural networks performance don't stagnant unlike traditional machine learning algorithms</p> <p>In this course $m$ denotes no of training examples.</p>"},{"location":"ml/intro-dl/neural-networks/week1/#about-this-course","title":"About this Course","text":"<ul> <li>Week 1: Introduction</li> <li>Week 2: Basics of Neural Network programming</li> <li>Week 3: One hidden layer Neural Networks</li> <li>Week 4: Deep Neural Networks</li> </ul> <p>Info</p> <p>Any image used here for illustration if not mentioned, is attributed to Andrew Ng's lecture slides.</p>"},{"location":"ml/intro-dl/neural-networks/week2/","title":"Week 2 \u2014 Basics of Neural Network programming","text":"<p>May 1, 2021</p>"},{"location":"ml/intro-dl/neural-networks/week2/#logistic-regression-as-a-neural-network","title":"Logistic Regression as a Neural Network","text":"<ul> <li>Logistic Regression is an algorithm for Binary Classification.</li> </ul> <p>Consider we have an image we want to classify as cat or not of 64x64 dimension</p> <p></p> <ul> <li> <p>The images are stored as matrix pixel intensity value for different color channels. (one matrix per color channel and with each layer dimension being the dimension of image).</p> <p>To represent image we can use a single feature vector:</p> </li> </ul> <p>So $n_x$ (input size) becomes $64\\times64\\times3 = 122288$.</p>  $$ x = \\begin{bmatrix} 255 \\\\ 231 \\\\...\\\\255\\\\134\\\\...\\\\255\\\\134\\\\93 \\end{bmatrix} $$  <p>Notations:</p> <p></p>"},{"location":"ml/intro-dl/neural-networks/week2/#logistic-regression","title":"Logistic Regression","text":"<p>In formal terms for cat classification problem:</p> <p>Given $x$, want $\\hat{y} = \\text{P}(y=1|x)$.</p> <p>where,</p> <ul> <li>$x \\in \\mathbb{R}^{n_x}$</li> </ul> <p>Parameters: $w\\in \\mathbb{R}^{n_x}, b\\in \\mathbb{R}$</p>"},{"location":"ml/intro-dl/neural-networks/week2/#output-or-activation-function","title":"Output or Activation function","text":"<p>Output : $\\hat{y} = \\sigma(w^\\intercal x + b)$</p> <p>where,</p>  $$ \\sigma(Z) = \\frac{1}{1+ e^{-z}} \\\\[10pt] z = w^\\intercal + b $$  Exponential function $y = e^x$ (source: Wikipedia <p>Hence, this satisfy the two conditions:dx</p> <p>If $z$ is large:</p> <p>$$ \\sigma(z) \\approx \\frac{1}{1+0} = 1 $$</p> <p>If z is large negative number then, the exponential function returns very big value.</p> <p>$$ \\sigma(z) \\approx \\frac{1}{1+bigno} \\approx 0 $$</p> <p></p> <p>Warning</p> <p>In a alternate notation you might see (like in Hands-on machine learning book) $\\hat{y} = \\sigma(x^{\\intercal}\\theta)$ instead of $\\sigma(w^\\intercal x + b)$. In this notation $\\theta_0$ represent bias vector and rest of the $\\theta$ vector contains the weights.</p> <p>But in this course we will not be using this convention.</p>"},{"location":"ml/intro-dl/neural-networks/week2/#logistic-regression-cost-function","title":"Logistic Regression Cost Function","text":"<ul> <li>Cost function (loss function for single training example) to optimize, want to be as small as possible.The Loss function is cost function averaged for all training instances.</li> </ul> <p>$$ J(w, b)=J(\\theta) = -\\frac{1}{m}\\sum^m_{i=1}\\bigg[y^{(i)}\\log\\Big(\\hat{y}^{(i)}\\Big)+ \\Big(1-y^{(i)}\\Big)\\log\\Big(1-\\hat{y}^{(i)}\\Big)\\bigg] $$</p> <p></p> <ul> <li>The cost function for single training example, we want:</li> </ul>  $$ c(\\theta) = \\begin{cases}-\\log(\\hat{y}) &amp; \\text{if}\\enspace y=1\\\\-\\log(1-\\hat{y}) &amp; \\text{if}\\enspace y=0\\end{cases} $$ \u200b <ul> <li>That means</li> <li>If label is 1, we want $(-\\log(\\hat{y}))$ to be as small as possible $\\rightarrow$ want $(\\log\\hat{y})$ to be large as possible $\\rightarrow$ want $\\hat{y}$ to be large (i.e. $\\approx 1$).</li> <li>If label is 0, we want $(-\\log(1- \\hat{y}))$ to be as small as possible $\\rightarrow$ want $(\\log1-\\hat{y})$ to be large $\\rightarrow$ want $\\hat{y}$ to be small.</li> </ul>"},{"location":"ml/intro-dl/neural-networks/week2/#gradient-descent","title":"Gradient Descent","text":"<p>May 2, 2021</p> <p>Gradient Descent is a generic optimization algorithm capable of finding optimal solutions to a wide range of problems, The general idea of Gradient Descent is to tweak parameters iteratively to minimize cost function.</p> <p>We have a cost function for logistic regression.</p> <p>$$ J(w, b)=J(\\theta) = -\\frac{1}{m}\\sum^m_{i=1}\\bigg[y^{(i)}\\log\\Big(\\hat{y}^{(i)}\\Big)+ \\Big(1-y^{(i)}\\Big)\\log\\Big(1-\\hat{y}^{(i)}\\Big)\\bigg] $$</p> <p>Where.</p> <ul> <li>$\\alpha$ is a hyperparmeter for learning rate.</li> <li>Derivative tells the slope of the function OR how a small change in a value what change comes to the function.</li> <li>On the left side the derivative (slope) will be negative making us increasing the weights.</li> <li>On the right side the derivative (slope) will be positive which will result in decreasing the weights.</li> </ul> <p></p> <ul> <li>So we will update the weights and bias like this:</li> </ul> <p>$$ w = w - \\alpha\\frac{\\partial J(w,b)}{\\partial w} $$</p> <p>$$ b = b - \\alpha \\frac{\\partial J(w, b)}{\\partial b} $$</p> <p>Tip</p> <p> Remember $d$ is used for derivative whereas $\\partial$ is used to denote partial derivative when the function we want to derivate has multiple other variables (which are considered constant at the time we are finding the derivative).</p>"},{"location":"ml/intro-dl/neural-networks/week2/#derivatives","title":"Derivatives","text":""},{"location":"ml/intro-dl/neural-networks/week2/#computation-graph","title":"Computation Graph","text":"<p>A computational graph is defined as a directed graph where the nodes correspond to mathematical operations. Computational graphs are a way of expressing and evaluating a mathematical expression.</p> <ul> <li>The computation graph explains why the computations of neural network is organised with first a forward pass and then a backward pass in Backpropagation algorithm.</li> <li>Reverse-mode autodiff performs a forward pass through a computation graph, computing every node's value for the current training batch, and then it performs a reverse pass, computing all the gradients at once.</li> <li>Let's say we have a function</li> </ul> <p>$$ J(a, b, c) = 3(a + bc) $$</p> <ul> <li>In this we have 3 steps (represented by nodes) to compute. These are:</li> </ul> <p>$u = bc$</p> <p>$v = a+u$</p> <p>$J = 3v$</p> The blue arrows represent the forward pass and Red arrows the Backward pass"},{"location":"ml/intro-dl/neural-networks/week2/#computing-with-a-computation-graph","title":"Computing with a computation Graph","text":"<ul> <li>Let's say we want to compute the derivative of $J$ with respect to $v$. $\\frac{dJ}{dv} = ?$</li> <li>i.e. for a little change in $v$ how the value of $J$ changes?</li> <li>We know from our calculus class, the derivative should be:</li> </ul> <p>$$ \\frac{dJ}{dv} = \\bold{3} $$</p> <ul> <li>Then we want to compute the derivative of $J$ with respect to $a$. We can use chain rule which says:</li> </ul> <p>If $a$ affects $v$ affects $J$ ($a \\rightarrow v \\rightarrow J$) then the amounts that $J$ changes when you nudge $a$ is the product of how much $v$ changes when you nudge $a$ times how much $J$ changes when you nudge $v$.</p> <p>$$ \\frac{dv}{da} = \\bold{1} \\ [10pt] \\frac{dJ}{da} = \\frac{dJ}{dv}\\frac{dv}{da} = 3 \\times 1 = \\bold{3} $$</p> <ul> <li>This illustrates how computing $$\\frac{dJ}{dv}$$ let's us compute $$\\frac{dJ}{da}$$.</li> <li>In the code we will be using <code>dvar</code> to represent the the derivative of the final output variable with respect to any variable <code>var</code>.</li> <li>Let's do it for other variables.</li> <li>What is the derivative of $J$ with respect to $u$.</li> </ul> <p>$$ \\frac{dJ}{du} = \\frac{dJ}{dv}\\frac{dv}{du} = 3 \\times 1 = \\bold{3} $$</p> <ul> <li>And then derivative of $J$ with respect to $b$</li> </ul> <p>$$ \\frac{du}{db} = 2 $$</p> <p>$$ \\frac{dJ}{db} = \\frac{dJ}{du}\\frac{du}{db} = 3 \\times 2 = \\bold{6} $$</p> <ul> <li>The derivative of $J$ with respect to $c$</li> </ul> <p>$$ \\frac{dJ}{dc} = \\frac{dJ}{du}\\frac{du}{dc} = 3 \\times 3 = \\bold{9} $$</p>"},{"location":"ml/intro-dl/neural-networks/week2/#gradient-descent-for-logistic-regression","title":"Gradient Descent for Logistic Regression","text":"<ul> <li>Logistic regression recap</li> </ul> <p>$z = w^\\intercal +b$</p> <p>$\\hat{y} = \\hat{p} = \\sigma(z)$</p> <p>$L(\\hat{y}, y) = - \\Big[y\\log (a) + (1-y)\\log(1-\\hat{y})\\Big]$</p> <ul> <li>Let's say we have only two features, then we will have two weight and a bias.</li> </ul> <p>The computation graph</p> <p></p> <p></p> The photo above uses $a$ for prediction $\\hat{y}$ <p>Hence, we use $dz = (a-y)$ for calculating Loss function for single instance with respect to $w_1$ by calulating:</p> <p>$$ \\frac{dL}{dw_1} = \\frac{dL}{da}\\frac{da}{dz}\\frac{dz}{dw_1} = (a-y)(x_1) = \\bold{x_1\\frac{dL}{dz}} $$</p> <p>and with respect to $w_2$</p> <p>$$ \\frac{dL}{dw_2} = \\frac{dL}{da}\\frac{da}{dz}\\frac{dz}{dw_2} = (a-y)(x_2) = \\bold{x_2\\frac{dL}{dz}} $$</p> <p>for bias it remains same:</p> <p>$$ \\frac{dL}{db} = \\bold{\\frac{dL}{dz}} $$</p> <p>This makes the updates like this:</p> <p>$$ w_1 = w_1 - \\alpha dw_1\\\\ w_2 = w_2 = \\alpha dw_2\\\\b = b -\\alpha db $$</p> <ul> <li>where $dw_1, dw_2, db$ as said earlier represents the derivative of the final output variable (Loss function) with respect to respective variable.</li> </ul>"},{"location":"ml/intro-dl/neural-networks/week2/#gradient-descent-on-m-examples","title":"Gradient Descent on $m$ Examples","text":"<p>This Loss function is for $m$ training examples computed over $(i)^\\text{th}$ instances.</p> <p>$$ J(w, b)=J(\\theta) = -\\frac{1}{m}\\sum^m_{i=1}\\bigg[y^{(i)}\\log\\Big(\\hat{y}^{(i)}\\Big)+ \\Big(1-y^{(i)}\\Big)\\log\\Big(1-\\hat{y}^{(i)}\\Big)\\bigg] $$</p> <ul> <li>where $$\\hat{y} = \\sigma(z^{(i)}) = \\sigma(w^\\intercal x^{(i)} + b)$$ is the prediction over one training example.</li> </ul> <p>So for calculating Cost function derivative with respect to first weight becomes:</p> <p>$$ \\frac{\\partial J}{\\partial w_1} = \\frac{1}{m}\\sum^m_{i=1} \\frac{\\partial}{\\partial w_1}L(\\hat{y}^{(i)}, y^{(i)}) $$</p> <p>Which as we have seen will then be:</p> <p>$$ \\frac{\\partial J}{\\partial w_1} = \\frac{1}{m}\\sum^m_{i=1} dw_1^{(i)} \\qquad\\qquad \\frac{\\partial J}{\\partial w_2} = \\frac{1}{m}\\sum^m_{i=1} dw_2^{(i)} $$</p>"},{"location":"ml/intro-dl/neural-networks/week2/#wrap-up-in-algorithm-what-we-can-do","title":"Wrap up in algorithm (what we can do)","text":"<p>Assuming we have only two features. The single step of gradient descent will look like:</p> <p>$j=0,\\; dw_1=0,\\; dw_2=0,\\; db=0\\\\\\text{For} \\enspace i=1\\enspace\\text{to}\\enspace m:$</p> <p>$\\qquad z^{(i)} = w^\\intercal x^{(i)} + b\\\\\\qquad \\hat{y}^{(i)} = \\sigma(z^{(i)})\\\\\\qquad J \\enspace+= -\\Big[y^{(i)}\\log\\hat{y}^{(i)}+ \\big(1-y^{(i)}\\big)\\log\\big(1-\\hat{y}^{(i)}\\big)]\\\\\\qquad dz^{(i)} = \\hat{y}^{(i)} - y^{(i)}\\\\\\qquad dw_1\\enspace += x_1^{(i)}dz^{(i)}\\enspace\\fcolorbox{red}{white}{here $dw_1$ is used as accumulator}\\\\\\qquad dw_2 \\enspace += x_2^{(i)}dz^{(i)}\\enspace\\fcolorbox{red}{white}{h ere $dw_2$ is used as accumulator}\\\\\\qquad db\\enspace+= dz^{(i)}$</p> <p>$J \\enspace/=m \\qquad\\fcolorbox{red}{white}{averaging cost function over $m$ training examples}\\\\dw_1\\enspace /=m; \\; dw_2\\enspace/=m\\; db\\enspace/=m\\\\w_1 = w_1 - \\alpha dw_1\\\\ w_2 = w_2 = \\alpha dw_2\\\\b = b -\\alpha db$</p> <ul> <li>We have a problem here in which we require two for loops:</li> <li>One for iterating over $m$ training example.</li> <li>another one iterating over feature set calculating the derivative per feature.</li> </ul>"},{"location":"ml/intro-dl/neural-networks/week2/#vectorization","title":"Vectorization","text":"<p>Whenever possible, avoid explicit for-loops. Use vectorization.</p> <pre><code>z = np.dot(w, x) + b\n</code></pre> <p>Vectorization relies on parallelization instructions called SIMD (Single Instruction Multiple Data), which can be be found in both CPU and GPU but GPUs are better at that.</p> And that's how we eliminate one for loop which was iterating over the features for calculating derivative and updating derivatives.. <p>IMPORTANT</p> <p>Andrew refers to $dz = a (1-a)$</p> <p>Note that Andrew is using \"$dz$\" as a shorthand to refer to $\\frac{da}{dz} = a (1-a)$ .</p> <p>Earlier in this week's videos, Andrew used the name \"$dz$\" to refer to a different derivative: $\\frac{dL}{dz} = a -y$.</p> <p>Note: here $a$ is $\\hat{y}$</p>"},{"location":"ml/intro-dl/neural-networks/week2/#vectorizing-logistic-regression","title":"Vectorizing Logistic Regression","text":"<p>We can make the prediction or the forward propagation step like this:</p>"},{"location":"ml/intro-dl/neural-networks/week2/#step-1","title":"Step 1","text":"<p>$$ Z = [z^{(i)}, z^{(2)}, ..., z^{(m)}]= w^\\intercal X + [b, b, b, ....b] $$</p> <p>where,</p> <ul> <li>$X$ represent the $(n_x, m)$ dimensional feature matrix.</li> <li>$[b, b, b, ....b]$ is bias matrix with $(1, m)$ dimension.</li> </ul> <p></p> <p>In Python</p> <pre><code>Z = np.dot(w.T, X) + b # here b is a real number which is broadcasted\n</code></pre>"},{"location":"ml/intro-dl/neural-networks/week2/#step-2","title":"Step 2","text":"<p>$$ \\hat{Y} = [\\hat{y}^{(1)}, \\hat{y}^{(2)}, ..., \\hat{y}^{(m)}] = \\sigma(Z) $$</p>"},{"location":"ml/intro-dl/neural-networks/week2/#vectorizing-logistic-regressions-gradient-output","title":"Vectorizing Logistic Regression's Gradient Output","text":""},{"location":"ml/intro-dl/neural-networks/week2/#vectorizing-calculation-of-dz-fracdldz","title":"vectorizing calculation of \"dz\" ($\\frac{dL}{dz}$)","text":"<p>We want: $dz^{(1)} = [\\hat{y}^{(1)} - y^{(1)}]$, $dz^{(2)}= [\\hat{y}^{(2)} - y^{(2)}]$,...</p> <p>$dZ = [dz^{(1)}, dz^{(2)}, ..., dz^{(m)}]$</p> <p>Then:</p> <p>$\\hat{Y} = [\\hat{y}^{(1)}, ..., \\hat{(m)}]$ $Y = [y^{(1)},..., y^{(m)}]$</p> <p>$dZ = \\hat{Y} - Y$</p>"},{"location":"ml/intro-dl/neural-networks/week2/#vectorizing-updates-of-weights-and-bias","title":"vectorizing updates of weights and bias","text":"<p>$dw = \\frac{1}{m}X{dZ}^\\intercal$</p> <p>Which creates as ($n, 1$) dimensional vector with each element being from $dz_{(i)}$ to $dz_{(n)}$ where $n$ is the number of features.</p> <pre><code>dw = (1/m)*np.dot(X, dZ.T)\n</code></pre> <p>$db = \\frac{1}{m} \\sum^m_{i=1} dz^{(i)}$</p> <p>which in python is done using</p> <pre><code>db = (1/m)*np.sum(dZ)\n</code></pre>"},{"location":"ml/intro-dl/neural-networks/week2/#implementing-logistic-regression","title":"Implementing Logistic Regression","text":"<p>$Z = w^\\intercal X +b = \\text{np.dot(w.T, X)+b}$</p> <p>$\\hat{Y} = \\sigma(Z) $</p> <p>$dZ = \\hat{Y} - Y$</p> <p>$dw= \\frac{1}{m}XdZ^\\intercal$</p> <p>$db = \\frac{1}{m}\\text{ * np.sum($dZ$)}$</p> <p>$w = w -\\alpha dw$</p> <p>$b = b - \\alpha db$</p>"},{"location":"ml/intro-dl/neural-networks/week2/#general-principle-of-broadcasting","title":"General Principle of Broadcasting","text":"<p>If we have $(m, n)$ matrix and for any operation we want to do with $(1, n)$ or $(m, 1)$ matrix, the matrix with be converted to $(m, n)$ dimensional matrix by copying.</p> <p>If we have $(m, 1)$ or $(1, m)$ matrix and for any operation we want to do with a real number $R$ get converted to $(m, 1)$ or $(1, m)$ dimensional matrix by copying $R$.</p>"},{"location":"ml/intro-dl/neural-networks/week2/#a-note-on-pythonnumpy-vectors","title":"A note on python/NumPy vectors","text":"<pre><code>import numpy as np\n\na = np.random.randn(5)\nprint(a)\n&gt;&gt;&gt; [0.502, -0.296, 0.954, -0.821, -1.462]\n\nprint(a.shape)\n&gt;&gt;&gt;(5,)\n</code></pre> <p>Vectors like <code>a</code> are called rank 1 array in python. That is it is neither a row vector, nor a column vector. Which leads to some slightly non-intuitive effects such as-</p> <pre><code>print(a.T)\n&gt;&gt;&gt; [0.502, -0.296, 0.954, -0.821, -1.462]\n\nprint(np.dot(a, a.T)   # should probanly be matrix product\n&gt;&gt;&gt;  4.06570109321\n</code></pre> <p>Instead do this:</p> <pre><code>a = np.random.rand(5, 1)\nprint(a)\n&gt;&gt;&gt; [[-0.0967]\n     [-2.3861]\n     [-9.1231]\n     [ 0.1231]\n     [ 9.1111]]\n</code></pre> <pre><code>assert(a.shape == (5, 1))\n</code></pre> <p>If you get a rank 1 array, just <code>reshape</code> it.</p>"},{"location":"ml/intro-dl/neural-networks/week2/#some-important-points","title":"Some important Points","text":"<ul> <li>Softmax function</li> </ul> <p>$$ \\text{for a matrix } x \\in \\mathbb{R}^{m \\times n} \\text{, $x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$, thus we have: } $$</p>  $$ softmax(x) = softmax\\begin{bmatrix} x\\_{11} &amp; x\\_{12} &amp; x\\_{13} &amp; \\dots &amp; x\\_{1n} \\\\\\ x\\_{21} &amp; x\\_{22} &amp; x\\_{23} &amp; \\dots &amp; x\\_{2n} \\\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\\\ x\\_{m1} &amp; x\\_{m2} &amp; x\\_{m3} &amp; \\dots &amp; x\\_{mn}\\end{bmatrix} \\\\[10pt]= \\begin{bmatrix} \\frac{e^{x\\_{11}}}{\\sum\\_{j}e^{x\\_{1j}}} &amp; \\frac{e^{x\\_{12}}}{\\sum\\_{j}e^{x\\_{1j}}} &amp; \\frac{e^{x\\_{13}}}{\\sum\\_{j}e^{x\\_{1j}}} &amp; \\dots &amp; \\frac{e^{x\\_{1n}}}{\\sum\\_{j}e^{x\\_{1j}}} \\\\\\ \\frac{e^{x\\_{21}}}{\\sum\\_{j}e^{x\\_{2j}}} &amp; \\frac{e^{x\\_{22}}}{\\sum\\_{j}e^{x\\_{2j}}} &amp; \\frac{e^{x\\_{23}}}{\\sum\\_{j}e^{x\\_{2j}}} &amp; \\dots &amp; \\frac{e^{x\\_{2n}}}{\\sum\\_{j}e^{x\\_{2j}}} \\\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\\\ \\frac{e^{x\\_{m1}}}{\\sum\\_{j}e^{x\\_{mj}}} &amp; \\frac{e^{x\\_{m2}}}{\\sum\\_{j}e^{x\\_{mj}}} &amp; \\frac{e^{x\\_{m3}}}{\\sum\\_{j}e^{x\\_{mj}}} &amp; \\dots &amp; \\frac{e^{x\\_{mn}}}{\\sum\\_{j}e^{x\\_{mj}}}\\end{bmatrix} \\\\[10pt]= \\begin{pmatrix} softmax\\text{(first row of x)} \\\\\\ softmax\\text{(second row of x)} \\\\\\ ... \\\\\\ softmax\\text{(last row of x)} \\\\\\\\\\end{pmatrix} $$  <ul> <li>L1 loss is defined as:</li> </ul> <p>$$ \\begin{align*} &amp; L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{1} $$</p> <ul> <li>L2 loss is defined as:</li> </ul> <p>$$ \\begin{align*} &amp; L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{2} $$</p> <p>A trick when you want to flatten a matrix $X$ of shape $(a,b,c,d)$ to a matrix $X_flatten$ of shape $(b\u2217c\u2217d, a)$ is to use:</p> <pre><code>X_flatten = X.reshape(X.shape[0],-1).T\n</code></pre>"},{"location":"ml/intro-dl/neural-networks/week3/","title":"Week 3 \u2014 One hidden layer Neural Networks","text":"<p>May 4, 2021</p>"},{"location":"ml/intro-dl/neural-networks/week3/#shallow-neural-network","title":"Shallow Neural Network","text":""},{"location":"ml/intro-dl/neural-networks/week3/#neural-networks-overview","title":"Neural Networks Overview","text":"<p>For new notation</p> <p>We'll use superscript square bracket one to refer to quantities associated with a particular layer.</p> <p></p>"},{"location":"ml/intro-dl/neural-networks/week3/#neural-network-representation","title":"Neural Network Representation","text":"<p>The term hidden layer refers to the fact that in the training set, for these nodes in the middle (the hidden layer neurons) are not observed.</p> Source: Wikipedia. Used for representation only. <p>The term $a$ also stands for the activations, we will use it for denoting the outputs/activations of a layer that the layer is passing on to the subsequent layers.</p> <ul> <li> <p>Input layer is passing activations $a^{[0]}$ to the hidden layer (which we previously denoting by $X$.</p> </li> <li> <p>The hidden layer will in turn generate its own activations $a^{[1]}$</p> </li> <li> <p>Which is a four dimensional vector b/c the hidden layer has four neurons</p> </li> </ul>  $$ a^{[1]} = \\begin{bmatrix}a^{[1]}_1\\\\[5pt] a^{[1]}_2\\\\[5pt]a^{[1]}_3\\\\[5pt]a^{[1]}_4\\end{bmatrix} $$  <ul> <li>The output layer generates some value $a^{[2]}$ which is just a real number $\\hat{y}$.</li> </ul> 2 layer NN: We don't count the 'input layer' <ul> <li>The parameter associated with the layers will also be denoted with Superscript notations like $W^{[1]}$, $b^{[1]}$. Here,</li> <li>$W^{[1]}$ will be $(4. 3)$ dimensional vector for $(\\text{noOfNeuronsInCurrentLayer},\\text{ noOfNeuronsInPreviviousLayer})$ with each value associated with the connection weights.</li> <li>$b^{[1]}$ will be $(4, 1)$ dimensional vector, only stores biases for number of neurons in current layer.</li> </ul>"},{"location":"ml/intro-dl/neural-networks/week3/#computing-a-neural-networks-output","title":"Computing a Neural Network's Output","text":"This is what logistic regression does. A NN is composed bunch of neurons which performs the same operation (With the activation function different if required other than sigmoid, most popular being ReLU) <p>Let's go a little deeper:</p> <ul> <li>The first neuron of the hidden layer we just saw earlier gets all inputs from input layer (and so does other neurons in hidden layer)</li> <li>It computes</li> </ul>  $$ z^{[1]}_1 = w^{[1]}_1 X + b_1^{[1]}\\\\[10pt] a^{[1]}_1 = \\sigma(z^{[1]}_1) $$  <p>where,</p>  $$ a^{[1](m)\\rightarrow \\text{ 1st neuron and m$^{th}$ training example}}_{l\\rightarrow \\text{node in layer l}} $$  <p> -   The second neuron computes,</p>  $$ z^{[1]}_2 = w^{[1]}_2 X + b^{[1]}_2\\\\[10pt] a^{[1]}_2 = \\sigma(z^{[1]}_2) $$  <ul> <li>and so does other neurons in the layer.</li> </ul> <p></p> <p>Once again writing all equations:</p> <p>$$ z^{[1]}_1 = w^{[1]}_1 X + b_1^{[1]}, \\qquad a^{[1]}_1 = \\sigma(z^{[1]}_1)\\tag{1} $$</p> <p>$$ z^{[1]}_2 = w^{[1]}_2 X + b_2^{[1]}, \\qquad a^{[1]}_2 = \\sigma(z^{[1]}_2) \\tag{2} $$</p> <p>$$ z^{[1]}_3 = w^{[1]}_3 X + b_3^{[1]}, \\qquad a^{[1]}_3 = \\sigma(z^{[1]}_3) \\tag{3} $$</p> <p>$$ z^{[1]}_4 = w^{[1]}_4 X + b_4^{[1]}, \\qquad a^{[1]}_4 = \\sigma(z^{[1]}_4) \\tag{4} $$</p>"},{"location":"ml/intro-dl/neural-networks/week3/#to-vectorize-we-can-do-like-this","title":"To vectorize we can do like this:","text":"$$ \\overbrace{\\begin{bmatrix}--w^{[1]\\bold{\\top}}_1--\\\\--w^{[1]\\bold{\\top}}_2--\\\\--w^{[1]\\bold{\\top}}_3--\\\\--w^{[1]\\bold{\\top}}_4--\\end{bmatrix}}^{W^{[1]}} \\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix}  +\\overbrace{\\begin{bmatrix}b^{[1]}_1\\\\b^{[1]}_2\\\\b^{[1]}_3\\\\b^{[1]}_4\\end{bmatrix}}^{b^{[1]}}  =\\begin{bmatrix}w^{[1]\\bold{\\top}}_1X + b^{[1]}_1\\\\w^{[1]\\bold{\\top}}_2X + b^{[1]}_2\\\\w^{[1]\\bold{\\top}}_3X + b^{[1]}_3\\\\w^{[1]\\bold{\\top}}_4X + b^{[1]}_4\\end{bmatrix}  = \\overbrace{\\begin{bmatrix}z^{[1]}_1\\\\z^{[1]}_2\\\\z^{[1]}_3\\\\z^{[1]}_4\\end{bmatrix}}^{Z^{[1]}} $$   $$ \\overbrace{ \\begin{bmatrix}a^{[1]}_1\\\\ a^{[1]}_2\\\\a^{[1]}_3\\\\ a^{[1]}_4\\end{bmatrix}}^{a^{[1]}}= \\sigma(Z^{[1]}) $$  <p>Give input $X = a^{[0]}$, then</p> <p>$z^{[1]} = W^{[1]}a^{[0]} + b^{[1]}$</p> <p>$a^{[1]} =  \\sigma(z^{[1]})$</p> <p>$z^{[1]} = W^{[2]}a^{[1]} + b ^{[2]}$</p> <p>$a^{[2]} = \\sigma(z^{[2]})$</p> <p>Where $W^{[2]}$ is $(1, 4)$ and $a^{[1]}$ is $(4, 1)$ dimensional matrices.</p>"},{"location":"ml/intro-dl/neural-networks/week3/#vectorizing-across-multiple-examples","title":"Vectorizing across multiple examples","text":"<p>If we have defined a neural network like this:</p> <p></p> <p><code>for i=1 to m:</code></p> $z^{[1](i)} = W^{[1]}a^{[0](i)} + b^{[1]}$ $a^{[1](i)} =  \\sigma(z^{[1](i)})$ $z^{[2](i)} = W^{[2]}a^{[1](i)} + b ^{[2]}$ $a^{[2](i)} = \\sigma(z^{[2](i)})$ <p>then for $m$ training examples we need,</p>  $$ X \\rightarrow a^{[2]} = \\hat{y}\\\\X^{(1)} \\rightarrow a^{[2](1)} = \\hat{y}^{(1)}\\\\X^{(2)} \\rightarrow a^{[2](2)} = \\hat{y}^{(2)}\\\\...\\\\X^{(1)} \\rightarrow a^{[2](m)} = \\hat{y}^{m)} $$"},{"location":"ml/intro-dl/neural-networks/week3/#we-can-vectorize-over-m-training-examples-as","title":"we can vectorize over m training examples as:","text":"$$ Z^{[1]} = W^{[1]}X + b^{[1]}\\\\A^{[1]} =  \\sigma(Z^{[1]})\\\\Z^{[2]} = W^{[2]}A^{[1]} + b ^{[2]}\\\\A^{[2]} = \\sigma(Z^{[2]}) $$  The blue arrows represent the forward pass and Red arrows the Backward pass <p>A good thing about these matrices $(Z, A)$ is that they represent vertically hidden units and horizontally they represent training examples.</p> <ul> <li>That means first row first column represent output from first hidden unit for first training example. The second in the first row output of same first hidden unit for second training example.</li> <li>Vertically you get output from second, third and so on hidden unit for the training examples.</li> </ul>"},{"location":"ml/intro-dl/neural-networks/week3/#explanation-for-vectorized-implementation","title":"Explanation for Vectorized Implementation","text":""},{"location":"ml/intro-dl/neural-networks/week3/#activation-functions","title":"Activation Functions","text":"<p>In real world NN, instead of using $\\sigma$ (sigmoid) activation function for every problem, we can use other activation functions. So in general we replace $\\sigma(z^{[\\mathbb{R}]})$ with $g(z^{[\\mathbb{R}]})$</p> <p>The problem with $\\tanh$ $\\big(tanh(z) = 2\\sigma(2z) -1\\big)$ or $\\sigma$ (sigmoid) function is that when inputs become large, the function saturates at -1 or 1/ 0 or 1 respectively, with derivative extremely close to zero. Thus for backpropagation there is no gradients to work with.</p> <p>One of the very commonly used Activation function is</p>"},{"location":"ml/intro-dl/neural-networks/week3/#rectified-linear-unit-relu","title":"Rectified Linear Unit (ReLU)","text":"<p>$$ ReLU(z) = \\max(0, z) $$</p> <p></p> <p>This function is continuous but unfortunately not differentiable at $z=0$ as slope changes abruptly and its derivative is zero for $z&lt;0$. But is practice, it works really well.</p> <p>But might suffer from dying ReLUs, i.e., that is some neurons stop outputting anything other than 0. In that case Leaky ReLUs and its variant but be better choice.</p>"},{"location":"ml/intro-dl/neural-networks/week3/#leaky-relu","title":"Leaky ReLU","text":"<p>$$ \\text{Leaky ReLU}_\\alpha(z) = \\max(\\alpha z. z) $$</p> <p>$\\alpha$ defines the slope of the function for $z&lt;0$, that is how much it should \"leak.</p> Source: Wikipedia <p>This small slope ensures that the neurons never really \"die\".</p>"},{"location":"ml/intro-dl/neural-networks/week3/#why-do-you-need-non-linear-activation-functions","title":"Why do you need non-linear activation functions?","text":"<p>If your activation function is linear then,</p> <ul> <li>the entire neural network simplifies down to a single neuron, since the hidden neurons don\u2019t do any \u2018processing\u2019, they just pass on their z values to the next layer, which essentially is like applying a different set of weights and running a single neuron.</li> </ul>"},{"location":"ml/intro-dl/neural-networks/week3/#derivatives-of-activation-functions","title":"Derivatives of Activation Functions","text":"<p>We often use $g'(z)$ to represent the derivative of a function with input $z$ with respect to $z$ i.e., $\\frac{d}{dz}g(z) = g'(z)$</p>"},{"location":"ml/intro-dl/neural-networks/week3/#derivative-of-sigmoid","title":"Derivative of Sigmoid","text":"<p>$$ g'(z) = \\frac{d}{dz}g(z) = g(z)\\big(1-g(z)\\big) $$</p> <p>for $z=10$, $g(z) \\approx1$ \u00a0\u00a0\u00a0| $g'(z)=1(1-1)\\approx0$</p> <p>for $z=-10$, $g(z)\\approx0$ | $g'(z)\\approx0.(1-0) \\approx0$</p> <p>for $z=0$, $g(z)=\\frac{1}{2}$ \u00a0\u00a0\u00a0\u00a0| $g'(z)=\\frac{1}{2}(1-\\frac{1}{2}) = \\frac{1}{4}$</p>"},{"location":"ml/intro-dl/neural-networks/week3/#derivative-of-tanh","title":"Derivative of \\tanh","text":"<p>$$ g'(z) = 1-\\bigg(tanh(z)\\bigg)^2 $$</p> <p>for $z=10$, $g(z) \\approx1$ | $g'(z)=1(1-1)\\approx0$</p> <p>for $z=-10$, $g(z)\\approx-1$ | $g'(z)\\approx0$</p> <p>for $z=0$, $g(z)=0$ | $g'(z)=1$</p>"},{"location":"ml/intro-dl/neural-networks/week3/#derivative-of-relu","title":"Derivative of ReLU","text":"$$ g'(x) = \\begin{cases}0&amp;\\text{if } z&lt;0 \\\\                      1&amp;\\text{if }z\\geqslant0\\\\                      \\text{undefined} &amp;\\text{if } z=0\\end{cases} $$  <p>if $z=0$, It might be undefined but will still work when implementing, you could set the derivative to be 0 or 1, it doesn't matter.</p>"},{"location":"ml/intro-dl/neural-networks/week3/#leaky-relu_1","title":"Leaky ReLU","text":"$$ g'(z)=\\begin{cases}\\alpha &amp;\\text{if } z&gt;0\\\\1 &amp;\\text{if }z\\geqslant0\\end{cases} $$"},{"location":"ml/intro-dl/neural-networks/week3/#gradient-descent-for-neural-networks-2-layer-1-hidden","title":"Gradient Descent for Neural Networks (2 layer, 1 hidden)","text":"<p>Parameters:</p> <p>$W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}$</p> <p>$n_x=n^{[0]}, n^{[1]}, n^{[2]}=1$</p> <p>Where,</p> <ul> <li>$W^{[1]}$ is $(n^{[1]}, n^{[0]})$ dimensional matrix</li> <li>$W^{[2]}$ is $(n^{[2]}, n^{[1]})$ dimensional matrix</li> <li>$b^{[1]}$ is $(n^{[1]}, 1)$ dimensional matrix</li> <li>$b^{[2]}$ is $(n^{[2]}, 1)$ dimensional matrix</li> </ul> <p>Cost Function</p> <p>$$ J(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}) = \\frac{1}{m}\\sum^m_{i=1}L(\\hat{y}, y) $$</p> <p>Where,</p> <ul> <li>$\\hat{y}$ is $a^{[2]}$</li> </ul> <p>Gradient Descent</p> <p>When training a neural network it is important to initialize the parameters randomly rather than to all zeros.</p> <p>$\\text{Repeat}\\lbrace$</p> <p>$\\newline\\qquad\\text{Compute the prediction} (\\hat{y}^{[i]}, i=1...m)$</p> <p>$\\qquad dW^{[1]}=\\frac{dJ}{dW^{[1]}}$, $db^{[1]} = \\frac{dJ}{db^{[1]}}$, $...$</p> <p>$\\qquad W^{[1]} =W^{[1]} - \\alpha dW^{[1]}$</p> <p>$\\qquad b^{[1]} = b^{[1]} - \\alpha db^{[1]}$</p> <p>$\\qquad W^{[1]} = W^{[2]} - \\alpha dW^{[2]}$</p> <p>$\\qquad b^{[2]} = b^{[2]} - \\alpha db^{[2]}$</p> <p>$\\}$</p>"},{"location":"ml/intro-dl/neural-networks/week3/#formulas-for-computing-derivatives-assuming-for-binary-classification","title":"Formulas for computing derivatives (assuming for binary classification)","text":"<p>Forward Propagation</p> <p>$Z^{[1]} = W^{[1]}X + b^{[1]}$</p> <p>$A^{[1]} =  \\sigma(Z^{[1]})$</p> <p>$Z^{[2]} = W^{[2]}A^{[1]} + b ^{[2]}$</p> <p>$A^{[2]}=\\hat{Y}^{[2]} = g^{[1]}(Z^{[2]})=\\sigma(Z^{[2]})$</p> <p>Back Propagation</p> <p>$dZ^{[2]} = A^{[2]} - Y$</p> <p>$dW^{[2]} = \\frac{1}{m}dZ^{[2]}A^{[1]\\bold{\\top}}$</p> <p>$db^{[2]} = \\frac{1}{m}\\text{ np.sum($dZ^{[2]}$, axis=1, keepdims=True)}\\;\\fcolorbox{red}{white}{avoids  1 rank matrix}$</p> <p>$dZ^{[1]} = W^{[2]\\bold{\\top}} dZ^{[2]} * g'^{[1]}(Z^{[1]})$</p> <p>$dW^{[1]} = \\frac{1}{m} dZ^{[1]}X^\\bold{\\top}$</p> <p>$db^{[1]} = \\frac{1}{m}\\text{np.sum($dZ^{[1]}$, axis=1, keepdims=True})$</p> <p>Where,</p> <ul> <li>$g'^{[1]}(Z^{[1]})$ is $(1-\\text{np.power($A^{[1]}$},\\, 2)$</li> </ul>"},{"location":"ml/intro-dl/neural-networks/week3/#backpropagation-intuition-optional","title":"Backpropagation intuition (optional)","text":"<p>Warning</p> <p>Please note that in the this video at 8:20, the text should be \"$dw^{[1]}$\" instead of \"$dw^{[2]}$\".</p>"},{"location":"ml/intro-dl/neural-networks/week3/#random-initialization","title":"Random Initialization","text":"<p>If two hidden units with the same activation function are connected to the same inputs, then these units must have different initial parameters. If they have the same initial parameters, then a deterministic learning algorithm applied to a deterministic cost and model will constantly update both of these units in the same way.</p> <p>This helps in breaking symmetry and every neuron is no longer performing the same computation.</p> <p>$W^{[1]} = \\text{np.random.randn((2, 2)) * 0.01}$</p> <p>$b^{[1]} = \\text{np.zero((2, 1))}$</p> <p>We need to initialize the weights to a very small value, as when</p> <ul> <li>During forward propagation calculating $Z$ with become very big and hence increasing the output of activation function (for $\\sigma$ or $\\tanh$) thus saturating the function and slowing down learning.</li> </ul> <p>If we don't have these functions throughout our NN, it's less of an issue.</p> <p>But for binary classification this might be required for output layer.</p>"},{"location":"ml/intro-dl/neural-networks/week4/","title":"Week 4 \u2014 L Layers Deep Neural Networks","text":"<p>Computation of L layered neural network</p> <p>May 6, 2021</p> <p>New notations introduces</p> <ul> <li>$L$ to denote the number of layers in the network</li> </ul> <p>In previous week example $L = 2$ - $n^{[l]}$ to denote the number of units/neurons in the layer $l$</p> <p>In previous week example $n^{[1]} = 4, \\, n^{[0]} = n_x = 3$</p>"},{"location":"ml/intro-dl/neural-networks/week4/#forward-propagation-in-a-deep-network","title":"Forward Propagation in a Deep Network","text":"<p>Let's go over what forward propagation will look like for a single training example $x$ (which is also $a^{[0]}$).</p> <p></p> <p>$z^{[1]} = W^{[1]}a^{[0]}+b^{[1]}$</p> <p>$a^{[1]} = g^{[1]}(z^{[1]})$</p> <p>$z^{[2]} = W^{[2]}a^{[1]}+b^{[2]}$</p> <p>$a^{[2]} = g^{[2]}(z^{[2]})$</p> <p>$\\qquad....$</p> <p>$z^{[4]} = W^{[4]}a^{[3]} + b^{[4]}$</p> <p>$a^{[4]} = \\hat{y} = g^{[4]}(z^{[4]})$</p>"},{"location":"ml/intro-dl/neural-networks/week4/#generally-equation-for-the-forward-propagation-of-any-layer-l","title":"Generally equation for the forward propagation of any layer l","text":"<p>$z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$ <p>$ a^{[l]} = g^{[l]}(z^{[l]})$</p>"},{"location":"ml/intro-dl/neural-networks/week4/#vectorizing-over-whole-training-set-or-m-training-example","title":"Vectorizing over whole training set or m training example","text":"<p>$Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$</p> <p>$A^{[l]} = g^{[l]}(Z^{[l]})$</p> <p>Here $A^{[0]}$ is going to be $X$.</p>"},{"location":"ml/intro-dl/neural-networks/week4/#getting-your-matrix-dimensions-right","title":"Getting your matrix dimensions right","text":"<p>Warning</p> <p>In the this video, at time 6:35, the correct formula should be:</p> <p>$a^{[l]} = g^{[l]}(z^{[l]})$</p> <p>Note that \"a\" and \"z have dimensions $(n^{[l]},1)$.</p> <ul> <li>Check the dimension of the matrix $W^{[l]}$ as to be $(n^{[l]}, n^{[l-1]})$</li> <li>and dimension of $X$ or $A^{[0]}$ to be $(n^{[0]}, m)$.</li> <li>The dimension of the matrix $b^{[l]}$ to be $(n^{[l]},1)$</li> <li>The dimension of $Z^{[l]}$ and $A^{[l]}$ to be $(n^{[l]}, m)$</li> <li>The dimension of $dW^{[l]}$ be $(n^{[l]}, n^{[l-1]})$.</li> <li>The dimension of $db^{[l]}$ be $(n^{[l]}, 1)$</li> <li>The dimension of $dZ^{[l]}$ and $dA^{[l]}$ be $(n^{[l]}, m )$.</li> </ul>"},{"location":"ml/intro-dl/neural-networks/week4/#why-deep-representations","title":"Why deep representations?","text":"<p>Deep representations help in generating lower level features which can then be used to build and then composing these features in later layer of NN to create higher level features and understand complex functions.</p>"},{"location":"ml/intro-dl/neural-networks/week4/#circuit-theory-and-deep-learning","title":"Circuit theory and deep learning","text":"<p>Informally: There are functions you can compute with a \"small\" L-layer deep neural network that shallower networks require exponentially more hidden units to compute.</p>"},{"location":"ml/intro-dl/neural-networks/week4/#building-blocks-of-deep-neural-networks","title":"Building blocks of deep neural networks","text":"<p>We have already seen the forward propagation for the $L$ layers neural network in which we cached $Z^{[l]}$.</p> <p>The the backward step would be like to calculate $dA^{[l-1]},\\enspace dZ^{[l]},\\enspace dW^{[l]}.\\enspace db^{[l]}$ with input as cache and $dA^{[l]}$.</p> <p></p>"},{"location":"ml/intro-dl/neural-networks/week4/#forward-and-backward-propagation","title":"Forward and Backward Propagation","text":"<p>Warning</p> <p>Note that in the next video at 2:30, the text that's written should be $dw^{[l]} = dz^{[l]} * a^{[l-1]\\bold{\\top}}$</p> Forward and backward propagation. Source: Coursera Assignment <p>To output $dA^{[l-1]},\\enspace dZ^{[l]},\\enspace dW^{[l]}.\\enspace db^{[l]}$; we do:</p> <p>$dZ^{[l]} = dA^{[l]}* g'^{[l]}(Z^{[l]})$</p> <p>$dW^{[l]}=\\frac{1}{m}dZ^{[l]}.A^{[l-1]\\bold{\\top}}$</p> <p>$db^{[l]}=\\frac{1}{m}\\text{np.sum($dZ^{[l]}$, axis=1, keepdims=True)}$</p> <p>$dA^{[l-1]}=W^{[l]\\bold{\\top}}dZ^{[l]}$</p> <p>$dAL = -(\\text{np.divide($Y, AL$)} - \\text{np.divide($1-Y$, $1-AL$)}$</p>"},{"location":"ml/intro-dl/neural-networks/week4/#parameters-vs-hyperparameters","title":"Parameters VS Hyperparameters","text":"<p>Parameters are values that the neural net needs to learn by itself such as $W$ and $b$.</p> <p>Hyperparameters are the extra values we decide and that we give it that it needs to run - the learning rate, the number of iterations of gradient descent, the number of hidden layers, the number of hidden units in each layer, and the choice of activation function.</p> <p>We call these hyperparameters since these influence the actual parameters that the neural net is responsible for calculating.</p>"},{"location":"ml/mle-for-production/","title":"Index","text":"<p>title: \"MLE for Production\" description: \"\" lead: \"\" date: 2022-03-26T14:41:21+01:00 lastmod: 2022-03-26T14:41:21+01:00 draft: false images: [] type: docs weight: 20</p>"},{"location":"ml/mle-for-production/deploying-ml-models/","title":"ML Models Deployment in Production","text":"<p>Modeling serving Infrastructure, deployment options, improving prediction latency, and reduceing cost, TensorFlow Serving</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week1/","title":"Week 1 \u2014 Intro to Model Serving and Infrastructure","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week1/#introduction-to-model-serving","title":"Introduction to Model Serving","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week1/#what-exactly-is-serving-a-model","title":"What exactly is Serving a Model?","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week1/#model-serving-patterns","title":"Model Serving Patterns","text":"<p>Components used by inference process:</p> <ul> <li>A model,</li> <li>An interpreter</li> <li>Input data</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week1/#ml-worflows","title":"ML worflows","text":"<p>Batch/static Learning: model training is performed offline on a set of already collected data.</p> <p>Online/Dynamic learning: model is regularly being retrained as new data arrives as is in the case of time series data.</p> <p>Batch Inference: when the inference is not done instantly and is pushed in a batch to be done.</p> <p>Realtime Inference: predictions are generated in real time using available data at the time of the request.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week1/#important-metrics","title":"Important Metrics","text":"<ol> <li>Latency: Delay between user's action and response of application to user's action. Latency of the whole process, starting from sending data to server, performing inference using model and returning response</li> </ol> <p>Minimal latency is a key requirement to maintain customer satisfaction</p> <ol> <li> <p>Throughput: Number of successful requests served per unit time say one second</p> <p>In some applications only throughput is important and not latency.</p> </li> <li> <p>Cost</p> <p>The cost associated with each inference should be minimised</p> <p>Important infrastructure requirements that are expensive:</p> <ul> <li>CPU</li> <li>Hardware Accelerators like GPU</li> <li>Caching infrastructure for faster data retrieval</li> </ul> </li> </ol>"},{"location":"ml/mle-for-production/deploying-ml-models/week1/#minimizing-latency-maximising-throughput","title":"Minimizing latency, Maximising Throughput","text":"<p>Minimizing latency</p> <ul> <li>Airline Recommendation Service</li> <li>Reduce latency for user satisfaction</li> </ul> <p>Maximising Throughput</p> <ul> <li>Airline recommendation service faces high load of inference requests per second.</li> </ul> <p>Scale infrastructure (number of servers, caching requirements etc.) to meet requirements</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week1/#balance-cost-latency-and-throughput","title":"Balance Cost, Latency and Throughput","text":"<ul> <li>Cost increases as infrastructure is scaled</li> <li>In applications where latency and throughput can suffer slightly:<ul> <li>Reduce costs by GPU sharing</li> <li>Multi-model serving etc..,</li> <li>Optimizing models used for inference</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week1/#introduction-to-model-serving-infrastructure","title":"Introduction to Model Serving Infrastructure","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week1/#optimizing-models-for-serving","title":"Optimizing Models for Serving","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week1/#as-model-complexity-increases-cost-increases","title":"As Model Complexity Increases Cost Increases","text":"<p>Increased requirements means Increased cost and increases hardware requirement management of larger model registries leading to higher support and maintenance burden.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week1/#balancing-cost-and-complexity","title":"Balancing Cost and Complexity","text":"<p>The challenge for ML practitioners is to balance complexity and cost</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week1/#optimizing-and-satisficing-metrics","title":"Optimizing and Satisficing Metrics","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week1/#use-of-accelerators-in-serving-infrastructure","title":"Use of Accelerators in Serving Infrastructure","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week1/#maintaining-input-feature-lookup","title":"Maintaining Input Feature Lookup","text":"<ul> <li>Prediction request to your ML model might not provide all features required for prediction</li> <li>For example, estimating how long food delivery will require accessing features from a data store:<ul> <li>Incoming orders (not included in request)</li> <li>Outstanding orders per minute in the past hour</li> </ul> </li> <li>Additional pre-computed or aggregated features might be read in real-time form a data store.</li> <li>Providing that data store is a cost</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week1/#nosql-databases-caching-and-feature-lookup","title":"NoSQL Databases: Caching and Feature Lookup","text":"<p>Google Cloud Memorystore: In memory cache, sub-millisecond read latency</p> <p>Google Cloud Firestore: Scaleable, can handle slowly changing data, millisecond read latency</p> <p>Google Cloud Bigtable: Scaleable, handles dynamically changing data, millisecond read latency</p> <p>Amazon DynamoDB: Single digit millisecond read latency, in memory cache available.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week1/#deployment-options","title":"Deployment Options","text":"<p>Two options:</p> <ol> <li>Data centers</li> <li>Embedded devices</li> </ol>"},{"location":"ml/mle-for-production/deploying-ml-models/week1/#running-in-huge-data-centers","title":"Running in Huge Data Centers","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week1/#constrained-environment-mobile-phone","title":"Constrained Environment: Mobile Phone","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week1/#restrictions-in-a-constrained-environement","title":"Restrictions in a Constrained Environement","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week1/#prediction-latency-is-almost-always-important","title":"Prediction Latency is Almost Always Important","text":"<ul> <li>Opt for on-device inference whenever possible<ul> <li>Enhances user experience by reducing the response time of your app</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week1/#improving-prediction-latency-and-reducing-resource-costs","title":"Improving Prediction Latency and Reducing Resource Costs","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week1/#for-mobile-deployed-models","title":"For mobile deployed models","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week1/#web-applications-for-users","title":"Web Applications for Users","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week1/#serving-systems-for-easy-deployment","title":"Serving systems for easy deployment","text":"<ul> <li>Centralized model deployment</li> <li>Predictions as service</li> </ul> <p>Eliminates need for custom web applications by getting the mode deployed in just a few lines of code. They can also make is easy to rollback/update models on the fly.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week1/#clipper","title":"Clipper","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week1/#tensorflow-serving","title":"TensorFlow Serving","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week1/#advantages-of-serving-with-a-managed-service","title":"Advantages of Serving with a Managed Service","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week1/#installing-tensorflow-serving","title":"Installing TensorFlow Serving","text":"<ul> <li> <p>Docker Image:</p> <ul> <li>Easiest and most recommended method</li> </ul> <pre><code>docker pull tensorflow/serving\ndocker pull tensorflow/serving:latest-gpu\n</code></pre> </li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week1/#install-tensorflow-serving","title":"Install TensorFlow Serving","text":"<ul> <li>Building From Source</li> <li>Install using Aptitude (<code>apt-get</code>) on a Debian-based Linux system</li> </ul> <pre><code>!echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stabletensorflow-model-server tensorflow-model-server-universal\" |\ntee /etc/apt/source.list.d/tensorflow-serving.list &amp;&amp; \\\ncurl https://storage/googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -\n\n!apt update\n\n!apt-get install tensorflow-model-server\n</code></pre>"},{"location":"ml/mle-for-production/deploying-ml-models/week1/#import-the-mnist-dataset","title":"Import the MNIST Dataset","text":"<pre><code>mnist = tf.keras.datasets.mnist\n(train_image, train_labels), (test_images, test_labels) = mnist.load_data()\n\n# Scale the values of the arrays below to be between 0.0 and 1.0\ntrain_images = train_images / 255.0\ntest_images = test_images / 255.0\n\n# Reshape the arrays below\ntrain_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\ntest_images = test_images.reshape(test_images.shape[0], 28, 28, 1)\n\nprint(\"\\ntrain_images.shape: {}, of {}\".format(\n  train_images.shape train_images.dtype))\nprint(\"test_images.shape: {}, of {}\".format(\n  test_images.shape test_images.dtype))\n</code></pre>"},{"location":"ml/mle-for-production/deploying-ml-models/week1/#look-at-a-sample-image","title":"Look at a sample Image","text":"<pre><code>idx = 42\n\nplt.imshow(test_images[idx].reshape(28, 28), cmap=plt.cm.binary)\nplt.title(\"True label: {}\".format(test_labels[idx]), fontdict={'size': 16})\nplt.show()\n</code></pre>"},{"location":"ml/mle-for-production/deploying-ml-models/week1/#build-a-model","title":"Build a Model","text":"<pre><code># Create a model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(\n      input_shape=(28, 28, 1),\n      filters=8, kernel_size=3,\n      strides=2, activation='relu',\n      name='Conv1'),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax, name='softmax')\n])\n\nmodel.summary()\n</code></pre>"},{"location":"ml/mle-for-production/deploying-ml-models/week1/#train-the-model","title":"Train the Model","text":"<pre><code># Configure the model for training\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nepochs=5\nhisory = model.fit(train_images, train_labels, epochs=epochs)\n</code></pre>"},{"location":"ml/mle-for-production/deploying-ml-models/week1/#evaluate-the-model","title":"Evaluate the Model","text":"<pre><code># evaluate the model on the test iamges\nresults_eval = model.evaluate(test_images, test_labels, verbose=0)\n\nfor metric, value in zip(model.metrics_names, results_eval):\n        print(metric + ': {:.3}'.format(value))\n</code></pre>"},{"location":"ml/mle-for-production/deploying-ml-models/week1/#save-the-model","title":"Save the Model","text":"<pre><code>MODEL_DIR = tempfile.gettempdir()\nversion = 1\nexport_path = os.path.join(MODEL_DIR, str(version))\n\nif os.path.isdir(export_path):\n    print('\\n Already saved a model, clearning up\\n')\n    !rm -r {export_path}\n\nmodel.save(export_path, save_format='tf')\n\nprint('\\nexport_path = {}'.format(export_path))\n!ls -l {export_path}\n</code></pre>"},{"location":"ml/mle-for-production/deploying-ml-models/week1/#launch-your-saved-model","title":"Launch Your Saved Model","text":"<pre><code>os.environ['MODEL_DIR'] = MODEL_DIR\n\n%%bash --bg\nnohup tensorflow_model_server \\\n    --rest_apt_port=8501 \\\n    --model_name=digits_model \\\n    --model_base_path=\"${MODEL_DIR}\" &gt; server.log 2&gt;&amp;1\n!tail server.log\n</code></pre>"},{"location":"ml/mle-for-production/deploying-ml-models/week1/#sending-an-inference-request","title":"Sending an Inference Request","text":"<pre><code>data = json.dumps({\n  \"signature_name\": \"serving_default\",\n  \"isntances\": test_images[0:3].tolist()})\n\nheaders = {\"content-type\": \"application/json\"}\n\njson_response = requests.post(\n  'https://localhost:8501/v1/models/digits_model:predict',\n  data=data, headers=headers)\n\npredictions = json.loads(json_response.text)['predictions']\n</code></pre>"},{"location":"ml/mle-for-production/deploying-ml-models/week1/#plot-predictions","title":"Plot Predictions","text":"<pre><code>plt.figure(figsize=(10, 15))\n\nfor i in range(3):\n  plt.subplot(1, 3, i+1)\n  plt.imshow(test_iamges[i].reshape(28, 28), cmap=plt.cm.binary)\n  plt.axis('off')\n  color = 'green' if np.argmax(predictions[i]) == test_labels[i]  else 'red'\n  plt.title('Prediction: {}\\n True Label: {}'.format(\n    np.argmax(predictions[i]),\n    test_labels[i]), color=color)\n\nplt.show()\n</code></pre>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/","title":"Week 2 \u2014 Model Serving Architecture, Scaling Infrastructure and More.","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week2/#model-serving-architecture","title":"Model Serving Architecture","text":"<p>On Prem</p> <ul> <li>Train and deploy on your own hardware infrastructure</li> <li>Manually procure hardware GPUs, CPUs etc</li> <li>Profitable for large companies running ML projects for longer times</li> <li>Can use open source, pre-built servers<ul> <li>TF-Serving, KF-Serving, NVidia and more</li> </ul> </li> </ul> <p>On cloud</p> <ul> <li>Train and deploy on cloud choosing from several service providers<ul> <li>Amazon Web Services, Google Cloud Platform, Microsoft Azure</li> </ul> </li> <li>Create VMs and use open source pre-built servers</li> <li>Use the provided ML workflow</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#model-servers","title":"Model Servers","text":"<ul> <li>Simplify the task of deploying machine learning models at scale</li> <li>Can handle scaling, performance, some model lifecycle management etc.,</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#model-servers-tensorflow-serving","title":"Model Servers: TensorFlow Serving","text":"<p>TensorFlow Serving allows:</p> <ul> <li>Batch and Real-Time Inference: batch inference if you have such requirement like in the case of recommendation engine, or real-time for end user facing application</li> <li>Multi-Model serving: allows multiple models for the same task and the server chooses between them, this can be useful for A/B testing, audience segmentation or more.</li> <li>Exposes gRPC (Remote Procedure Call) and REST endpoints</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#tensorflow-serving-architecture","title":"TensorFlow Serving Architecture","text":"<p>A typical <code>servable</code> is a TensorFlow saved model but it could also be something like a lookup up table for an embedding.</p> <p>The <code>Loader</code> manages a servable's lifecycle by standardizing the API for loading and unloading a servable. The loader API enables common infrastructure independent form specific learning algorithms, data or whatever product use cases were involved.</p> <p>A <code>Source</code> creates a loader and notifies <code>DynamicManger</code> of the aspired version.</p> <p>Together this will create Aspired versions representing the set of servable versions that should be loaded and ready.</p> <p>The <code>DynamicManager</code> handles the full life cycle of the servable, including loading the servables, serving the servables and unloading the servables. The manager will listen to the <code>Source</code>s and will track all of the versions according to a <code>VersionPolicy</code>.</p> <p>When the Client request a handle to new version, <code>DynamicManager</code> returns a handle to the new version of the servable.</p> <p>The <code>ServableHandle</code> provides the exterior interface to the client</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#model-servers-other-providers","title":"Model Servers: Other Providers","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week2/#nvidia-triton-inference-server","title":"NVIDIA Triton Inference Server","text":"<ul> <li>Simplifies deployment of AI models at scale in production</li> <li>Open source inference serving software</li> <li>Deploy trained models from any framework<ul> <li>TensorFlow, TensorRT, PyTorch, ONNX Runtime or a custom framework</li> </ul> </li> <li>Models can be stored on:<ul> <li>Local storage, AWS S3, GCP, Any CPU-GPU architecture (cloud, data centre or edge)</li> </ul> </li> <li>HTTP REST or gRPC endpoints are supported</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#torch-serve","title":"Torch Serve","text":"<ul> <li>Model serving framework for PyTorch models</li> <li>Initiative from AWS and Facebook</li> <li>Supports Batch and Real-Time Inference</li> <li>Supports REST Endpoints</li> <li>Default handlers for Image Classification, Object Detection, Image Segmentation, Text Classification.</li> <li>Supports Multi-Model Serving</li> <li>Monitor Detail logs and Customized Metrics</li> <li>A/B Testing</li> </ul> <p>The Frontend is responsible for handling both requests and responses.</p> <p>The Backend uses model workers that are running instances of the model loaded form a model store, responsible for performing the actual inference.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#kfserving","title":"KFServing","text":"Documentation on model servers"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#documentation-on-model-servers","title":"Documentation on model servers","text":"<p>The video lecture covered some of the most popular model servers: TensorFlow Serving, TorchServer, KubeFlow Serving and the NVidia Triton inference server.  Here are the links to relevant documentation for each of these options:</p> <ul> <li>TensorFlow Serving</li> <li>TorchServe</li> <li>KubeFlow Serving</li> <li>NVIDIA Triton</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#scaling-infrastructure","title":"Scaling Infrastructure","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week2/#why-is-scaling-importatnt","title":"Why is Scaling Importatnt?","text":"<ul> <li>Large NN training might take too long on single CPU or GPU, so distributed training and scaling is required.</li> <li>Larger the network more parameters and hyperparameters need to be tuned and fine-tuned.</li> <li>In production settings, huge volume of request to single server instance may overwhelm it.</li> </ul> <p>Two main ways to scale:</p> <ol> <li> <p>Horizontal Scale</p> <p>Adding more powerful hardware to the serving infrastructure, more RAM, more storage, faster CPU &amp; GPU.</p> </li> <li> <p>Vertical Scale</p> <p>Adding more components to the infrastructure.</p> <p>More CPUs, GPUs or instantiating more servers.</p> </li> </ol>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#why-horizontal-over-vertical-scaling","title":"Why Horizontal Over Vertical Scaling?","text":"<ul> <li>Benefit of elasticity: Shrink or grow no of nodes based on load throughput, latency requirements.</li> <li>Application never goes offline: No need for taking existing servers offline for scaling</li> <li>No limit on hardware capacity: Add more nodes any time at increased cost</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#keep-an-eye-out-for-three-thing","title":"Keep an eye out for three thing:","text":"<ol> <li>Can I manually scale?</li> <li> <p>Can I autoscale?</p> <p>How much latency?</p> </li> <li> <p>How aggressive the system at spinning up and down based on my need?</p> <p>How can I manage my additional VMs to ensure that they have the content on them that I want?</p> </li> </ol>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#containerization","title":"Containerization","text":"<p>Advantages:</p> <ul> <li>Less OS requirements - more apps.</li> <li>Abstraction</li> <li>Easy deployment based on container runtime</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#docker","title":"Docker","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week2/#container-orchestration","title":"Container Orchestration","text":"<p>The idea behind container orchestration is to have a set of tools like managing the lifecycle of containers, including their scaling.</p> <p></p> <p>Popular container Orchestration Tools include:</p> <ul> <li>Kubernetes</li> <li>Docker Swarm</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#ml-workflows-on-kubernetes-kubeflow","title":"ML Workflows on Kubernetes - KubeFlow","text":"<ul> <li>Dedicated to making deployments of machine learning (ML) workflows on Kubernetes simple, portable and scalable</li> <li>Anywhere you are running Kubernetes, you should be able to run Kubeflow</li> <li>Can be run on premise or on Kubernetes engine on cloud offerings AWS, GCP, Azure etc.,</li> </ul> Learn about scaling with boy bands Exploring Kubernetes with KubeFlow"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#learn-about-scaling-with-boy-bands","title":"Learn about scaling with boy bands","text":"<p>In the next few minutes you\u2019ll learn about horizontal and vertical scaling. Before going into that, here\u2019s a fun case study on managing scale.</p> <p>In this extreme case a famous boy band called \u2018One Direction\u2019 hosted a 10-hour live stream on YouTube, where they instructed fans to go visit a web site with a quiz on it every 10 minutes. This led to a really interesting pattern in scalability where the application would have zero usage for the vast majority of the time, but then, every 10 minutes may have hundreds of thousands of people hitting it.</p> <p>It\u2019s a complex problem to solve when it comes to scaling. It could be very expensive to operate. Using smart scaling strategies, Sony Music and Google solved this problem very inexpensively. Laurence isn\u2019t allowed to share how much it cost for the cloud services, but, when he and several of the other engineers went out for celebration drinks after the success of the project, the bar bill was more expensive than the cloud bill. (And they didn\u2019t drink a lot!)</p> <p>Check out the talk about how scaling worked for this system here: https://www.youtube.com/watch?v=aIxNm5Eed_8</p> <p>Learn about the event and the app here: https://www.computerweekly.com/news/2240228060/Sony-Music-Google-cloud-One-Directions-1D-Day-event-platform-services</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#explore-kubernetes-and-kubeflow","title":"Explore Kubernetes and KubeFlow","text":"<p>In the videos we explored Kubernetes and KubeFlow, and before going further, I strongly recommend that you have a play with them to see how they work.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#kubernetes","title":"Kubernetes","text":"<p>First is Kubernetes. The site is https://kubernetes.io/, and at the top of the page, there\u2019s a big friendly button that says \u2018Learn Kubernetes Basics\u2019:</p> <p>Click on this, and you\u2019ll be taken to: https://kubernetes.io/docs/tutorials/kubernetes-basics/</p> <p>From here you can go through a lesson to create a cluster, deploy and app, scale it, update it and more. It\u2019s interactive, fun, and worth a couple of hours of your time to really get into how Kubernetes works.</p> <p>You may also want to check this video tutorial out.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#kubeflow","title":"KubeFlow","text":"<p>For KubeFlow, visit: https://www.kubeflow.org/, and at the top of the page, there\u2019s a Get Started button.</p> <p>Click on it to go to the tutorials. It doesn\u2019t have the nice interactive tutorials that Kubernetes has, but, if you can, try to at least install KubeFlow on one of the deployment options listed on this page \u2013 even if it\u2019s just your development machine.\u00a0If you find it tricky to follow, don't worry because you will have an ungraded lab next week that will walk you through installing Kubeflow Pipelines (one of the Kubeflow components) in Kubernetes. In the meantime, you can watch this playlist particularly video #5 on Kubeflow Pipelines to get a short intro to this toolkit.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#onine-inference","title":"Onine Inference","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week2/#optimising-ml-inference","title":"Optimising ML Inference","text":"<ol> <li> <p>Latency</p> <p>Latency is the total delay as experienced by the user. The model might be working fine having least possible latency, but the problem may be arise with the UI, or carrier of data. So latency need to seen as a whole metric for the system.</p> </li> <li> <p>Throughput</p> <p>The throughput measured in request managed per unit time is often more important for non-customer facing systems. While it might still be very important metric in case of user facing system.</p> </li> <li> <p>Cost</p> <p>Both latency and throughput along with many other has to taken into account to be balanced out with limited cost.</p> </li> </ol>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#inference-optimization-where-to-optimize","title":"Inference Optimization \u2014 Where to optimize?","text":"<p>Infrastructure</p> <p>Scale with additional or more powerful hardware as well as containerized virtualized environments.</p> <p>Model Architecture</p> <p>Understand the model architecture and metrics that goes behind to training and testing.</p> <p>Often there's a trade off between inference speed and accuracy.</p> <p>Model Compilation</p> <p>If we know the hardware on which we're going to deploy the model, for example, a particular type of GPU there's often a post training step that consists of creating a model artifact and a model execution runtime that's finally adapted to the underlying support hardware.</p> <p>We can refine model graph and inference runtime to reduce memory consumption and latency</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#additional-optimizations","title":"Additional Optimizations","text":"<p>Additional optimizations can be done for infrastructure like introducing a cache, for faster lookup for relevant information</p> <p></p>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#data-preprocessing","title":"Data Preprocessing","text":"<p>Often the data given to the system is not actually in the format the model supports, the data need to be pre-processed. Question is where this preprocessing is supposed to be done?</p> <p></p>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#and-post-processing","title":"And post-processing","text":"<p>Offcourse model isn't going to output that might be interpretable by a human, and some kind of post processing might needs to done in order to completed the whole process.</p> Data Pre-processing"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#data-preprocessing_1","title":"Data preprocessing","text":"<p>Apache Beam is a product that gives you a unified programming model that lets you implement batch and streaming data processing jobs on any execution engine. It\u2019s ideally suited for data preprocessing!</p> <p>Go to https://beam.apache.org/get-started/try-apache-beam/ to try Apache Beam in a Colab so you can get a handle on how the APIs work. Make sure you try it in Python as well as Java by using the tabs at the top.</p> <p>Note: You can click the Run in Colab button below the code snippet to launch Colab. In the Colab menu bar, click Runtime &gt; Change Runtime type then select Python 3 before running the code cells. You can get more explanations on the WordCount example here and you can use the Beam Programming Guide as well to look up any of the concepts.</p> <p>You can learn about TensorFlow Transform here: https://www.tensorflow.org/tfx/transform/get_started . It also uses Beam style pipelines but has modules optimized for preprocessing Tensorflow datasets.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#batch-inference-scenarios","title":"Batch Inference Scenarios","text":"<p>Prediction based on batch inference is when your ML model is used in a batch scoring job</p> <p>for a large number of data points, where predictions are not required or not feasible to generate in real-time</p> <p></p> <p>These batch jobs for prediction are usually on some recurring schedule like daily, weekly or monthly. Predictions are then stored in a database which can be made available to developers or end-users.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#advantages-of-batch-inference","title":"Advantages of Batch Inference","text":"<ul> <li>Complex machine learning models for improved accuracy</li> <li>Caching not required</li> <li>This leads to lower cost</li> <li>Long data retrieval does not become a problem</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#disadvantages-of-batch-inference","title":"Disadvantages of Batch Inference","text":"<ul> <li>Long Update latency</li> <li>Predictions based on older data. Possible solutions:<ul> <li>In that case recommendations can come from same age bracket</li> <li>Or recommendations can come from same geolocation</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#important-metrics-to-optimize","title":"Important Metrics to Optimize","text":"<p>Most important metric to optimize while performing batch predictions: Throughput</p> <ul> <li>Prediction service should be able to handle large volumes of inference at a time.</li> <li>Predictions need not be available immediately</li> <li>Latency can be compromised</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#how-to-increase-throughput","title":"How to Increase Throughput?","text":"<ul> <li>Use hardware accelerators like GPU's, TPU's.</li> <li>Increase number of servers/workers<ul> <li>Load several instances of model on multiple workers to increase throughput</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#batch-processing-with-etl","title":"Batch Processing with ETL","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week2/#data-processing-batch-and-streaming","title":"Data Processing \u2014 Batch and Streaming","text":"<ul> <li>Data can be of different types based on the source</li> <li>Batch Data<ul> <li>Data processing can be done on data available in huge volumes in data lakes, from csv files, log files, etc.,</li> </ul> </li> <li>Streaming Data<ul> <li>Real-time streaming data, like data from sensors</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#etl-on-data-extract-transform-and-load","title":"ETL on Data (Extract, Transform and Load)","text":"<p>Before data is used for making batch predictions:</p> <ul> <li>It has to be extracted from multiple sources like log files, streaming sources, APIs, apps, etc.,</li> <li>Transformed</li> <li>Loaded into a database for prediction</li> </ul> <p>This is done using ETL Pipelines</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#etl-pipelines","title":"ETL Pipelines","text":"<p>Set of processes for</p> <ul> <li>extracting data from data sources</li> <li>Transforming data</li> <li>Loading into an output destination like data warehourse</li> </ul> <p>From there data can be consumed for training or making predictions using ML models.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week2/#distributed-processing","title":"Distributed Processing","text":"Image by DeepLearning.ai <ul> <li>ETL can be performed on huge volumes of data in distributed manner</li> <li>Data is split into chunks and parallely processed by multiple workers</li> <li>The results of the ETL workflow are stored in a database</li> <li>Results in lower latency and higher throughput of data processing</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/","title":"Week 3 \u2014 ML Experiment Management and Workflow Automation &amp; MLOps Methodology","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week3/#ml-experiments-management-and-workflow-automation","title":"ML Experiments Management and Workflow Automation","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week3/#experiment-tracking","title":"Experiment Tracking","text":"<p>The need for rigorous processes and reproducible results, creates a need for experiment tracking.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#why-experiment-tracking","title":"Why experiment tracking?","text":"<ul> <li>ML projects have far more branching and experimentation</li> <li>Debugging in ML is difficult and time consuming</li> <li>Small changes can lead to drastic changes in a model's performance and resource requirements.</li> <li>Running experiments can be time consuming and expensive</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#what-does-it-mean-to-track-experiments","title":"What does it mean to track experiments?","text":"<ul> <li>Enable you to duplicate a result</li> <li>Enable you to meaningfully compare experiments</li> <li>Mange code/data versions, hyperparameters, environment, metrics</li> <li>Organize them in a meaningful way</li> <li>Make them available to access and collaborate on within your organization</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#simple-experiments-with-notebooks","title":"Simple Experiments with Notebooks","text":"<ul> <li>Notebooks are great tools</li> <li>Notebook code is usually not promotes to production</li> <li>Tools for managing notebook code<ul> <li>nbconvert (<code>.ipynb</code> \u2192 <code>.py</code> conversion)</li> <li>nbdime (diffing)</li> <li>jupytext (conversion+versioning)</li> <li>neptune-notebooks (versioning+diffing+sharing)</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#smoke-testing-for-notebooks","title":"Smoke testing for Notebooks","text":"<pre><code>jupyter nbconvert --to script train_model.ipynb python train_model.py;\npython train_model.py\n</code></pre>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#not-just-one-big-file","title":"Not Just One Big File","text":"<ul> <li>Modular code, not monolithic</li> <li>Collections of independent and versioned files</li> <li>Directory hierarchies or monorepos</li> <li>Code repositories and commits</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#tracking-runtime-parameters","title":"Tracking Runtime Parameters","text":"<p>Config files</p> <p>The parameters value can be tracked along with other code files</p> <pre><code>data:\n    train_path: '/path/to/my/train.csv'\n    valid_path: 'path/to/my/vaild.csv'\n\nmodel:\n    objective: 'binary'\n    metric: 'auc'\n    learning_rate: 0.1\n    num_boost_round: 200\n    num_leaves: 60\n    feature_fraction: 0.2\n</code></pre> <p>or in Command line</p> <p>But this requires additional code to be save these values and associate them with the experiment. This is an additional burden but it also makes those values available for analysis and visualization rather than having to parse them out of specific commit.</p> <pre><code>python train_evaluate.py \\\n    --train_path '/path/to/my/train.csv' \\\n    --valid_path 'path/to/my/vaild.csv' \\\n    --objective 'binary' \\\n    --metric 'auc' \\\n    --learning_rate 0.1 \\\n    --num_boost_round 200 \\\n    --num_leaves 60\n    --feature_fraction 0.2\n</code></pre>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#log-runtime-parameters","title":"Log Runtime Parameters","text":"<p>Example of what the code to save the runtime parameters where we're setting runtime parameters from command line.</p> <pre><code>parser = argparse.ArgumentParser()\nparser.add_argumnet('--numeber_tress')\nparser.add_argument('--learning_rate')\nargs = parser.parse_args()\n\nneptune.create_experiment(params=vars(args))\n...\n# experiment logic\n...\n</code></pre>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#tools-for-experiment-tracking","title":"Tools for Experiment Tracking","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week3/#data-versioning","title":"Data Versioning","text":"<ul> <li>Data reflects the world, and the world changes</li> <li>Experimental changes include changes in data</li> <li>Tracking, understanding, comparing, and duplicating experiments includes data</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#tools-for-data-versioning","title":"Tools for Data Versioning","text":"<ul> <li>Neptune</li> <li>Pachyderm</li> <li>Delta Lake</li> <li> <p>Git LFS</p> </li> <li> <p>Dolt</p> </li> <li>lakeFS</li> <li>DVC</li> <li>ML-Metadata</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#experiment-tracking-to-compare-results","title":"Experiment tracking to compare results","text":"<ul> <li>As you gain experience with the tools, you'll get more confortable</li> <li>Log every metric that you might care about</li> <li>Tag experiments with a few consistent tags which are meaningful to you</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#example-logging-metrics-using-tensorboard","title":"Example: Logging metrics using TensorBoard","text":"<pre><code>logdir = \"logs/image\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\ntensorboard_callback = keras.callbacks.TensorBoard(\n  log_dir=logdir, historgram_freq=1)\ncm_callback = keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)\n\nmodel.fit(..., callbacks=[tensorboard_callback, cm_callback])\n</code></pre>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#organizing-model-development","title":"Organizing model development","text":"<ul> <li>Search through &amp; visualize all experiments</li> <li>Organize into something digestible</li> <li>Make data shareable and accessible</li> <li>Tag and add notes that will be more meaningful</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#tooling-for-teams","title":"Tooling for Teams","text":"<p>Vertex TensorBoard</p> <ul> <li>Managed service with enterprise-grade security, privacy, and compliance</li> <li>Persistent, shareable link to you experiment dashboard</li> <li>Searchable list of all experiments in a project</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#experiments-are-iterative-in-nature","title":"Experiments are iterative in nature","text":"<ul> <li>Creative iterations for ML experimentation</li> <li>Define a baseline approach</li> <li>Develop, implement, and evaluate to get metrics</li> <li>Asses the results, and decide on next steps</li> <li>Latency, cost, fairness, GDPR, etc.</li> <li>Experiment Tracking</li> </ul> Experiment"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#experiment-tracking_1","title":"Experiment Tracking","text":"<p>Learn more about experiment tracking by checking this two resources out:</p> <ol> <li>Machine Learning Experiment Tracking</li> <li>Machine Learning Experiment Management: How to Organize Your Model Development Process</li> </ol>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#introduction-to-mlops","title":"Introduction to MLOps","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week3/#data-scientists-vs-software-engineers","title":"Data Scientists vs Software Engineers","text":"<p>Data Scientists</p> <ul> <li>Often work on fixed datasets</li> <li>Focused on model metrics</li> <li>Prototyping on Jupyter notebooks</li> <li>Expert in modeling techniques and feature engineering</li> <li>Model size, cost, latency, and fairness are often ignored</li> </ul> <p>Software Engineers</p> <ul> <li>Build a product</li> <li>Concerned about cost, performance, stability, schedule</li> <li>Identify quality through customer satisfaction</li> <li>Must scale solution, handle large amounts of data</li> <li>Detect and handle error conditions, preferably automatically</li> <li>Consider requirements for security, safety, fairness</li> <li>Maintain, evolve, and extend the product over long periods</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#growing-need-for-ml-in-products-and-services","title":"Growing Need for ML in products and Services","text":"<ul> <li>Large datasets</li> <li>Inexpensive on-demand compute resources</li> <li>Increasingly powerful accelerators for ML</li> <li>Rapid advances in many ML research fields (such as computer vision, natural language understanding, and recommendation systems)</li> <li>Business are investing in their data science teams and ML capabilities to develop predictive models that can deliver business value to their customers</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#key-problems-affecting-ml-efforts-today","title":"Key problems affecting ML efforts today","text":"<p>We've been here before</p> <ul> <li>In the 90s, Software Engineering was siloed</li> <li>Weak version control CI/CD didn't exist</li> <li>Software was slow to ship; now it ships in minutes</li> <li>Is that ML today?</li> </ul> <p>Today's perspective</p> <ul> <li>Models blocked before deployment</li> <li>Slow to market</li> <li>Manual Tracking</li> <li>No reproducibility or provenance</li> <li>Inefficient collaboration</li> <li>unmonitored models</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#bridging-ml-and-it-with-mlops","title":"Bridging ML and IT with MLOps","text":"<ul> <li>Continuous Integration (CI): Testing and validating code, components, data, data schemas, and models</li> <li>Continuous Delivery (CD): Not only about deploying a single software package or a service, but a system which automatically deploys another service (model prediction service)</li> <li>Continuous Training (CT): A new process, unique to ML systems, that automatically retrains candidate models for testing and serving</li> <li>Continuous Monitoring (CM): Catching errors in production systems, and monitoring production inference data and models performance metrics tied to business outcomes.</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#ml-solution-lifecycle","title":"ML Solution Lifecycle","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week3/#standardizing-ml-processes-with-mlops","title":"Standardizing ML processes with MLOps","text":"<ul> <li>ML Lifecycle Management</li> <li>Model Versioning &amp; iteration</li> <li>Model Monitoring and Management</li> <li>Model Governance</li> <li>Model Security</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#mlops-methodology","title":"MLOps Methodology","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week3/#mlops-level-0","title":"MLOps Level 0","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week3/#what-defines-an-mlops-process-maturity","title":"What defines an MLOps process' maturity?","text":"<ul> <li>The level of automation of ML pipelines determines the maturity of the MLOps process</li> <li>As maturity increases, the available velocity for the training and deployment of new models also increases</li> <li>Goal is to automate training and deployment of ML models into the core software system, and provide monitoring.</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#mlops-level-0-manual-process","title":"MLOps level 0: Manual process","text":"<p>The process of developing and deploying the model is manual. This creates a disconnect between the ML and operations teams. It also leads to the possibility of training serving skew.</p> <p></p> <p>A new model version is probably only deployed a couple of times a year, so because of fewer code changes Continuous Integration (CI) and often even unit testing is totally ignored.</p> <p>A level 0 process is concerned only with deploying the trained model as a prediction service. Also we do not tracking and logging the model predictions and actions which are required for detecting model degradation and other model behavioral drifts</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#challenges-for-mlops-level-0","title":"Challenges for MLOps level 0","text":"<ul> <li>Need for actively monitoring the quality of your model in production</li> <li>Retraining your production models with new data</li> <li>Continuously experimenting with new implementation to improve the data and model</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#mlops-levels-12","title":"MLOps Levels 1&amp;2","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week3/#mlops-level-1-ml-pipeline-automation","title":"MLOps level 1: ML pipeline automation","text":"<p>One of the main goal of level one is to perform continuous training of the model.</p> <p>That requires the need to introduce automated data and model validation steps to the pipeline as well as pipeline triggers and metadata management.</p> <p></p> <ul> <li>Notice how the transition from one step to another in the experiment orchestration is automated.</li> <li>Models are automatically retrained using fresh data based on live pipeline triggers.</li> <li>The pipeline implementation that is used in the development or experimentation is also used in the pre-production and production environment.</li> <li>The components need to be modularized, ideally be containerized.</li> <li>An ML pipeline in production continuously delivers new models that are trained on new data to prediction services</li> <li>Compared to level 0 where we where just deploying the model, here we are deploying the whole training pipeline, which automatically and recurrently runs to serve the trained model.</li> </ul> <p></p> <ul> <li>When we deploy the pipeline to production, one or more of the triggers automatically executes the pipeline.</li> <li>The pipeline expects a new live data to produce a new model version that is trained on the new data. So automated data validation &amp; model validation steps are required in ML pipelines.</li> <li> <p>Whether you should retrain the model, or stop the execution of the pipeline:</p> <p>This decision is automatically made only if the data is deemed valid. Like data schema mismatch are considered anomalies, in that case the pipeline execution should be stopped and notification should be raised for the team to investigate.</p> </li> <li> <p>Model validation and evaluation of the model is done before promoting the model to production.</p> <p>The newly trained model needs to be assessed on a test data and then comparing the evaluation metric produced by newly trained model with current model in production.</p> <p>Also the performance needs to be consistent on different slices of data.</p> </li> <li> <p>In addition to offline model validation, a newly deployed model undergoes online model validation in either a canary deployment or an AB testing setup during the transition to serving prediction for the online traffic.</p> </li> <li>Feature Store: A feature store is a centralized repository where you standardize the definition, storage, and access of features for training and serving.<ul> <li>A feature store also lets you rediscover and reuse available feature sets instead of recreating the same or similar feature sets, avoiding having similar features that have different definition by maintaining features and their related metadata.</li> </ul> </li> <li>Metadata store: This is where information about each execution of the pipeline is recorded in order to help with data and artifact lineage, reproducibility, and comparisons. This can help us debug errors and anomalies. In case of an interruption, it also allows you to resume execution seamlessly.</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#mlops-level-2-cicd-pipeline-automation","title":"MLOps level 2: CI/CD pipeline automation","text":"<p>The truth is that at the current stage of the development of MLOps best practices, level two is still somewhat speculative</p> <p>The diagram presents on of the current architectures, focused on enabling rapid and reliable update of the pipelines in production.</p> <p></p> MLOps Resources"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#mlops-resources","title":"MLOps Resources","text":"<p>If you want to learn more about MLOps check this blog out, and visit this curated list of references  for more information, ideas, and tools.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#developing-components-for-an-orchestrated-workflow","title":"Developing Components for an Orchestrated Workflow","text":"<ul> <li>Pre-built and standard components, and 3 styles of custom components</li> <li>Components can also be containerized</li> <li>Examples of thing you can do with TFX components:<ul> <li>Data augmentation, upsampling, or downsampling</li> <li>Anomaly detection based on confidence intervals or autoencoder reproduction error</li> <li>Interfacing with external systems like help desks for alerting and monitoring and more...</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#anatomy-of-a-tfx-component","title":"Anatomy of a TFX component","text":"<p>Components are essentially composed of a component specification and executor class packaged inside a component class.</p> <p>Component specification</p> <ul> <li>The component's input and output contract and parameters used for component execution.</li> </ul> <p>Executor class</p> <ul> <li>Provides the implementation for component's processing</li> </ul> <p>Component Class</p> <ul> <li>Combines the component specification with the executor to create a TFX component</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#tfx-components-at-runtime","title":"TFX components at runtime","text":"<p>When a pipeline runs a TFX component the component is executed in three phases:</p> <ol> <li>The driver uses the component specification to retrieve the required artifacts form the metadata store and pass them into the component.</li> <li>The executor perform the components work</li> <li>The publisher uses the component specification and the results from the executor to store the components, output in the metadata store.</li> </ol> <p></p> <p>Custom components only requires modification to executor class. Modification to the driver or publisher should only be necessary if we want to change the interaction between pipeline's components and the metadata store</p> <p>If we want to change the inputs, outputs or parameters for your component, we only need to modify the component specification.</p> <p>Types of custom components:</p> <ol> <li> <p>Python function based custom components</p> <p>Only require Python function for the executor with a decorator and argument annotations.</p> </li> <li> <p>Container based custom components</p> <p>Provide the flexibility to integrate code written in any language into your pipeline by wrapping the components inside a Docker container.  </p> </li> <li> <p>Fully custom components</p> <p>Fully custom components lets us build components by defining the component specification, executor and component interface classes.</p> </li> </ol>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#python-function-based-components","title":"Python function-based components","text":"<p>In this style you write a function that is decorated and annotated with type hints.</p> <p>The type hints describe the <code>InputArtifacts</code> , <code>OutputArtifacts</code>, parameters of your component.</p> <pre><code>@component\ndef MuValidationComponent(\n  model: InputArtifact[Model],\n  blessing: OutputArtifact[Model],\n  accuracy_threshold: Parameter[int] = 10,\n  ) -&gt; OutputDict(accuracy=float):\n  '''My simple custom model validation component.'''\n\n  accuracy = evaluate_model(model)\n  if accuracy &gt;= accuracy_threshold:\n    write_output_blessing(blessiing)\n\n  return {\n    'accuracy': accuracy\n  }\n</code></pre>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#container-based-components","title":"Container-based components","text":"<p>To create one we need to specify a Docker container image that includes our component dependencies.</p> <pre><code>from tfx.dsl.component.experimental import container_component, placeholders\nfrom tfx.types import standard_artifacts\n\ngrep_component = container_component.create_container_component(\n  name='FilterWithGrep',\n  inputs={'text': standard_artifacts.ExternalArtifact},\n  outputs={'filtered_text': standard_artifacts.ExternalArtifact},\n  parameters={'pattern': str},\n  ...\n  image='google/cloud-sdk:278.0.0',\n  command=[\n    'sh', '-exec',\n    ...\n    ...\n    '--pattern', placeholders.InputValuePlaceholder('pattern'),\n    '--text', placeholders.InputUriPlaceholder('text'),\n    '--filtered-text',\n    placeholders.OutputUriPlaceholder('filtered_text'),\n  ],\n)\n</code></pre> <p>Their are other parts of the configuration like container image name and optionally the image tag. For the body of the component we can have command parameter which defines the container entry point command line. The command line can use placeholder objects that are replaced at compilation time with the input, output or parameters.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#fully-custom-components","title":"Fully custom components","text":"<ul> <li>Define custom component spec, executor class, and component class</li> <li>Component reusability<ul> <li>Reuse a component spec and implement a new executor that derives from an existing component</li> </ul> </li> </ul> <p>Defining input and output specifications</p> <p>These inputs and outputs are wrapped in channels, essentially dictionaries of typed parameters for input and output artifacts.</p> <p>The <code>PARAMETERS</code> is a dictionary of additional execution parameter items that are passed into the executor and are not metadata artifacts.</p> <pre><code>class HelloComponentSepc(types.ComponentSpec):\n  INPUTS = {\n    # This will be a dictionary with input artifacts, including URIs\n    'input_data': ChannelParameter(type=standard_artifacts.Examples),\n  }\n  OUTPUTS = {\n    # This will be a dictionary which this component will populate\n    'output_data': ChannelParameter(type=standard_artifacts.Examples),\n  }\n  PARAMETERS = {\n    # These are parameters that will be passed in the call to create\n    # an instance of this component\n    'name': ExecutionParameter(type=Text),\n  }\n</code></pre> <p>Implement the executor</p> <pre><code>class Executor(base_executor.BaseExecutor):\n  def Do(self, input_dict: Dict[Text, List[type.Artifact]],\n          output_dict: Dict[Text, List[types.Artifact]],\n          exec_properties: Dict[Text, Any]) -&gt; None:\n    ...\n    split_to_instances = {}\n    for artifact in input_dict['input_data']:\n      for split in json.loads(artifacts.split_names):\n        uri = os.path.join(artifact.uri, split)\n        split_to_instance[split] = uri\n    for split, instance in split_to_instance.items():\n      input_dir = instance\n      output_dir = artifact_utils.get_split_uri(\n                      output_dict['output_data'], split)\n    for filename in tf.io.gfile.listdir(input_dir):\n      input_uri = os.path.join(input_dir, filename)\n      output_uri = os.path.join(output_dir, filename)\n      io_utils.copy_file(src=input_uri, dst=output_uri, overwrite=True)\n</code></pre> <p>Make the component pipeline-compatible</p> <pre><code>from tfx.types import standard_artifacts\nfrom hello_component import executor\n\nclass HelloComponent(base_component.BaseComponent):\n    SPEC_CLASS = HelloComponentSpec\n    EXECUTOR_SPEC = ExecutorClassSpec(executor.Executor)\n\n    def __init__(self,\n      input_data: types.Channel = None,\n      output_data: types.Channel = None,\n      name: Optional[Text] = None):\n      if not output_data:\n        examples_artifact = standard_artifacts.Examples()\n        examples_artifact.split_names = input_data.get()[0].split_names\n        output_data = channel_utils.as_channel([examples_artifact])\n\n      spec = HelloComponentSpec(input_data=input_data, output_data, name=name)\n      super(HelloComponent, self).__init__(spec=spec)\n</code></pre> <p>Assemble into a TFX pipeline</p> <pre><code>def _create_pipeline():\n  ...\n  example_gen = CsvExampleGen(input_base=examples)\n\n  hello = component.HelloComponent(\n    input_data=example_Gen.outputs['examples'],\n    name='HelloWorld')\n\n  statistics_gen = StatisticsGen(\n    examples=hello.outputs['output_data'])\n  ...\n  return pipeline.Pipeline(\n    ...\n    components=[example_gen, hello, statistics_gen, ...],\n    ...\n  )\n</code></pre> Architecture for MLOps using TFX, Kubeflow Pipelines, and Cloud Build"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build","title":"Architecture for MLOps using TFX, Kubeflow Pipelines, and Cloud Build","text":"<p>To learn more about MLOps using TFX please check this document out.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#model-management-and-deployment-infrastructure","title":"Model Management and Deployment Infrastructure","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week3/#managing-model-versions","title":"Managing Model Versions","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week3/#why-versioning-ml-models","title":"Why versioning ML Models?","text":"<p>In normal software development, teams and individual rely on version control software to help teams manage and control changes to their code. This helps them stay in sync with each other, rollback if new changes cause havoc and assess their development.</p> <p>Similarly in ML model development, model versioning helps teams keep track of changes to code, data and configs to properly reproduce the results and do collaboration.</p> <p></p>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#how-ml-models-are-versioned","title":"How ML Models are versioned?","text":"<p>How software is versioned</p> <p>Version: <code>MAJOR.MINOR.PATCH</code></p> <ul> <li><code>MAJOR</code>: Contains incompatible API changes</li> <li><code>MINOR</code>: Adds functionality in a backwards compatible manner</li> <li><code>PATCH</code>: Makes backwards compatible bug fixes.</li> </ul> <p>ML models versioning</p> <ul> <li>No uniform standard accepted yet</li> <li>Different organizations have different meanings and conventions</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#a-model-versioning-proposal","title":"A Model Versioning Proposal","text":"<p>Version: <code>MAJOR.MINOR.PIPLEINE</code></p> <ul> <li><code>Major</code>: Incompatibility in data or target variable</li> <li><code>MINOR</code>: Model performance in improved</li> <li><code>PIPELINE</code>: Pipeline of model training is changed</li> </ul> <p>TFX uses pipeline execution versioning. In this style, a new version is defined with each successfully run training pipeline. Models will be versioned regardless of changes to model architecture, input, or output.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#retrieving-older-models","title":"Retrieving older models","text":"<ul> <li>Can ML framework be leveraged to retrieve previously trained models?</li> <li>ML framework may internally be versioning models</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#what-is-model-lineage","title":"What is model lineage?","text":"<p>Model lineage is a set of relationships among the artifacts that resulted in the trained model.</p> <p>Artifacts</p> <p>Artifacts are information needed to preprocess data and generate result (code, data, config, model)</p> <p>To build model artifacts, you have to be able to track the code that build them and the data including pre-processing operations that the model was trained and tested upon.</p> <p>ML orchestration frameworks (like TFX) may store operations and data artifacts to recreate model. Model lineage usually only includes those artifacts and operations that were part of model training. Post-training artifacts and operations are usually not part of the lineage.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#what-is-model-registry","title":"What is model registry?","text":"<p>A model registry is a central repository for storing trained models.</p> <ul> <li>Provides various operations of ML model development lifecycle</li> <li>Promotes model discovery, model understanding, and model reuse</li> <li>Integrated into OSS and commercial ML platforms</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#metadata-stored-by-model-registry","title":"Metadata stored by model registry","text":"<p>Metadata usually includes:</p> <ul> <li>Model versions</li> <li>Model serialized artifacts</li> <li>Free text annotations and structured properties.</li> <li>Links to other ML artifact and metadata stores</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#capabilities-enabled-by-model-registries","title":"Capabilities Enabled by Model Registries","text":"<ul> <li>Model search/discovery and understanding</li> <li>Approval/Governance</li> <li>Collaboration/Discussion</li> <li>Streamlined deployments</li> <li>Continuous evaluation and monitoring</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#examples-of-model-registries","title":"Examples of Model Registries","text":"<ul> <li>Azure ML Model registry</li> <li>SAS model manager</li> <li>MLflow Model Registry</li> <li>Google AI platform</li> <li>Algorithmia</li> <li> <p>ML Model Management</p> </li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#ml-model-management","title":"ML Model Management","text":"<p>Take a deeper dive into managing ML model versions by checking this blog out.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#continuous-delivery","title":"Continuous Delivery","text":"<p>Continuous Delivery helps promotes robust deployment.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#what-is-continuous-integration-ci","title":"What is Continuous Integration (CI)","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week3/#what-is-continuous-delivery-cd","title":"What is Continuous Delivery (CD)","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week3/#cicd-infrastructure","title":"CI/CD Infrastructure","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week3/#unit-testing-in-ci","title":"Unit Testing in CI","text":"<p>In unit test we test each component in the pipeline produces the expected artifacts</p> <p>In addition to unit testing our code, following the standard practices of software development, there are two additional types of unit tests when doing CI for ML:</p> <ol> <li>Unit testing Data</li> <li>Unit testing Model performance</li> </ol>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#unit-testing-input-data","title":"Unit Testing Input Data","text":"<p>Unit testing of data is not the same as performing data validation on your raw features.</p> <p></p>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#unit-testing-model-performance","title":"Unit Testing Model performance","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week3/#ml-unit-testing-considerations","title":"ML Unit Testing Considerations","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week3/#infrastructure-validation","title":"Infrastructure validation","text":"<p>Infrastructure validation acts as an early warning layer before pushing a model into production to avoid issues with models that might nor run or might perform badly when actually serving requests in production.</p> <p>When to apply infrastructure validation</p> <ul> <li>Before starting CI/CD as part of model training</li> <li>Can also occur as part of CI/CD as a last check to check to verify that the model is deployable to the serving infrastructure.</li> </ul> <p>TFX InfraValidator</p> <ul> <li>TFX InfraValidator takes the model, launches a sand-boxed model server with the model and sees if it can be successfully loaded and optionally queried</li> <li>InfraValidator is using the same model server binary, same resources, and same server configuration as production.</li> <li>InfraValidator only interacts with the model server in the user configured environment to see of it works as expected. Configuring this environment correctly will ensure that your inferred validation passing or failing will be indicative of whether the model would be survivable in the production serving environment.</li> </ul> Continuous Delivery"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#continuous-delivery_1","title":"Continuous Delivery","text":"<p>Explore this website to learn more about continuous delivery.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#progressive-delivery","title":"Progressive Delivery","text":"<p>Progressive Delivery is essentially an improvement over Continuous Delivery. </p>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#complex-model-deployment-scenarios","title":"Complex Model Deployment Scenarios","text":"<p>Progressive delivery usually involves having multiple versions deployed at the same time so that comparisons in performance can be made</p> <ul> <li>You can deploy multiple models performing same task</li> <li>Deploying competing models, as in A/B testing</li> <li>Deploying as shadow models, as in Canary testing</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#bluegreen-deployment","title":"Blue/Green deployment","text":"<p>Traffic is passing through the load balancer directing it to current live environment called Blue. Meanwhile a new version is deployed to the green environment which acts as a staging setup where a series of tests are conducted to ensure performance and functionality.</p> <p>After passing the tests the traffic is then directed to the green deployment. If problem arises traffic can be moved back to the Blue version.</p> <p></p> <p></p> <ul> <li>No Downtime</li> <li>Quick rollback &amp; reliable</li> <li>Smoke testing in production environment</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#canary-deployment","title":"Canary deployment","text":"<p>Similar to blue/green deployment, but instead of switching the entire incoming traffic from blue to green all at once, traffic is switched gradually.</p> <p>As traffic begins consuming new version, the performance of the new version is monitored. If necessary the deployment can be stopped and reversed with no downtime and minimal exposure of user to new version.</p> <p>Eventually all traffic is transferred to the new version.  </p>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#live-experimentation","title":"Live Experimentation","text":"<ul> <li>Model metrics are usually not exact matches for business objectives</li> <li>Examples: Recommender systems<ul> <li>Model trained on clicks</li> <li>Business wants to maximise profit</li> <li>Example: Different products have different profit margins</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#live-experimentaion-ab-testing","title":"Live Experimentaion:; A/B Testing","text":"<p>In A/B testing we have at least two different models (or n) and we compare the business results between them to select the model that gives the best business performance.</p> <ul> <li>Users are divided into two groups</li> <li>Users are randomly routed to different models in environment</li> <li>You gather business results from each model to see which one is performing better</li> </ul> <p> </p> Progressive Delivery"},{"location":"ml/mle-for-production/deploying-ml-models/week3/#progressive-delivery_1","title":"Progressive Delivery","text":"<p>Explore more about progressive delivery with Kubernetes operators allowing for minimum downtime and easy rollbacks in this documentation.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/","title":"Week 4 \u2014 Model monitoring, Logging &amp; model decay, GDPR, and Privacy","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week4/#model-monitoring-and-logging","title":"Model Monitoring and Logging","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week4/#why-monitoring-matters","title":"Why Monitoring Matters","text":"<p>Monitoring not only include monitoring of our ML models, but also the monitoring of the systems and infrastructure which are included in our entire product or service such as Databases and web servers</p> <p></p> <p>\"An ounce of prevention is worth a pound of cure\" \u2014 Benjamin Franklin</p> <ul> <li>Immediate Data Skews<ul> <li>Training data is too old, not representative of live data</li> </ul> </li> <li>Model Staleness<ul> <li>Environment shifts</li> <li>Consumer behaviour</li> <li>Adversarial scenarios</li> </ul> </li> <li>Negative Feedback Loops: when you train your models on data collected in production. If that data is biased/corrupted in any way, then the model trained on that data will also perform poorlly.</li> </ul> <p></p>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#why-is-ml-monitoring-different","title":"Why is ML monitoring different?","text":"<p>Unlike a pure software system, there are two additional components to consider in an ML system, the data and the model. Unlike in traditional software systems, the accuracy of an ML system depends on how well the model reflects the world it is meant to model which in turn depends on the data used for training and on the data that it receives while serving requests.</p> <p>Code and config also take on additional complexity and sensitivity in an ML system due to two aspects:</p> <ol> <li>Entanglement: Refers to the issue where changing anything, changes everything</li> <li>Configuration: Model hyperparameters, versions and features are often controlled in a system config and the slightest error here can cause radically different model behavior that won't be picked by traditional software tests.</li> </ol>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#observability-in-ml","title":"Observability in ML","text":"<p>Observability measures how well the internal states of a system can be inferred by knowing the inputs and outputs.</p> <p>For ML, that means monitoring and analyzing the prediction requests and the generated predictions from your models.</p> <p>Observability comes from control system theory where observability and controllability are closely linked.</p> <p>i.e., controlling the accuracy of results overall usually across different versions of the model, requires observability.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#complexity-of-observing-modern-systems","title":"Complexity of observing modern systems","text":"<ul> <li>Modern systems can make observability difficult<ul> <li>Cloud-based systems</li> <li>Containerized infrastructure</li> <li>Distributed systems</li> <li>Microservices</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#deep-observability-for-ml","title":"Deep observability for ML","text":"<ul> <li>Not only top-level metrics<ul> <li>Data slices provide a way to analyze different groups of people or different type of conditions</li> </ul> </li> <li>Domain knowledge is important for observability</li> <li>TensorFlow Model Analysis (TFMA)</li> <li>Both supervised and unsupervised analysis</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#goals-of-ml-observability","title":"Goals of ML observability","text":"<p>The main goal here in the context of observability is to prevent or act upon system failures.</p> <p>Observations need to provide alert when a failure happens and ideally provide recommended actions to bring the system back to normal behavior.</p> <ul> <li>Alertable<ul> <li>Metrics and thresholds designed to make failures obvious</li> </ul> </li> <li>Actionable<ul> <li>Root cause clearly identified</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#monitoring-targets-in-ml","title":"Monitoring Targets in ML","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week4/#basics-input-and-output-monitoring","title":"Basics: Input and output monitoring","text":"<ul> <li> <p>Model input distribution</p> <p>Measure high level statistics on slices of data relevant to domain</p> </li> <li> <p>Model prediction distribution</p> </li> <li>Model versions</li> <li>Input/prediction correlation</li> </ul> <p> </p> <p>Collecting all the context information is often not practical, as the amount of data to process and store could be very large, so it's important the most relevant context and try to gather that information.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#logging-for-ml-monitoring","title":"Logging for ML Monitoring","text":"<p>A log is an immutable, time stamped record of discrete events that happened over time for the system along with additional information.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#steps-for-building-observability","title":"Steps for building observability","text":"<ul> <li>Start with the out-of-the-box logs, metrics and dashboards</li> <li>Add agents to collect additional logs and metrics</li> <li>Add logs-based metrics and alerting to create your own metrics and alerts</li> <li>Use aggregated sinks and workspaces to centralize your logs and monitoring</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#tools-for-building-observability","title":"Tools for building observability","text":"<ul> <li>Google Cloud Monitoring</li> <li>Amazon CloudWatch</li> <li>Azure Monitor</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#logging-advantages","title":"Logging - Advantages","text":"<ul> <li>Easy to generate</li> <li>Great when it comes to providing valuable insight</li> <li>Focus on specific events</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#logging-disadvantages","title":"Logging - Disadvantages","text":"<ul> <li>Excessive logging can impact system performance</li> <li>Aggregation operations on logs can be expensive (i.e., treat logs-based alerts with caution)</li> <li>Setting up &amp; maintaining tooling carries with it a significant operational cost</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#logging-in-machine-learning","title":"Logging in Machine Learning","text":"<p>Key areas: Use logs to keep track of the model inputs and predictions</p> <p>Input red flags:</p> <ul> <li>A feature becoming unavailable</li> <li>Notable shifts in the distribution</li> <li>Patterns specific to your model</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#storing-log-data-for-analysis","title":"Storing log data for analysis","text":"<ul> <li>Basic log storage is often unstructured</li> <li>Parsing and storing log data in a queryable format enables analysis<ul> <li>Extracting values to generate distributions and statistics</li> <li>Associating events with timestamps</li> <li>Identifying the systems</li> </ul> </li> <li>Enables automated reporting, dashboards, and alerting</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#new-training-data","title":"New Training Data","text":"<ul> <li>Prediction requests form new training datasets</li> <li>For supervised learning, labels are required<ul> <li>Direct labeling</li> <li>Manual labeling</li> <li>Active learning</li> <li>Weak supervision</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#tracing-for-ml-systems","title":"Tracing for ML systems","text":"<p>Tracing focuses on monitoring and understanding system performance, especially for microservice-based applications.</p> <p>In monolithic systems, it's relatively easy to collect diagnostic data form different parts of a system. All modules might even run within one process and share common resources for logging.</p> <p></p> <p>Solving this problem becomes even more difficult if your services are running as separate processes in a distributed system. We can't depend on the traditional approaches that help diagnose monolithic systems. We somehow need to know the fine grain information of what's going on inside each service.</p> <p></p>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#tools-for-building-observability_1","title":"Tools for building observability","text":"<ul> <li>Sequence and parallelism of service requests</li> <li>Distributed tracing<ul> <li>Dapper</li> <li>Zipkin</li> <li>Jaeger</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#dapper-style-tracing","title":"Dapper-Style Tracing","text":"<p>In service based architectures, Dapper style tracing works by propagating tracing data between services.</p> <p>Each service annotate the trace with additional data and passes the tracing header to other services until the final request completes</p> <p>Services are responsible for uploading their traces to a tracing back-end. The tracing backend then puts related latency data together like pieces of a puzzle.</p> <p>Each trace is a call tree, beginning with the entry point of a request and ending with the server's response including all of the RPCs along the way. Each trace consists of small units called spans.</p> <p></p> Monitoring Machine Learning Models in Production"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#monitoring-machine-learning-models-in-production","title":"Monitoring Machine Learning Models in Production","text":"<p>Check this resource out to learn more about ML monitoring and logging.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#model-decay","title":"Model Decay","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week4/#what-is-model-decay","title":"What is Model Decay?","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week4/#model-decay_1","title":"Model Decay","text":"<ul> <li>Production ML models often operate in a dynamic environments</li> <li>The ground truth in dynamic environments changes</li> <li>If the model is static and does not change, then it gradually moves farther and farther away from the ground truth</li> </ul> <p>Two main causes of model drift: <ul> <li>Data Drift</li> <li>Concept Drift</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#data-drift-aka-feature-drift","title":"Data Drift (aka Feature Drift)","text":"<ul> <li>Statistical properties of input changes</li> <li>Trained model is not relevant for changed data</li> <li>For e.g., distribution of demographic data like age might change over time.</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#concept-drift","title":"Concept Drift","text":"<ul> <li>Relationship between features and labels changes</li> <li>The very meaning of what you are trying to predict changes</li> <li>Prediction drift and label drift are similar</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#detecting-drift-on-time","title":"Detecting Drift on Time","text":"<ul> <li>Drift creeps into the system slowly with time</li> <li>If it goes undetected, model accuracy suffers</li> <li>Important to monitor and detect drift early</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#model-decay-detection","title":"Model Decay Detection","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week4/#detecting-concept-and-data-drift","title":"Detecting Concept and Data Drift","text":"<p>Log Predictions (Full Requests and Reponses)</p> <ul> <li>Incoming prediction requests and generated prediction should be logged</li> <li>If possible log the ground truth that should have been predicted<ul> <li>Can be used as labels for new training data</li> </ul> </li> <li>At a minimum log data in prediction request<ul> <li>This data can be analyzed using unsupervised statistical methods to detect data drift that will cause model decay</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#detecting-drift","title":"Detecting Drift","text":"<ul> <li>Detected by observing the statistical properties of logged data, model predictions, and possible ground truth.</li> <li>Deploy dashboard that plot statistical properties to observe how they change over time</li> <li>Use specialized libraries for detecting drift<ul> <li>TensorFlow Data Validation (TFDV)</li> <li>Scikit-multiflow library</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#continuous-evaluation-and-labelling-in-vertex-prediction","title":"Continuous Evaluation and Labelling in Vertex Prediction","text":"<ul> <li>Vertex Prediction offers continuous evaluation</li> <li>Vertex Labelling Service can be used to assign ground truth labels to prediction input for retraining.</li> <li>Azure, AWS, and other cloud providers provide similar services.</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#ways-to-mitigate-model-decay","title":"Ways to Mitigate Model Decay","text":"<p>When you've detected model decay:</p> <ul> <li>At the minimum operational and business stakeholders should be notified of the decay</li> <li>Take steps to bring model back to acceptable performance</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#steps-in-mitigating-model-decay","title":"Steps in Mitigating Model Decay","text":"<ul> <li>What if Drift is Detected?<ul> <li>If possible, determine the portion of your training set that is still correct using unsupervised methods, such as clustering or statistical methods that look at divergence, KL divergence, JS divergence, KS test.</li> <li>Keep the good data, discard the bad, and add new data OR</li> <li>Discard data collected before a certain date and add new data OR</li> <li>Create an entirely new training set from new data</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#fine-tune-or-start-over","title":"Fine Tune, or Start Over?","text":"<ul> <li>you can either continue training your model, fine tuning from the last checkpoint using new data OR</li> <li>Start over, reinitialize your model, and completely retrain it</li> <li>Either approach is valid, so it really depends on results<ul> <li>How much new labelled data do you have?</li> <li>How far has it drifted?</li> </ul> </li> <li>Ideally Try both and compare the results</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#model-re-training-policy","title":"Model Re-Training Policy","text":"<p>It's usually a good idea to establish policies around when you're going to retrain your model, well it depends.</p> <p></p>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#automated-model-retraining","title":"Automated Model Retraining","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week4/#redesign-data-processing-steps-and-model-architecture","title":"Redesign Data Processing Steps and Model Architecture","text":"<ul> <li>When model performance decays beyond an acceptable threshold you might have to consider redesigning your entire pipeline</li> <li>Re-think feature engineering, feature-selection</li> <li>You may have to train your model from scrath</li> <li>Investigate on alternative architectures</li> <li>Addressing Model Decay</li> </ul> Addressing Model Decay"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#addressing-model-decay","title":"Addressing Model Decay","text":"<p>Check this blog out to figure out best retraining strategies toi prevent model decay.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#gdpr-and-privacy","title":"GDPR and Privacy","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week4/#responsible-ai","title":"Responsible AI","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week4/#responsible-ai-practices","title":"Responsible AI Practices","text":"<ul> <li>Development of AI Creates new opportunities to improve the lives of people around the world<ul> <li>Business, healthcare, education, etc</li> </ul> </li> <li>But it also Raises new questions about implementing responsible practices<ul> <li>Fairness, interpretability, privacy, and security</li> <li>Far from solved, active areas of research and development</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#human-centered-desing","title":"Human-Centered Desing","text":"<p>Actual users's experience is essential</p> <ul> <li>Design your features with appropriate disclosures built-in</li> <li>Consider augmentation and assistance<ul> <li>Offering multiple suggestions instead of one right answer</li> </ul> </li> <li>Model potential adverse feedback early in the design process</li> <li>Engage with a diverse set of users and use-case scenarios</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#identify-multiple-metrics","title":"Identify Multiple Metrics","text":"<ul> <li>Using several metrics help you understand the tradeoffs<ul> <li>Feedback from user suverys</li> <li>Quantiles that track overall system performance</li> <li>False positive and false negative sliced across subgroups</li> </ul> </li> <li>Metrics must be appropriate for the context and goals of your system</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#analyze-your-raw-data-carefully","title":"Analyze your raw data carefully","text":"<ul> <li>For sensitive raw data, respect privacy<ul> <li>Compute aggregate, annonymized summaries</li> </ul> </li> <li>Does your data reflect your users?<ul> <li>e.g., will be used for all ages, but all data from senior citizens</li> </ul> </li> <li>Imperfect proxy labels?<ul> <li>Relationships between the labels and actual targets</li> </ul> </li> <li>Responsible AI</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#responsible-ai_1","title":"Responsible AI","text":"<p>New technologies always bring new challenges. Ensuring that your applications adhere to responsible AI is a must. Please read this resource to keep yourself updated with this fascinating active research  subject.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#legal-requirements-for-secure-and-private-ai","title":"Legal Requirements for Secure and Private AI","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week4/#legal-implications-of-data-security-and-privacy","title":"Legal Implications of Data Security and Privacy","text":"<p>Companies must comply with data privacy protection laws in regions where they operate</p> <p>In Europe for example, you need to comply with GDPR, General Data Protection Regulation.</p> <p>In California, with CCPA, California Consumer Privacy Act</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#general-data-protection-regulation-gdpr","title":"General Data Protection Regulation (GDPR)","text":"<ul> <li>Regulation in EU law on data protection and privacy in the European Union (EU) and the European Economic Area (EEA)</li> <li>Give control to individuals over their data</li> <li>Companies should protect the data of employees</li> <li>When the data processing is based on consent, the data subject has the right to revoke their consent at any time.</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#california-consumer-privacy-act-ccpa","title":"California Consumer Privacy Act (CCPA)","text":"<ul> <li>Similar to GDPR</li> <li>Intended to enhance privacy rights and consumer protection for residents of California</li> <li>User has the right to know what personal data is being collected about them, whether the personal data is sold or disclosed, and to whom</li> <li>User can access the personal data, block the sale of their data, and request a business to delete their data.</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#security-and-privacy-harms-from-ml-models","title":"Security and Privacy Harms from ML Models","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week4/#defenses","title":"Defenses","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week4/#cryptography","title":"Cryptography","text":"<ul> <li>Privacy-enhancing tools (like SMPC and FHE) should be considered to securely train supervised machine learning models</li> <li>Users can send encrypted prediction requests while preserving the confidentiality of the model</li> <li>Protects confidentiality of the training data</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#differential-privacy","title":"Differential Privacy","text":"<p>Roughly speaking, a model is differentially private if an attacker seeing its predictions cannot tell if a particular user's information was included in the training data.</p> <p>System for publicly sharing information about a dataset by describing the patterns of groups within the dataset while withholding information about individuals in the dataset.</p> <p>There are three different approaches to implement differential privacy:</p> <ol> <li>DP-SGD (Differentially Private Stochastic Gradient Descent) <li>PATE (Private Aggregation of Teacher Ensembles)</li> <li>CaPC (Confidential and Private Collaborative learning)</li>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#differentially-private-stochastic-gradient-descent-dp-sgd","title":"Differentially-Private Stochastic Gradient Descent (DP-SGD)","text":"<p>If an attacker is able to get a copy of a normally trained model, then they can use the weights to extract private information.</p> <p>DP-SGD eliminates that possibility by applying differential privacy during model training.</p> <ul> <li>Modifies the minibatch stochastic optimization process by adding noise.</li> <li>Trained model retains differential privacy because of the post processing immunity property of differential privacy.<ul> <li>Post-processing immunity is a fundamental property of a differential privacy.</li> <li>It means that regardless how you process the models predictions, you can't affect their privacy guarantees.</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#private-aggregation-of-teacher-ensembles-pate","title":"Private Aggregation of Teacher Ensembles (PATE)","text":"<ul> <li>Begins by dividing sensitive information into k partitions with no overlaps. It trains k models on that data separately as teacher models and then aggregate the result in an aggregate teacher model.</li> <li>During the aggregation for the aggregate teacher, we will add noise to the output in a way that won't affect the resulting predictions.</li> <li>For deployment, we will create a student model. To train the student model, we'll take unlabeled public data and feed it to aggregate teacher model, outputting a labeled data, which maintains privacy. This data is then used as the training set for the student model.</li> <li> <p>Discard everything on left side of diagram and deploy student model.</p> </li> <li> <p></p> </li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#confidential-and-private-collaborative-learning-capc","title":"Confidential and Private Collaborative Learning (CaPC)","text":"<ul> <li>Enables models to collaborate while preserving the privacy of the underlying data</li> <li>Integrates building blocks from cryptography and differential privacy to provide confidential and private collaborative learning.</li> <li>Encrypts prediction requests using Homomorphic Encryption (HE)</li> <li>Uses PATE to add noise to predictions for voting</li> </ul> GDPR and CCPA"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#gdpr-and-ccpa","title":"GDPR and CCPA","text":"<p>Check the GDPR  and CCPA websites  out to learn more about its regulations and compliance.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#anonymization-and-pseudonymisation","title":"Anonymization and Pseudonymisation","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week4/#data-anonymization-in-gdpr","title":"Data Anonymization in GDPR","text":"<ul> <li>GDPR includes many regulations to preserve privacy of user data</li> <li>Since introduction of GDPR, two terms have been discussed widely<ol> <li>Anonymization</li> <li>Pseudonymisation</li> </ol> </li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#data-anonymization","title":"Data Anonymization","text":"<p>Removes Personally Identifiable Information (PII) form data sets.</p> <p>Recital 26 of GDPR defines Data Anonymization</p> <p>True data anonymization is :</p> <ul> <li>Irreversible</li> <li>Done in such a way that it is impossible to identify the person</li> <li>Impossible to derive insights or discrete information, even by the party responsible for a anonymization</li> </ul> <p>GDPR does not apply to data that has been anonymized</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#pseudonymisation","title":"Pseudonymisation","text":"<ul> <li> <p>GDPR Article 4(5) defines pseudonymisation as:</p> <p>\"...the processing of personal data in such a way that the data can no longer be attributed to a specific data subject without the use of additional information\"</p> </li> <li> <p>The data is anonymized by switching the identifiers (like email or name) with an alias or pseudonym.</p> </li> <li> <p></p> </li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#spectrum-of-privary-prevention","title":"Spectrum of Privary prevention","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week4/#what-data-should-be-anonymized","title":"What data should be Anonymized?","text":"<ul> <li>Any data that reveals the identify of a person, referred to as identifies</li> <li>Identifiers applies to any natural or legal person, living or dead, including their dependents, ascendants, and descendants.</li> <li>Included are other related persons, direct or through interaction</li> <li>For example: Family names, patronyms, first names, maiden names, aliases, address, phone, bank account details, credit cards, IDs like SSN.</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#right-to-be-forgotten","title":"Right to be Forgotten","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week4/#what-is-right-to-be-forgotten","title":"What is Right to Be Forgotten?","text":"<p>\"The data subject shall have the right to obtain from the controller the erasure of personal data concerning him or her without undue delay and the controller shall have the obligation to erase personal data without undue delay\"</p> <ul> <li>Recitals 65 and 66 and in Article 17 of the GDPR</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#right-to-rectification","title":"Right to Rectification","text":"<p>\"The data subject shall have the right to obtain from the controller without undue delay the rectification of inaccurate personal data concerning him or her. Taking into account the purposes of the processing, the data subject shall have the right to have incomplete personal data completed, including by means of providing a supplementary statement.\"</p> <ul> <li>Chapter 3, Art. 16 GDPR</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#other-rights-of-the-data-subject","title":"Other Rights of the Data Subject","text":"<p>Chapter 3 defines a number of other rights of the data subject, including:</p> <ul> <li>Art. 15 GDPR - Right of access by the data subject</li> <li>Art. 18 GDPR - Right to restriction of processing</li> <li>Art. 20 GDPR - Right to data portability</li> <li>Art. 21 GDPR - Right to object</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#implementing-right-to-be-forgotten-tracking-data","title":"Implementing Right To Be Forgotten: Tracking Data","text":"<p>For a valid erasure claim</p> <ul> <li>Company need to identify all of the information related to the content requested to be removed.</li> <li>All of the associated metadata must also be erased<ul> <li>e.g., Derived data, logs etc.</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#forgetting-digital-memories","title":"Forgetting Digital Memories","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week4/#issues-with-hard-delete","title":"Issues with Hard Delete","text":"<ul> <li>Deleting records from a database can cause havoc</li> <li>User data is often referenced in multiple tables</li> <li>Deletion breaks the connections, which can be difficult in large, complex databases</li> <li>Can break foreign keys</li> <li>Anonymization keeps the records, and only anonymizes the fields containing PII.</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#challenges-in-implementing-right-to-be-forgotten","title":"Challenges in Implementing Right to Be Forgotten","text":"<ul> <li>Identifying if data privacy is violated</li> <li>Organisational changes for enforcing GDPR</li> <li>Deleting personal data from multiple back-ups</li> </ul> Course 4 Optional References"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#course-4-optional-references","title":"Course 4 Optional References","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week4/#machine-learning-modeling-pipelines-in-production","title":"Machine Learning Modeling Pipelines in Production","text":"<p>This is a compilation of resources including URLs and papers appearing in lecture videos. If you wish to dive more deeply into the topics covered this week, feel free to check out these optional references.</p>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#week-1-model-serving-introduction","title":"Week 1. Model Serving: introduction","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week4/#nosql-databases","title":"NoSQL Databases:","text":"<ul> <li>Google Cloud Memorystore</li> <li>Google Cloud Firestore</li> <li>Google Cloud Bigtable</li> <li>Amazon DynamoDB</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#mobilenets","title":"MobileNets:","text":"<ul> <li>MobileNets</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#serving-systems","title":"Serving Systems:","text":"<ul> <li>Clipper</li> <li>TensorFlow Serving</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#week-2-model-serving-patterns-and-infrastructure","title":"Week 2. Model Serving: patterns and infrastructure","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week4/#model-serving-architecture","title":"Model Serving Architecture:","text":"<ul> <li>Model Server Architecture</li> <li>TensorFlow Serving</li> <li>NVIDIA Triton Inference Server</li> <li>Torch Serve</li> <li>Kubeflow Serving</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#scaling-infrastructure","title":"Scaling Infrastructure:","text":"<ul> <li>Container Orchestration</li> <li>Kubernetes</li> <li>Docker Swarm</li> <li>Kubeflow</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#online-inference","title":"Online Inference:","text":"<ul> <li>Batch vs. Online Inference</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#batch-processing-with-etl","title":"Batch Processing with ETL:","text":"<ul> <li>Kafka ML</li> <li>Pub Sub</li> <li>Cloud DataFlow</li> <li>Apache Spark</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#week-3-model-management-and-delivery","title":"Week 3. Model Management and Delivery","text":""},{"location":"ml/mle-for-production/deploying-ml-models/week4/#experiment-tracking-and-management","title":"Experiment Tracking and Management:","text":"<ul> <li>Tracking</li> <li>Management</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#notebooks","title":"Notebooks:","text":"<ul> <li>nbconvert</li> <li>nbdime</li> <li>jupytext</li> <li>neptune-notebooks</li> <li>git</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#tools-for-data-versioning","title":"Tools for Data Versioning:","text":"<ul> <li>Neptune</li> <li>Pachyderm</li> <li>Delta Lake</li> <li>Git LFS</li> <li>DoIt</li> <li>lakeFS</li> <li>DVC</li> <li>ML-Metadata</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#tooling-for-teams","title":"Tooling for Teams:","text":"<ul> <li>Image Summaries</li> <li>neptune-ai</li> <li>Vertex TensorBoard</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#mlops","title":"MLOps:","text":"<ul> <li>MLOps: Continuous delivery and automation pipelines in machine learning</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#orchestrated-workflows-with-tfx","title":"Orchestrated Workflows with TFX:","text":"<ul> <li>Creating a Custom TFX Component</li> <li>Building Fully Custom Components</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#continuous-and-progressive-delivery","title":"Continuous and Progressive Delivery:","text":"<ul> <li>Progressive Delivery</li> <li>Continuous, Incremental, &amp; Progressive Delivery</li> <li>Deployment Strategies</li> <li>Blue/Green Deployment</li> <li>A/B Testing</li> </ul>"},{"location":"ml/mle-for-production/deploying-ml-models/week4/#week-4-model-monitoring-and-logging","title":"Week 4. Model Monitoring and Logging","text":"<ul> <li>Hidden Technical Debt in Machine Learning Systems</li> <li>Monitoring Machine Learning Models in Production</li> <li>Google Cloud Monitoring</li> <li>Amazon CloudWatch</li> <li>Azure Monitor</li> <li>Dapper</li> <li>Jaeger</li> <li>Zipkin</li> <li>Vertex Prediction</li> <li>Vertex Labelling Service</li> <li>How \u201cAnonymous\u201d is Anonymized Data?</li> <li>Pseudonymization</li> </ul>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/","title":"Intro to ML in Production","text":"<p>Getting started with Machine Learing Engineering for Production</p>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week1/","title":"Week 1 \u2014 The ML project lifecycle","text":"<p>Ahhaha...You got your fine tuned model trained in Jupyter notebook...now what's next?</p> <p>Get it delivered to customers...ASAP. right? maybe...</p> <p>There are some hurdles before deploying the model in Production which needs to be taken care of like data drift, model updation, and deploying model to edge devices comes with it own set of challenges. It isn't all software engineering, it's more than that and that's what we are going are cover in this specialization on MLOps \u2014 Machine Learning Operations.</p> <p>The whole specialization is divided into 4 courses:</p> <ol> <li> <p>Introduction to Machine Learning in Production</p> <p>Taught by Andrew Ng</p> <p>The overview of entire life cycle of Machine learning Project \u2014 from scoping and getting data to modeling and deployment.</p> </li> <li> <p>Machine Learning Data Lifecycle in Production</p> <p>Taught by Robert Crowe</p> <p>This will be about the Data Pipelines.</p> </li> <li> <p>Machine Learning Modeling Pipelines in Production</p> <p>After data gathering comes modeling, we will learn managing modeling resources to best serve inference requests and minimize cost of training and also apply to analytics to better understand model.</p> </li> <li> <p>Deploying Machine Learning Models in Production</p> <p>At last comes the production, where we are ready to serve customers. This will be about how to built deployment pipelines and variety of infrastructure to better serve the needs and keep the production environment up in top condition.</p> </li> </ol> <p>  Image by Daniel Bourke | Source</p> <p>ML code is just a very small component in a machine learning project. The other non-ML code makes up for what is said to be a useful product.</p>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week1/#the-ml-project-lifecycle","title":"The ML Project Lifecycle","text":"<p>  Image by Steven Layett and Daniel Pipryata from landing.ai</p> <p>The first stage is Scoping:</p> <ul> <li>Decide the scope of the project, who is going to use it  by thinking from user's perspective</li> <li>Decide on key metrics</li> <li>Estimate resources and timeline</li> </ul> <p>The second stage is to do with Data</p> <p>Ask questions like:</p> <ul> <li>Is the data labeled consistently?</li> <li>Is the data balanced?</li> </ul> <p>The third stage is Modeling which is an iterative process which often leads us to question data and learn what might be wrong with data.</p> <p>The fourth and final stage is Deployment, either in cloud accessible via APIs or directly on edge devices. This also involves monitoring and maintaining the system to check for visible signs of degrading performance or shift in distribution of data (data drift).</p> <p>MLOps or Machine Learning Operations is an emerging discipline, and comprises a set of tools and principles to support progress through the ML project lifecycle. It involves four stages Scoping, Data gathering and analysing, Modeling and Deployment.</p>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week1/#deployment-key-challenges","title":"Deployment Key Challenges","text":"<p>The issues can be broadly categorised into 2:</p>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week1/#machine-learningstatistical-issues","title":"Machine Learning/statistical issues","text":"<ul> <li> <p>Concept Drift: The mapping from $x\\rightarrow y$ changes</p> <p>Concept drift is a phenomenon where the statistical properties of the target variable ($y$ - which the model is trying to predict), change over time.</p> <p>e.g., Say you are building the house price prediction system then concept drift will be said to  occur when the size of the house remains same but price increases.</p> </li> <li> <p>Data Drift: The distribution of the data ($x$) changes</p> <p>For same example data drift may occur when people start building larger/smaller houses.</p> </li> </ul>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week1/#software-engineering-issues","title":"Software engineering issues","text":"<ul> <li>Realtime or Batch</li> <li>Cloud or Edge</li> <li>Compute resources (CPU/GPU/memory)</li> <li>Latency, throughput (QPS) (Queries Per Second)</li> <li>Logging</li> <li>Security and privacy</li> </ul>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week1/#deployment-patterns","title":"Deployment patterns","text":"<p>Common deployment cases:</p> <ol> <li>New product/capability</li> <li>Automate/assist with manual task</li> <li>Replace previous ML system</li> </ol> <p>Key ideas:</p> <ul> <li>Rolling release/upgrade</li> <li>Rollback</li> </ul> <p>Shadow mode deployment is where the ML system shadows the human and runs in parallel but not used actively for making any kind of decisions. The only purpose of such kind of deployment is to gather more data and insight before full deployemnt</p> <p>Canary deployment is gradual deployment where the system is only allowed to handle small fraction of traffic while monitoring and gradually ramping up traffic.</p> <p>Blue green deployment is a method of installing changes to service by swapping alternating production (older service) and staging server (newer service). This swapping can be either complete or partial.</p>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week1/#degrees-of-automation","title":"Degrees of Automation","text":"<p> Image by DeepLearning.AI</p>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week1/#monitoring","title":"Monitoring","text":"<ul> <li>Brainstorm things that could go wrong</li> <li>Brainstorm a few statistics/metrics that will detect the problem</li> <li>It's ok to use many metrics initially and gradually remove the ones you find not useful</li> </ul> <p>Software metrics - Memory, compute, latency, throughput, server load</p> <p>Input metrics - Average input length, average input volume, number of missing values etc</p> <p>Output metrics - Times return \" \" (null), times user redoes search, times user switches to typing</p> <p></p> <p>By monitoring different metrics we can set thresholds for alarms/notification.</p> <p>This also can prompts to update/retrain our models either manually or automatically.</p>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week1/#pipeline-monitoring","title":"Pipeline Monitoring","text":"<p>There can also be components to our system that are used before the data even gets as input. We might need to monitor those components as well.</p> <p>We may also ask, How quickly do the data change?</p> <ul> <li>User data generally has slower drift</li> <li>Enterprise data (B2B applications) can shift faster</li> </ul> Week 1 references"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week1/#week-1-overview-of-the-ml-lifecycle-and-deployment","title":"Week 1: Overview of the ML Lifecycle and Deployment","text":"<p>If you wish to dive more deeply into the topics covered this week, feel free to check out these optional references. You won\u2019t have to read these to complete this week\u2019s practice quizzes.</p> <p>Concept and Data Drift</p> <p>Monitoring ML Models</p> <p>A Chat with Andrew on MLOps: From Model-centric to Data-centric</p> <p>Papers</p> <p>Konstantinos, Katsiapis, Karmarkar, A., Altay, A., Zaks, A., Polyzotis, N., \u2026 Li, Z. (2020). Towards ML Engineering: A brief history of TensorFlow Extended (TFX). http://arxiv.org/abs/2010.02013</p> <p>Paleyes, A., Urma, R.-G., &amp; Lawrence, N. D. (2020). Challenges in deploying machine learning: A survey of case studies. http://arxiv.org/abs/2011.09926</p> <p>Sculley, D., Holt, G., Golovin, D., Davydov, E., &amp; Phillips, T. (n.d.). Hidden technical debt in machine learning systems. Retrieved April 28, 2021, from Nips.c https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf</p>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week2/","title":"Week 2 \u2014 Key challenges in Modelling","text":"<ol> <li>Doing well on training set (usually measured by average training error)</li> <li>Doing well on dev/test set(s).</li> <li>Doing well on business metrics/project goals</li> </ol>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week2/#maybe-low-average-error-isnt-good-enough","title":"Maybe Low average error isn't good enough","text":"<p>Performance on disproportionately important examples might be more important.</p> <ul> <li>Less relevant \"How to __\" or informational and transactional queries might be forgiven by user whereas Navigational queries where the user wants to visit particular platform might not be forgiven by user.</li> <li>Model fairness is also an important aspect</li> <li>Think about recall vs precision</li> </ul> <p>Skewed datasets with Rare classes can easily achieve low average error but in reality the model will be very na\u00efve.</p>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week2/#establish-a-baseline","title":"Establish a baseline","text":"<p>We can use Human Level Performance (HLP) as point of comparison or a na\u00efve model as a baseline for improvement in next iteration. This gives a sense of what is irreducible error/Bayes error.</p> <ul> <li>Human Level Performance is often very useful for unstructured data as humans are very good at understanding unstructured data..</li> </ul> <p>Ways to establish a baseline:</p> <ul> <li>Human level performance (HLP)</li> <li>Literature search for state-of-the-art (SOTA)/open source</li> <li>Quick-and-Dirty implementation</li> <li>Performance of older system</li> </ul>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week2/#tips-for-getting-started","title":"Tips for getting started","text":"<ul> <li>Literature search to see what's possible (courses, blogs, open-source projects)</li> <li>Find open-source implementations if availaible</li> <li>A reasonable algorithm with good data will often outperform a great algorithm with not so good data.</li> </ul> <p>Sanity-Check by trying to overfit a small training dataset before training on a larger set.</p>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week2/#error-analysis","title":"Error Analysis","text":"<p>Perform error analysis on the instances misclassified by trained model by iteratively finding tags related to the instances. Like for voice detection misclassified instances, find which has people's noise or low bandwidth.</p> <p>Useful metrics for each tag:</p> <ul> <li>What fraction of errors has that tag?</li> <li>Of all data with that tag, what fraction is misclassified?</li> <li>What fraction of all the data has that tag?</li> <li>How much room for improvement is there on data with that tag?</li> </ul>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week2/#prioritizing-what-to-work-on","title":"Prioritizing what to work on","text":"<p>Only using \"Gap to HLP\" might not be good idea to work upon. Instead by measuring % of data with that tag and then estimating the decrease in error rate might give a better route for what to work upon.</p> <p>Decide on most important categories to work on based on:</p> <ul> <li>How much room for improvement there is (like the above example)</li> <li>How frequently that category appears</li> <li>How easy is to improve accuracy in that category</li> <li>How important it is to improve in that category</li> </ul> <p>What you can do to make improvement in that class?</p> <ul> <li>Collect more that data</li> <li>Use data augmentation to get more data</li> <li>Improve label accuracy/data quality</li> </ul>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week2/#skewed-datasets","title":"Skewed datasets","text":"<p>A skewed dataset is referred to dataset having a class imbalance problem. (few classes are outnumbered by other)</p> <p>Instead of thinking just accuracy, use Confusion matrix and precision and recall.</p> <p></p> <p>$$ \\text{Recall} = \\frac{TP}{TP+FN} $$</p> <p>$$ \\text{Precision} = \\frac{TP}{TP+FP} $$</p> <ul> <li>Recall oriented tasks involve where consequences of False negatives are high like, Tumor detection.</li> <li>Precision oriented tasks involve where consequences of False Positive are high, like Search engine ranking, document classification.</li> </ul> <p>If both are important then F1 scores can be helpful:</p> <p>$$ F_1 = \\frac{2.\\text{Precision}.\\text{Recall}}{\\text{Precision + Recall}} $$</p>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week2/#performance-auditing","title":"Performance auditing","text":""},{"location":"ml/mle-for-production/intro-to-mle-for-production/week2/#auditing-framework","title":"Auditing Framework","text":"<p>Check for accuracy, fairness/bias, and other problems.</p> <ol> <li>Brainstorm the ways the system might go wrong<ul> <li>Performance on subsets of data (e.g., ethnicity, gender)</li> <li>How common are certain errors (e.g., FP, FN)</li> <li>Performance on rare classes</li> </ul> </li> <li>Establish metrics to assess performance against these issues on appropriate slices of data</li> <li>Get business/product owner buy-in</li> </ol> <p></p>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week2/#data-centric-vs-model-centric-ai-development","title":"Data-centric vs Model-centric AI Development","text":"<p>Model-centric: Hold the data fixed and iteratively improve the code/model.</p> <p>Data-centric: Hold the code fixed and iteratively improve the data,</p>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week2/#data-centric-ai-development-using-data-augmentation","title":"Data-centric AI development using Data-augmentation","text":"<p>Data-augmentation can be really efficient way to generate more data as well as new maybe unseen variety of data especially for unstructured data.</p> <p>Goal: Create realistic examples that</p> <ol> <li>The algorithm does poorly on, but</li> <li>humans (or other baseline) do well on</li> </ol> <p>Checklist:</p> <ul> <li>  Does it sound realistic?</li> <li>  Is the $x \\rightarrow y$ mapping clear? (e.g., can humans recognize speech?)</li> <li>  Is the algorithm currently doing poorly on it?</li> </ul> <p></p>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week2/#can-adding-data-hurt-performance","title":"Can adding data hurt performance?","text":"<p>For unstructured data problems, if:</p> <ul> <li>The model is large (low bias)</li> <li>The mapping $x \\rightarrow  y$ is clear (e.g., given only the input $x$, humans can make accurate predictions).</li> </ul> <p>Then, adding data rarely hurts accuracy.</p> <p></p>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week2/#adding-features-if-data-augmentation-is-not-possible","title":"Adding Features if data-augmentation is not possible","text":""},{"location":"ml/mle-for-production/intro-to-mle-for-production/week2/#experiment-tracking","title":"Experiment Tracking","text":"<p>What to track?</p> <ul> <li>Algorithm/code versioning</li> <li>Dataset used</li> <li>Hyperparameters</li> <li>Results</li> </ul> <p>Tracking tools:</p> <ul> <li>Text files</li> <li>Spreadsheet</li> <li> <p>Experiment tracking system</p> </li> <li> <p>Information needed to replicate results</p> </li> <li>Experiment results, ideally with summary metrics/analysis</li> <li>Perhaps also: Resource monitoring, visualization, mode error analysis</li> </ul>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week2/#from-big-data-to-good-data","title":"From Big data to Good Data","text":"<p>Try to ensure consistently high-quality data in all phases of the ML project lifecycle</p> <p>Good data:</p> <ul> <li>Covers important cases (good coverage of inputs $x$)</li> <li>Is defined consistently (definition of labels $y$ is unambiguous)</li> <li>Has timely feedback from production data (distribution covers data data drift and concept drift)</li> <li>Is sized appropriately</li> </ul> Week 2 references"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week2/#week-2-select-and-train-model","title":"Week 2: Select and Train Model","text":"<p>If you wish to dive more deeply into the topics covered this week, feel free to check out these optional references. You won\u2019t have to read these to complete this week\u2019s practice quizzes.</p> <p>Establishing a baseline</p> <p>Error analysis</p> <p>Experiment tracking</p> <p>Papers</p> <p>Brundage, M., Avin, S., Wang, J., Belfield, H., Krueger, G., Hadfield, G., \u2026 Anderljung, M. (n.d.). Toward trustworthy AI development: Mechanisms for supporting verifiable claims\u2217. Retrieved May 7, 2021http://arxiv.org/abs/2004.07213v2</p> <p>Nakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B., &amp; Sutskever, I. (2019). Deep double descent: Where bigger models and more data hurt. Retrieved from http://arxiv.org/abs/1912.02292</p>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week3/","title":"Week 3 \u2014 Data","text":""},{"location":"ml/mle-for-production/intro-to-mle-for-production/week3/#data-definition-can-be-hard","title":"Data definition can be hard","text":"<ul> <li>Data might suffer from inconsistent labelling which might mean the instructions for labellers are ambiguous.</li> </ul> <p>Data Definition questions:</p> <ul> <li> <p>What is the input $x$?</p> <p>Lighting? Contrast? Resolution?</p> <p>What features need to be included?</p> </li> <li> <p>What is the target label $y$?</p> <p>How can we ensure labellers give consistent labels?</p> </li> </ul>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week3/#unstructured-vs-structured-data","title":"Unstructured v/s structured data","text":"<p>Unstructured data</p> <ul> <li>May or may not have huge collection of unlabeled examples $x$</li> <li>Humans can label more data.</li> <li>Data augmentation more likely to be helpful</li> </ul> <p>Structured data</p> <ul> <li>May be more difficult to obtain more data</li> <li>Human labeling may not be possible (with some exceptions).</li> </ul>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week3/#small-data-vs-big-data","title":"Small data vs Big Data","text":"<p>Small data</p> <ul> <li>Clean labels are critical</li> <li>Can manually look through dataset and fix labels</li> <li>Can get all the labelers to talk to each other</li> </ul> <p>Big data</p> <ul> <li>Emphasis on data Process</li> </ul>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week3/#small-data-and-label-consistency","title":"Small data and label consistency","text":"<ul> <li>Label consistency is very important for small data regime</li> </ul> <p>Big datasets can also have small data challenged for those long tail of rare events.</p>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week3/#improving-label-consistency","title":"Improving label consistency","text":"<ul> <li>Have multiple labelers label same example</li> <li>When there is disagreement, have MLE, subject matter expert (SME) and/or labelers discuss definition of $y$ to reach agreement.</li> <li>If labelers believe that $x$ doesn't contain enough information, consider changing $x$</li> <li>Iterate until it is hard to significantly increase agreement.</li> <li>Create a new class/label to capture uncertainty (\"[unintelligible]\" class for ambiguity in sound)</li> </ul>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week3/#small-data-vs-big-data-unstructured-data","title":"Small data v/s big data (unstructured data)","text":"<p>Small data</p> <ul> <li>Usually small numbers of labelers</li> <li>Can ask labelers to discuss specific labels</li> </ul> <p>Big data</p> <ul> <li>Get to consistent definition with a small group</li> <li>Then send labeling instructions to labelers</li> <li>Can consider having multiple labelers label every example and using voting or consensus labels to increase accuracy.</li> </ul>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week3/#human-level-performance-hlp","title":"Human Level Performance (HLP)","text":"<p>Why?</p> <ul> <li>HLP help estimate bayes error/ irreducible error to help with error analysis and prioritization.</li> <li>In academia, establish and beat a respectable benchmark to support publication.</li> <li>Business or product owner asks for 99% accuracy. HLP helps establish a more reasonable target.</li> <li>\"Prove\" the MLP system is superior to humans doing the job and thus the business or product owner should adopt it. (\u26a0\ufe0fCAUTION)</li> </ul> <p></p>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week3/#raising-hlp","title":"Raising HLP","text":"<p>Often times the ground truth is just another human label. That means instead of trying of beat HLP, we should try to analyze why inspector didn't agree with the ground truth label.</p> <ul> <li>When the label $y$ comes from a human label, HLP &lt;&lt; 100% may indicate ambiguous labelling instructions</li> <li>Improving label consistency will raise HLP.</li> <li>This makes it harder for ML to beat HLP. And that is good because the more consistent labels will raise ML performance, which is ultimately likely to benefit the actual application performance.</li> </ul> <p>HLP is less frequently used for structured data problems where human labelers are less likely to involve but with few exceptions like, Based on network traffic, is computer hacked?, Spam account? Bot? like task.</p>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week3/#obtaining-data","title":"Obtaining data","text":""},{"location":"ml/mle-for-production/intro-to-mle-for-production/week3/#how-long-should-you-spend-obtaining-data","title":"How long should you spend obtaining data?","text":"<ul> <li>Get into this iteration loop as quickly as possible</li> <li> <p>Instead of asking: How long it would take to obtain $m$ examples?</p> <p>Ask: How much data can we obtain in $k$ days?</p> </li> <li> <p>Exception: If you have worked on the problem before and from experience you know you need $m$ examples.</p> </li> </ul> <p>You might also require to Brainstorm a list of data sources, cost and time it would take to get data. Other factors that might include: Data quality, privacy, regulatory constraints.</p>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week3/#labeling-data","title":"Labeling data","text":"<ul> <li>Options: In-house vs. outsourced vs. crowdsourced</li> <li>Having MLEs label data is expensive. But doing this for just a few days is usually fine.</li> <li> <p>Who is qualified to label?</p> <p>Specialized task like medical image diagnosis might require SME (subject matter expert) and can't be done anyone so easily.</p> <p>And for some task like recommender systems, maybe impossible to label well.</p> </li> <li> <p>Don't increase data by more than 10x at a time.</p> </li> </ul>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week3/#data-pipeline","title":"Data Pipeline","text":"<p>The important point is replicability.</p> <p>POC( proof-of-concept):</p> <ul> <li>Goal is to decide if the application is workable and worth deploying.</li> <li>Focus on getting the prototype to work!</li> <li>It's ok if data pre-processing is manual. But take extensive notes/comment.</li> </ul> <p>Production phase:</p> <ul> <li>After project utility is established, use more sophisticated tools to make sure the data pipeline is replicable.</li> <li>E.g., TensorFlow Transform, Apache Beam, Airflow,...</li> </ul>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week3/#meta-data-data-provenance-and-lineage","title":"Meta-data, data provenance and lineage","text":"<p>Keep track of data provenance and lineage.</p> <p>Data provenance: The documentation of where a piece of data comes from and the processes and methodology by which it was produced.</p> <p>Data lineage: Data lineage includes the data origin, what happens to it and where it moves over time i.e., sequence of steps.</p> <p>Metadata is data about data. Metadata can be really helpful to generate key insight during error analysis, spotting unexpected effects.</p>"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week3/#balanced-traindevtest-splits","title":"Balanced train/dev/test splits","text":"Week 3 References"},{"location":"ml/mle-for-production/intro-to-mle-for-production/week3/#week-3-data-definition-and-baseline","title":"Week 3: Data Definition and Baseline","text":"<p>Label ambiguity</p> <p>https://arxiv.org/pdf/1706.06969.pdf</p> <p>Data pipelines</p> <p>Data lineage</p> <p>MLops</p> <p>Geirhos, R., Janssen, D. H. J., Schutt, H. H., Rauber, J., Bethge, M., &amp; Wichmann, F. A. (n.d.). Comparing deep neural networks against humans: object recognition when the signal gets weaker\u2217. Retrieved May 7, 2021, from Arxiv.org website:</p>"},{"location":"ml/mle-for-production/ml-data-lifecycle/","title":"ML Data Lifecycle in Production","text":"<p>Second course of Machine Learning Engineering for Production</p>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/","title":"Week 1 \u2014 Collecting, labeling, and validating data","text":"<p>Overview</p> <p>This course is all about data, the first week we'll go through collecting our data, labeling it, and validating it. Along the way we'll also get familiarize with TensorFlow Extended (TFX) framework and data pipelines.</p> <p>\"Data is the hardest part of ML and the most important piece to get right...broken data is the most common cause of problems in production ML systems\" - Scaling Machine Learning at Uber with Michelangelo - Uber</p> <p></p> <p>The production setting for ML systems is really different than academic one where the data is fixed, already cleaned and ready to be experimented with.</p> <p>Success</p> <p>Production ML = ML development + software development</p> <p></p> <p>Using Modern software development also needs to account for:</p> <ul> <li>Scalability,</li> <li>Extensibility</li> <li>Configuration</li> <li>Consistency &amp; reproducibility</li> <li>Best practices</li> <li>Safety &amp; security</li> <li>Modularity</li> <li>Testability</li> <li>Monitoring</li> </ul> <p>Challenges in production grade ML</p> <ul> <li>Build integrated ML systems</li> <li>Continuously operate it in production</li> <li>Handle continuously changing data</li> <li>Optimize compute resource costs</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#ml-pipelines","title":"ML pipelines","text":"<p>ML pipeline is a software architecture for automating, monitoring, and maintaining the ML workflow from data to a trained model.</p> <p></p> <p>A directed acyclic graph (DAG) is a directed graph that has no cycles.</p> <p>ML pipeline workflow are usually DAGs,</p> <p>Pipeline orchestration frameworks are responsible for the various components in an ML pipeline depending on DAG dependencies. Basically help with pipeline automation.</p> <p>Examples: Airflow, Argo, Celery, Luigi, Kubeflow</p>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#tensorflow-extended-tfx","title":"TensorFlow Extended (TFX)","text":"<p>End-to-end platform for deploying production ML pipelines.</p> <p>TFX production components are designed for scalable, high-performance machine learning tasks.</p> <p>tion=\"Components represented by orange blocks.\" &gt;}}</p> <p></p>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#collecting-data-importance-of-data","title":"Collecting Data \u2014 Importance of Data","text":"<p>In ML arena Data is a first class citizen.</p> <ul> <li>Software 1.0: Explicit instructions were given to the computer</li> <li>Software 2.0:<ul> <li>Specify some goal on the behaviour of a program</li> <li>Find solution using optimization techniques.</li> <li>Good data is key for success</li> <li>Code in Software = Data in ML</li> </ul> </li> </ul> <p>Models aren't magic wands, they are statistical tools and so require meaningful data:</p> <ul> <li>Maximize predictive content</li> <li>remove non-informative data</li> <li>feature space coverage</li> </ul> <p>Key Points</p> <ul> <li> Understand users, translate user needs into data problems<ul> <li>What kind of/how much data is available</li> <li>What are the details and issues of your data</li> <li>What are your predictive features</li> <li>What are the labels you are tracking</li> <li>What are your metrics</li> </ul> </li> <li>Ensure data coverage and high predictive signal</li> <li>Source, store and monitor quality data responsibly</li> </ul> <p>Few issues while collecting data that may arise:</p> <ul> <li>Inconsistent formatting<ul> <li>Is zero \"0\", \"0.0\", or an indicator of a missing measurement, sea level, bad sensor?</li> </ul> </li> <li>Compounding errors from other ML models</li> <li>Monitor data sources for system issues and outages</li> <li>Outliers</li> </ul> <p>Measure data effectiveness</p> <ul> <li>Intuition about data value can be misleading<ul> <li>Which feature have predictive value and which ones do not?</li> </ul> </li> <li>Feature engineering helps to maximise the predictive signals</li> <li>Feature selection helps to measure the predictive signals</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#responsible-data-security-privacy-fairness","title":"Responsible Data: Security, Privacy &amp; Fairness","text":"<ul> <li>Data collection and management isn't just about your model<ul> <li>Give user control of what data can be collected</li> <li>Is there a risk of inadvertently revealing user data?</li> </ul> </li> <li>Compliance with regulations and policies (e.g. GPDR)</li> </ul> <p>Data privacy is proper usage, collection retention, deletion and storage of the data.</p> <ul> <li>Protect personally identifiable information<ul> <li>Aggregation - replace unique values with summary value</li> <li>Redaction - remove some data to create less complete picture</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#commit-to-fairness","title":"Commit to fairness","text":"<ul> <li>Make sure your models are fair<ul> <li>Group fairness, equal accuracy</li> </ul> </li> <li>Bias in human labeled and/or collected data.</li> <li>ML models can amplify biases.</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#reducing-bias-design-fair-labelling-systems","title":"Reducing bias: Design fair labelling systems","text":"<ul> <li>Accurate labels are necessary for supervised learning</li> <li>Labeling be done by<ul> <li>Automation (logging or weak supervision)</li> <li>Humans (aka \"Raters\", often semi-supervised)</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#how-ml-systems-can-fail-users","title":"How ML systems can fail users","text":"<ul> <li>Representational harm: A system will amplify or reflect a negative stereotype about particular groups.</li> <li>Opportunity denial: When a system makes predictions that have negative real life consequences that could result in lasting impacts.</li> <li>Disproportionate product failure: Where the effectiveness of your model is really skewed so that the output happen more frequently for particular groups of users, skewed outputs are generated.</li> <li>Harm by disadvantage: A system will infer disadvantageous associations between different demographic characteristics and user behaviour around that.</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#types-of-human-raters","title":"Types of human raters","text":"<ul> <li>Generalists (usually by crowdsourcing tools)</li> <li>Subject Matter Experts (requires specialized tools, like X-Rays)</li> <li>Your users (Derived labels, e.g. tagging photos.</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#key-points","title":"Key points","text":"<ul> <li>Ensure rater pool diversity</li> <li>Investigate rater context and incentives</li> <li>Evaluate rater tools</li> <li>Manage cost</li> <li>Determines freshness requirements</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#labeling-data-data-and-concept-change-in-production-ml","title":"Labeling data \u2014 Data and Concept Change in Production ML","text":""},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#detecting-problems-with-deployed-models","title":"Detecting problems with deployed models","text":"<ul> <li>Data and scope changes</li> <li>Monitor models and validate data to find problems early</li> <li> <p>Changing ground truth: label new training data</p> <p>The ground truth may change gradually (may be years, months) or faster (weeks) or maybe really really fast (days, hours, min). If the ground truth is changing really fast, you got a really hard problem and might be important to retrain ASAP after following a Direct feedback or Weak supervision.</p> </li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#key-points_1","title":"Key points","text":"<ul> <li>Model performance decays over times<ul> <li>Data and concept drift</li> </ul> </li> <li>Model retraining helps to improve performance<ul> <li>Data labeling for changing ground truth and scarce labels</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#process-feedback-and-human-labeling","title":"Process Feedback and Human Labeling","text":""},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#methods","title":"Methods","text":"<ul> <li>Process Feedback (Direct Labeling): e.g., Actual vs predicted click-through</li> <li>Human Labeling: e.g., Cardiologists labeling MRI images</li> <li>Semi-Supervised Labeling</li> <li>Active Learning</li> <li>Weak Supervision</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#why-is-labeling-important-in-production-ml","title":"Why is labeling important in production ML?","text":"<ul> <li>Using business/organisation available data</li> <li>Frequent model retraining</li> <li>Labeling ongoing and critical process</li> <li>Creating a training datasets requires labels</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#direct-labeling-continuous-creation-of-training-dataset","title":"Direct labeling \u2014 continuous creation of training dataset","text":"<p>Advantages:</p> <ul> <li>Training dataset continuous creation</li> <li>Labels evolve quickly</li> <li>Captures strong label signals</li> </ul> <p>Disadvantages:</p> <ul> <li>Hindered by inherent nature of the problem</li> <li>Failure to capture ground truth</li> <li>Largely custom designed</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#open-source-log-analysis-tools","title":"Open-Source log analysis tools","text":"<p>Logstash: Free and open source data processing pipeline</p> <ul> <li>Ingests data from a multitude of sources</li> <li>Transforms it</li> <li>Sends it to your favourite \"stash\"</li> </ul> <p>Fluentd: Open source data collector</p> <p>Unify the data collection and consumption</p>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#cloud-log-analytics","title":"Cloud log analytics","text":"<p>Google Cloud Logging</p> <ul> <li>Data and events from Google Cloud and AWS</li> <li>BindPlane. Logging: application components, on-premise and hybrid cloud systems</li> </ul> <p>AWS ElasticSearch</p> <p>Azure Monitor</p>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#human-labeling","title":"Human labeling","text":"<p>\"Raters\" examine data and assign labels manually</p> <p></p> <ul> <li>More labels</li> <li>Pure supervised learning</li> </ul> <p>Disadvantages:</p> <ul> <li>Quality consistency- many datasets difficult for human labeling</li> <li>Slow</li> <li>Expensive</li> <li>Small dataset curation</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#validating-data-detecting-data-issues","title":"Validating Data \u2014 Detecting Data issues","text":"<p>Concept Drift: Is the change in the statistical properties of the labels over time. The mapping from $x\\rightarrow y$ changes</p> <p>Data Drift: Changes in data over time, such as data collected once a day. The distribution of the data ($x$) changes</p> <p>Data skew: Difference between two static versions, or different sources, such as training set and serving set.</p>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#detecting-distribution-skew","title":"Detecting distribution skew","text":"<p>Dataset shift occurs when the joint probability of $x$ (features), $y$ (labels) is not same during training and serving.</p> <p>$$ P_\\text{train}(y,x) \\not = P_\\text{serve}(y, x) $$</p> <p>Covariate shift refers to the change in distribution of input variables present in training and serving data.</p> <p>Marginal distribution/probability of features is not the same during training and serving.&lt;</p> <p>$$ P_\\text{train}(y|x) = P_\\text{serve}(y|x) \\P_\\text{train}(x) \\not = P_\\text{serve}(x) $$</p> <p>Concept shift refers to a change in the relationship between the input and output variables as opposed to the differences in the Data Distribution or input itself./mark&gt; <p>$$ P_\\text{train}(y|x) \\not = P_\\text{serve}(y|x)\\P_\\text{train}(x) = P_\\text{serve}(x) $$</p>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#skew-detection-workflow","title":"Skew detection workflow","text":""},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#tensorflow-data-validation-tfdv","title":"TensorFlow Data Validation (TFDV)","text":"<ul> <li>Understand,  validate and monitor ML data at scale</li> <li>Used to analyze and validate petabytes of data at Google every day</li> <li>Proven track record in helping TFX users maintain the health of their ML pipelines</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#tfdv-capabilities","title":"TFDV capabilities","text":"<ul> <li>Generates data statistics and browser visualizations</li> <li>Infers the data schema</li> <li>Performs validity checks against schema</li> <li>Detects training/serving skew<ul> <li>Schema skew</li> <li>Feature skew</li> <li>Distribution skew</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#skew-tfdv","title":"Skew - TFDV","text":"<ul> <li>Supported for categorical features</li> <li>The Degree of data drift is expressed in terms of L-infinity distance (Chebyshev Distance):</li> </ul> <p>$$ D_\\text{Checbyshev}(x, y) = \\max_i(|x_i - y_i|) $$</p> <ul> <li>Set a threshold to receive warnings</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#schema-skew","title":"Schema skew","text":"<p>Serving and training data don't conform to same schema:</p> <ul> <li>For example, <code>int != float</code></li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#feature-skew","title":"Feature skew","text":"<p>Training feature values are different than the serving feature values:</p> <ul> <li>Feature values are modified between training and serving time</li> <li>Transformation applied only in one of the two instances</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#distribution-skew","title":"Distribution skew","text":"<p>Distribution of serving and training dataset is significantly different:</p> <ul> <li>Faulty sampling method during training</li> <li>Different data source for training and serving data</li> <li>Trend, seasonality, changes in data over time</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#key-points_2","title":"Key points","text":"<p>TFDV: Descriptive statistics at scale with the embedded facets visualizations</p> <p>It provides insight into:</p> <ul> <li>What are the underlying statistic of your data</li> <li>How does your training, evaluation, and serving dataset statistics compare</li> <li>How can you detect and fix data anomalies</li> </ul> Week 1 references"},{"location":"ml/mle-for-production/ml-data-lifecycle/week1/#week-1-collecting-labeling-and-validating-data_1","title":"Week 1: Collecting, Labeling and Validating Data","text":"<p>This is a compilation of optional resources including URLs and papers appearing in lecture videos. If you wish to dive more deeply into the topics covered this week, feel free to check out these optional references. You won\u2019t have to read these to complete this week\u2019s practice quizzes.</p> <p>MLops</p> <p>Data 1st class citizen</p> <p>Runners app</p> <p>Rules of ML</p> <p>Bias in datasets</p> <p>Logstash</p> <p>Fluentd</p> <p>Google Cloud Logging</p> <p>AWS ElasticSearch</p> <p>Azure Monitor</p> <p>TFDV</p> <p>Chebyshev distance</p> <p>Papers</p> <p>Konstantinos, Katsiapis, Karmarkar, A., Altay, A., Zaks, A., Polyzotis, N., \u2026 Li, Z. (2020). Towards ML Engineering: A brief history of TensorFlow Extended (TFX). http://arxiv.org/abs/2010.02013</p> <p>Paleyes, A., Urma, R.-G., &amp; Lawrence, N. D. (2020). Challenges in deploying machine learning: A survey of case studies. http://arxiv.org/abs/2011.09926</p> <p>ML code fraction:</p> <p>Sculley, D., Holt, G., Golovin, D., Davydov, E., &amp; Phillips, T. (n.d.). Hidden technical debt in machine learning systems. Retrieved April 28, 2021, from Nips.cc https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf</p>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/","title":"Week 2 \u2014 Feature Engineering","text":""},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#introduction-to-preprocessing","title":"Introduction to Preprocessing","text":"<p>Feature engineering can be difficult and time consuming, but also very important to success.</p>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#squeezing-the-most-out-of-data","title":"Squeezing the most out of data","text":"<ul> <li>Making data useful before training a model</li> <li>Representing data in forms that help models learn</li> <li>Increasing predictive quality</li> <li>Reducing dimensionality with feature engineering</li> <li>Feature Engineering within the model is limited to batch computations</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#art-of-feature-engineering","title":"Art of feature engineering","text":"<p>Increases the model ability to learn while simultaneously reducing (if possible) the compute resources it requires.</p> <p></p> <p></p> <p>During serving we typically process each request individually, so it becomes important that we include global properties of our features, such as the $\\sigma$ (standard deviation) </p>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#preprocessing-operations","title":"Preprocessing Operations","text":"<p>Data clearning to remove erroneous data.</p> <p>Feature tuning like normalizing, scaling.</p> <p>Representation transformation for better predictive signals</p> <p>Feature Extraction / dimensionality reduction for more data representation.</p> <p>Feature construction to create new features.</p>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#mapping-categorical-values","title":"Mapping categorical values","text":"<p>Categorical values can be one-hot encoded if two nearby values are not more similar than two distant values, otherwise ordinal encoded.</p>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#empirical-knowledge-of-data-will-guide-you-further","title":"Empirical knowledge of data will guide you further","text":"<p>Text: stemming, lemmatization, TF-IDF, n-grams, embedding lookup</p> <p>Images - clipping, resizing, cropping, blur, canny filters, soble filters, photometric distortions</p>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#key-points","title":"Key points","text":"<ul> <li>Data preprocessing: transforms raw data into a clean and training-ready dataset</li> <li>Feature engineering maps:<ul> <li>Raw data into feature vectors</li> <li>Integer values to floating-point values</li> <li>Normalizes numerical values</li> <li>String and categorical values to vectors of numeric values</li> <li>Data from one space into different space</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#feature-engineering-techniques","title":"Feature Engineering Techniques","text":""},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#scaling","title":"Scaling","text":"<ul> <li>Converts values from their natural range into a prescribed range<ul> <li>e.g., grayscale image pixel intensity scale is $[0, 255]$ usually rescaled to $[-1, 1]$</li> </ul> </li> </ul> <p>$$ x_\\text{scaled} = \\frac{(b-a)(x - x_{\\min})}{x_{\\max} - x_{\\min}} + a \\tag{$x$ $\\isin$ [a, b]} $$</p> <p>Normalization:</p> <p>$$ x_\\text{scaled} = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}} \\tag{$x$ $\\isin$ [0, 1]} $$</p> <ul> <li>Benefits<ul> <li>Helps NN converge faster</li> <li>Do away with <code>NaN</code> errors during training</li> <li>For each feature, the model learn the right weights.</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#standardization","title":"Standardization","text":"<ul> <li>Z-score relates the number of standard deviations away from the mean</li> </ul> <p>$$ x_\\text{std} = \\frac{x - \\mu}{\\sigma} $$</p>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#bucketizing-binning","title":"Bucketizing/ Binning","text":""},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#other-techniques","title":"Other techniques","text":"<p>Dimensionality reduction in embeddings</p> <ul> <li>Principle component analysis (PCA)</li> <li>t-Distribute stochastic neighbor embedding (t-SNE)</li> <li>uniform manifold approximation and projection (UMAP)</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#tensorflow-embedding-projector","title":"TensorFlow embedding projector","text":"<ul> <li>Intuitive explanation of high-dimensional data</li> <li>Visualize &amp; analyze</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#feature-crosses","title":"Feature Crosses","text":"<ul> <li>Combine multiple features together into a new feature</li> <li>Encodes nonlinearity in the feature space, or encodes the same information in fewer features</li> <li>$[A \\times B]$ : multiplying the values of two features</li> <li>$[A\\times B\\times C \\times D \\times E ]$: multiplying the values of 5 features</li> <li>$[\\text{Day of week, hour}] \\rightarrow [\\text{Hour of week}]$</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#key-points_1","title":"Key points","text":"<ul> <li>Feature crossing: synthetic feature encoding nonlinearity in feature space</li> <li>Feature coding: Transforming categorical to a continuous variable.</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#feature-transformation-at-scale-preprocessing-data-at-scale","title":"Feature Transformation at Scale \u2014 Preprocessing Data at Scale","text":"<p>To do feature transformation at scale we need ML pipeline to deploy our model with consistent and reproducible results.</p> <p></p>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#preprocessing-at-scale","title":"Preprocessing at scale","text":""},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#inconsistencies-in-feature-engineering","title":"Inconsistencies in feature engineering","text":"<ul> <li>Training &amp; serving code paths are different<ul> <li>Diverse deployment scenarios<ul> <li>Mobile (TensorFlow Lite)</li> <li>Server (TensorFlow Serving)</li> <li>Web (TensorFlow JS)</li> </ul> </li> </ul> </li> <li>Risks of introducing training-serving skews<ul> <li>Skews will lower the performance of your serving model</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#preprocessing-granularity","title":"Preprocessing granularity","text":""},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#applying-transformation-per-batch","title":"Applying Transformation per batch","text":"<ul> <li>For example, normalizing features by their average</li> <li>Access to a single batch of data, not the full dataset</li> <li>Ways to normalize per batch<ul> <li>Normalize by average within a batch</li> <li>Precompute average and reuse it during normalization</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#optimizing-instance-level-transformations","title":"Optimizing instance-level transformations","text":"<ul> <li>Indirectly affect training efficeincy</li> <li>Typically accelerators sit idle while the CPUs transform</li> <li>Solution:<ul> <li>Prefetching transforms for better accelerator efficiency</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#summarizing-the-challenges","title":"Summarizing the challenges","text":"<ul> <li>Balancing predictive performance</li> <li>Full-pass transformation on training data</li> <li>Optimizing instance-level transformation for better training efficiency (GPUs, TPUs,...)</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#key-points_2","title":"Key points","text":"<ul> <li>Inconsistent data affects the accuracy of the results</li> <li>Need for scaled data processing frameworks to process large datasets in an efficient and distribute manner</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#tensorflow-transform","title":"TensorFlow Transform","text":"<p>Example Gen: Generates Examples from the training &amp; evaluation data</p> <p>Statistics Gen: Generates Statistics</p> <p>Schema Gen: Generates schema after ingesting statistics. This schema is then fed to:</p> <ul> <li>Example validator: Takes schema and statistics and look for problems/anomalies in data</li> <li>Transform: takes schema and dataset and do feature engineering</li> </ul> <p>Trainer: Trains the model</p> <p>Evaluator: Evaluates the result</p> <p>Pusher: Pushes to wherever we want to serve our model.</p> <p></p>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#tftransform-going-deeper","title":"tf.Transform: Going Deeper","text":""},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#tftransform-analyzers","title":"tf.Transform Analyzers","text":"<p>Analyzers make a full pass over the dataset in order to collect constants that is required to do feature engineering. It also express the operations that we are going to do.</p> <p></p>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#how-transform-applies-feature-transformations","title":"How Transform applies feature transformations","text":""},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#benefits-of-using-tftransform","title":"Benefits of using tf.Transform","text":"<ul> <li>Emitted tf.Graph holds all necessary constants and transformations</li> <li>Focus on data preprocessing only at training time</li> <li>Works in-line during both training and serving</li> <li>No need for preprocessing code at serving time</li> <li>Consistently applied transformations irrespective of deployment platform</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#analyzers-framework","title":"Analyzers framework","text":""},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#tftransform-preprocessing_fn","title":"tf.Transform preprocessing_fn","text":"<pre><code>def preprocessing_fn(inputs):\n  ...\n  for key in DENSE_FLOAT_FEATURE_KEYS:\n    outputs[key] = tft.scale_to_z_score(inputs[key])\n\n  for key in VOCAB_FEATURE_KEYS:\n    outputs[key] = tft.vocabulary(inputs[key], vocab_filename=key)\n\n  for jey in BUCKET_FEATURE_KEYS:\n    outputs[key] = tft.bucketize(inputs[key], FEATURE_BUCKET_COUNT)\n</code></pre>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#commonly-used-imports","title":"Commonly Used Imports","text":"<pre><code>import tensorflow as tf\nimport apache_beam as beam\nimport apache_beam.io.iobase\n\nimport tensorflow_transform as tft\nimport tensorflow_transform.beam as tft_beam\n</code></pre>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#hello-world-with-tftransform","title":"Hello World with tf.Transform","text":""},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#inspect-data-and-prepare-metadata","title":"Inspect data and prepare metadata","text":"<pre><code>from tensorflow_transform.tf_metadata import (\n    dataset_metadata, dataset_scehma)\n\n# define sample data\nraw_data = [\n  {'x': 1, 'y': 1, 's': 'hello'},\n  {'x': 2, 'y': 2, 's': 'world'},\n  {'x': 3, 'y': 3, 's': 'hello'}\n]\n\nraw_data_metadata = dataset_metadata.DatasetMetadata(\n  dataset_schema.from_feature_spec({\n    'y': tf.io.FixedLenFeature([], tf.float32),\n    'x': tf.io.FixedLenFeature([], tf.float32),\n    's': tf.io.FixedLenFeature([], tf.string)\n}))\n</code></pre>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#preprocessing-data-transform","title":"Preprocessing data (Transform)","text":"<pre><code>def preprocessing_fn(inputs):\n  \"\"\"Preproceess input columns into transformed columns\"\"\"\n  x, y, s = inputs['x'], inputs['y'], inputs['s']\n  x_centered = x - tft.mean(x)\n  y_normalized = tft.scale_to_0_1(y)\n  s_integerized = tft.compute_and_apply_vocabulary(s)\n  # feature cross\n  x_centered_times_y_normalized  (x_centered * y_normalized)\n  return {\n    'x_centered': x_centered,\n    'y_normalized': y_normalized,\n    's_integerized': s_integerized,\n    'x_centered_times_y_normalized': x_centered_times_y_normalized,\n  }\n</code></pre>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#running-the-pipeline","title":"Running the pipeline","text":"<pre><code>def main():\n  with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n    # Define a beam pipeline\n    transformed_dataset, transform_fn = (\n      (raw_data, raw_data_metadata) | tft_beam.AnalyzeAndTransformDataset(\n          preprocessing_fn))\n\n  transformed_data, transformed_metadata = transformed_datset\n  print('\\nRaw data:\\n{}\\n'.format(pprint.pformat(raw_data)))\n  print('\\Transformed data:\\n{}'.format(pprint.pformat(tranformed_Data)))\n\nif __name__ == '__main__':\n        main()\n</code></pre>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#key-points_3","title":"Key points","text":"<ul> <li>tf.Transform allows the preprocessing of input data and creating features</li> <li>tf.Transform allows defining pre-processing pipelines and their execution using large-scale data processing frameworks, like Apache Beam.</li> <li>In a TFX pipeline, the Transform component implements feature engineering using TensorFlow Transform</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#feature-selection-feature-spaces","title":"Feature Selection \u2014 Feature Spaces","text":"<ul> <li>N dimensional space defined by your N features</li> <li>Not including the target label</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#feature-space-coverage","title":"Feature space coverage","text":"<ul> <li>Train/Eval datasets should be representative of the serving dataset<ul> <li>Same numerical ranges</li> <li>Same classes</li> <li>Similar characteristics for image data</li> <li>Similar vocabulary.</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#ensure-feature-space-coverage","title":"Ensure feature space coverage","text":"<ul> <li>Data affected by: seasonality, trend, drift</li> <li>Serving data: new values in features and labels</li> <li>Continuous monitoring, key for success!</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#feature-selection","title":"Feature Selection","text":"<ul> <li>Feature selection identifies the features that best represent the relationship between the features, and the target that we're trying to predict.</li> <li>Remove features that don't influence the outcome</li> <li>Reduce the size of the feature space</li> <li>Reduces the resource requirements and model complexity</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#unsupervised-feature-selection-methods","title":"Unsupervised Feature selection methods","text":"<ul> <li>Feature-target variable relationship not considered</li> <li>Removes redundant features (correlation)<ul> <li>Two features that are highly correlated, you might need only one</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#supervised-feature-selection","title":"Supervised feature selection","text":"<ul> <li>Uses features-target variable relationship</li> <li>Selects those contributing the most</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#filter-methods","title":"Filter methods","text":"<ul> <li>Correlated features are usually redunfant<ul> <li>We remove them.</li> </ul> </li> <li>Filter methods suffer from inefficiencies as they need to look at all the possible feature subsets</li> </ul> <p>Popular filter methods:</p> <ul> <li>Pearson Correlation<ul> <li>Between features, and between the features and the label</li> </ul> </li> <li>Univariate Feature Selection</li> </ul> <p></p>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#feature-comparison-statistical-tests","title":"Feature comparison statistical tests","text":"<ul> <li>Pearson's correlation: Linear relationships</li> <li>Kendall Tau Rank Correlation Coefficient: Monotonic relationships &amp; small sample size</li> <li>Spearman's Rank Correlation Coefficient: Monotonic relationships</li> </ul> <p>Other methods:</p> <ul> <li>Pearson Correlation (numeric features - numeric target,\u00a0exception: when target is 0/1 coded)</li> <li>ANOVA f-test (numeric features - categorical target)</li> <li>Chi-squared (categorical features - categorical target)</li> <li>Mutual information</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#determining-correlation","title":"Determining correlation","text":"<pre><code># Pearson's correlation by default\ncor = df.corr()\n\nplt.figure(figsize=(20,20))\nimport seaborn as sns\nsns.heatmap(cor, annot=True, cmap=plt.cm.PuBu)\nplt.show()\n</code></pre>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#selecting-features","title":"Selecting Features","text":"<pre><code>cor_target = abs(cor['diagnosis_int'])\n\n# Selecting highly correlated features as potential features to eliminate\nrelavant_features = cor_target[cor_target &gt; 0.2]\n</code></pre>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#univariate-feature-selection-in-sklearn","title":"Univariate feature selection in Sklearn","text":"<p>Sklearn Univarite feature selection routines:</p> <ol> <li><code>SelectKBest</code></li> <li><code>SelectPercentile</code></li> <li><code>GenericUnivariateSelect</code></li> </ol> <p>Statistical tests available:</p> <ul> <li>Regression: <code>f__regression</code>, <code>mutual_info_regression</code></li> <li>Classification: <code>chi2</code> , <code>f_classif</code>, <code>mutual_info_classif</code></li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#selectkbest-implementation","title":"<code>SelectKBest</code> implementation","text":"<pre><code>def univariate_selection():\n  X_train, X_test, y_train, y_test = train_test_split(X, y,\n                  test_size=0.2, stratify=y, random_state=123)\n  X_train_scaled = StandardScaler().fit_transform(X_train)\n  X_test_scaled = StandardScaler().fit(X_train).transform(X_test)\n\n  min_max_scaler = MinMaxScaler()\n  scaled_X = min_max_scaler.fit_transform(X_train_scaled)\n\n  selector = SelectKBest(chi2, k=20) # Use Chi-Squared test\n  X_new = selector.fit_transform(scaled_X, y_train)\n  feature_idx = selector.get_support()\n  feature_names = df.drop(\"diagnosis_int\", axis=1).columns[feature_idx]\n  return feature_names\n</code></pre>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#wrapper-methods","title":"Wrapper Methods","text":"<p>It's a search method against the features that you have using a model as the measure of their effectiveness</p> <p>Wrapper methods are based on greedy algorithm and this solutions are slow to compute.</p> <p>Popular methods include:</p> <ol> <li>Forward Selection</li> <li>Backward Elimiation</li> <li>Recursive Feature Elimination</li> </ol> <p></p>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#forward-selection","title":"Forward Selection","text":"<ol> <li>Iterative, greedy method</li> <li>Starts with 1 feature</li> <li>Evaluate model performance when adding each of the additional features, one at a time.</li> <li>Add next feature that gives the best performance</li> <li>Repeat until there is no improvement</li> </ol>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#backward-elimination","title":"Backward Elimination","text":"<ol> <li>Start with all features</li> <li>Evaluate model performance when removing each of the included features, one at a time.</li> <li>Remove next feature that gives the best performance</li> <li>Repeat until there is no improvement</li> </ol>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#recursive-feature-elimination-rfe","title":"Recursive Feature Elimination (RFE)","text":"<ol> <li>Select a model to use for evaluating feature importance</li> <li>Select the desired number of features</li> <li>Fit the model</li> <li>Rank features by importance</li> <li>Discard least important features</li> <li>Repeat until the desired number of features remains</li> </ol> <pre><code>def run_rfe():\n  X_train, X_test, y_train, y_test = train_test_split(X, y,\n                  test_size=0.2, stratify=y, random_state=123)\n\n  X_train_scaled = StandardScaler().fit_transform(X_train)\n  X_test_scaled = StandardScaler().fit(X_train).transform(X_test)\n\n  model = RandomForestClassifier(criterion='entropy', random_state=47)\n  rfe = RFE(model, 20)\n  rfe = rfe.fit(X_train_scaled, y_train)\n  feature_names = df.drop('diagnosis_int', axis=1).columns[rfe.get_support()]\n  return feature_names\n\nrfe_feature_names = run_rfe()\n\nrfe_eval_df = evaluate_model_on_features(df[rfe_feature_names], y)\n</code></pre>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#embedded-methods","title":"Embedded Methods","text":"<ul> <li>L1 regularization</li> <li>Feature importance</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#feature-importance","title":"Feature importance","text":"<ul> <li>Assigns scores for each feature in data</li> <li>Discard features scores lower by feature importance</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#feature-importance-with-sklearn","title":"Feature importance with Sklearn","text":"<ul> <li>Feature Importance class is in-built in Tree Based Model (e.g., <code>RandomForestClassifier</code>)</li> <li>Feature importance is available as a property <code>feature_importances_</code></li> <li>We can then use <code>SelectFromModel</code> to select features from the trained model based on assigned feature importances.</li> </ul> <pre><code>def feature_importances_from_tree_based_model_():\n  X_train, X_test, y_train, y_test = train_test_split(X, y,\n                  test_size=0.2, stratify=y, random_state=123)\n\n  model = RandomForestClassifier()\n  model = model.fitX_Train y_train)\n\n  feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n  feat_importances.nlargest(10).plot(kind='barh')\n  plt.show()\n  return model\n</code></pre>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#select-features-based-on-importance","title":"Select features based on importance","text":"<pre><code>def select_features_from_model(model):\n        model = SelectFromModel(model, prefit=True, threshold=0.012)\n        feature_idx = df.drop(\"diagnosis_int\", 1).columns[feature_idx]\n        return feature_names\n</code></pre>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#tying-together-and-evaluation","title":"Tying together and evaluation","text":"<pre><code># Calcualte and plot feature importances\nmodel = feature_importances_from_tree_based_model_()\n\n# Select fearues based on feature importances\nfeature_imp_feature_names = select_features_from_model(model)\n</code></pre> Week 2 References"},{"location":"ml/mle-for-production/ml-data-lifecycle/week2/#week-2-feature-engineering-transformation-and-selection","title":"Week 2: Feature Engineering, Transformation and Selection","text":"<p>If you wish to dive more deeply into the topics covered this week, feel free to check out these optional references. You won\u2019t have to read these to complete this week\u2019s practice quizzes.</p> <p>Mapping raw data into feature</p> <p>Feature engineering techniques</p> <p>Scaling</p> <p>Facets</p> <p>Embedding projector</p> <p>Encoding features</p> <p>TFX:</p> <ol> <li>https://www.tensorflow.org/tfx/guide#tfx_pipelines</li> <li>https://ai.googleblog.com/2017/02/preprocessing-for-machine-learning-with.html</li> </ol>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/","title":"Week 3 \u2014 Data Journey","text":""},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#data-storag-data-journey","title":"Data Storag -- Data Journey","text":"<ul> <li>Data transforms as it flows through the process</li> <li>Interpreting model results requires understanding data transformation</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#artifacts-and-the-ml-pipeline","title":"Artifacts and the ML pipeline","text":"<ul> <li>Artifacts are created as the component of the ML pipeline execute</li> <li>Artifacts include all of the data and objects which are produced by the pipeline components.</li> <li>This includes the data, in different stages of transformation, the schema, the model itself, metrics, etc.</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#data-provenance-and-lineage","title":"Data provenance and lineage","text":"<ul> <li>The chain of transformations that led to the creation of a particular artifact</li> <li>Important for debugging and reproducibility.</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#data-lineage-data-protection-regulation","title":"Data lineage: data protection regulation","text":"<ul> <li>Organizations must closely track and organize personal data</li> <li>Data lineage is extremely important for regulatory compliance</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#data-versionining","title":"Data versionining","text":"<ul> <li>Data pipeline management is a major challenge</li> <li>Machine learning requires reproducibility</li> <li>Code versioning: GitHub and similar code repositories</li> <li>Environment versioning: Docker, Terraform, and similar</li> <li>Data versioning:<ul> <li>Version control of datasets</li> <li>e.g., DVC, Git-LFS</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#introduction-to-ml-metadata","title":"Introduction to ML Metadata","text":"<ul> <li>Every run of a production ML pipeline generates metadata containing information about the various pipeline components, their executions and the resulting artifacts.</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#ml-metadata-library","title":"ML Metadata library","text":"<ul> <li>Tracks metadata flowing between components in pipeline</li> <li>Supports multiple storage backends</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#ml-metadata-terminology","title":"ML Metadata terminology","text":"<p>Artifact: An artifact is an elementary unit of data that gets fed into the ML metadata store and as the data is consumed as input or as generated as output of each component.</p> <p>Execution: Each execution is a record of any component run during the ML pipeline workflow, along with its associated runtime parameters.</p> <p>Context: A context may hold the metadata of the projects being run, experiments being conducted, details about pipeline, etc. It captures the shared information within the group. For example: project name, changelist commit id, experiment annotations etc. Artifacts and executions can be clustered together for each type of component separately.</p> <ul> <li>Each units can be of several different types having different properties stored in ML metadata.</li> <li>Relationships store the various units getting generated or consumed when interacting with other units.<ul> <li>Like Event is the record of a relationship between an artifact and an execution.</li> </ul> </li> </ul> <p></p>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#metdata-stored","title":"Metdata Stored","text":""},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#inside-metadatastore","title":"Inside MetadataStore","text":""},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#ml-metadata-in-action","title":"ML MetaData in Action","text":"<pre><code>!pip install ml-metadata\nfrom ml_metadata import metadata_store\nfrom ml_metadata.proto import metadata_store_pb2\n</code></pre>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#ml-metadata-storage-backend","title":"ML Metadata storage backend","text":"<ul> <li>ML metadata registers metadata in a database called Metadata Store</li> <li>APIs to record and retrieve metadata to and from the storage backend<ul> <li>Fake database: in-memory for fast experimentation/prototyping</li> <li>SQLite: in-memory and disk</li> <li>MySQL: server based</li> <li>Block storage: File system, storage area network, or cloud based</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#fake-database","title":"Fake database","text":"<pre><code>connection_config = metadata_store_pb2.ConnectionConfig()\n# Set an empty fake database proto\nconnection_config.fake_database.SetInParent()\n\nstore = metadata_store.MetadataStore(connection_config)\n</code></pre>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#sqlite","title":"SQLite","text":"<pre><code>connection_config = metadata_store_pb2.ConnectionConfig()\nconnection_config.sqlite.filename_uri = '...'\nconnection_config.sqlite.connection_mode = 3 # READWRITE_OPENCREATE\n\nstore = metadata_store.MetadataStore(connnection_config)\n</code></pre>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#mysql","title":"MySQL","text":"<pre><code>connection_config = metadata_store_pb2.ConnectionConfig()\n\nconnection_config.mysql.host = '...'\nconnection_config.mysql.port = '...'\nconnection_config.mysql.database = '...'\nconnection_config.mysql.user = '...'\nconnection_config.mysql.password = '...'\n\nstore = metadata_store.MetadataStore(connnection_config)\n</code></pre>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#schema-development","title":"Schema Development","text":"<p>Schema are relational objects summarizing the features in a given dataset or project. This includes:</p> <ul> <li>Feature name</li> <li>Type: float, int, string, etc</li> <li>Required or optional</li> <li>Valency (feature with multiple values)</li> <li>Domain: range, categories</li> <li>Default values</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#reliability-during-data-evolution","title":"Reliability during data evolution","text":"<p>Your system and your development process must treat data errors as first-class citizens, just like code bugs.</p> <p>Iteratively update and fin-tune schema to adapt to evolving data</p> <p>Platform needs to resilient to disruptions from:</p> <ul> <li>inconsistent data</li> <li>pipeline needs to gracefully handle software generated errors</li> <li>user misconfigurations</li> <li>uneven execution environments</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#scalability-during-data-evolution","title":"Scalability during data evolution","text":""},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#anomaly-detection-using-data-evolution","title":"Anomaly detection using Data evolution","text":""},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#schema-inspection-during-data-evolution","title":"Schema inspection during data evolution","text":""},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#schema-environments","title":"Schema Environments","text":"<ul> <li>You my have different schema versions for different environments, like development and testing.</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#maintaining-varieties-of-schema","title":"Maintaining varieties of schema","text":""},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#inspect-anomalies-in-serving-dataset","title":"Inspect anomalies in serving dataset","text":"<pre><code>stats_options = tfdv.StatsOptions(schema=schema,\n                                                                    infer_type_from_schema=True)\neval_stats = tfdv.generate_statistics_from_csv(\n    data_location=SERVING_DATASET,\n    stats_options=stats_options\n)\n\nserving_anomalies = tfdv.validate_statistics(eval_stats, schema)\ntfdv.display_anomalies(serving_anomalies)\n</code></pre>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#schema-environments_1","title":"Schema environments","text":"<ul> <li>Customize the schema for each environment</li> <li>e.g., Add or remove label in schema based on type of dataset</li> </ul> <pre><code>schema.default_environment.append('TRAINING')\nschema.default_environment.append('SERVING')\n\n# Modify serving environment feature,\n# removing from 'SERVING' environment as it will not be there\ntfdv.get_feature(schema, 'Cover_Type').not_in_environment.append('SERVING')\n</code></pre> <p>Now Inspecting anomalies will return no anomalies</p> <pre><code>serving_anomalies = tfdv.validate_statistics(\n  eval_stats,\n  schema,\n  environment='SERVING')\ntfdv.display_anomalies(serving_anomalies)\n# No anomalies found\n</code></pre>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#enterprise-data-storage-feature-stores","title":"Enterprise Data Storage \u2014 Feature Stores","text":"<p>A feature store is a central repository for storing documented, curated and access controlled features.</p> <p>A feature store make it easy for different teams to share, discover and consume highly curated features. Many modelling problems might use identical or similar features across organization, thus having a feature store greatly reduces redundant work.  This also enables teams to share and discover data.</p>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#key-aspects","title":"Key aspects","text":"<ul> <li>Managing feature data from a single person to large enterprises</li> <li>Scalable and performant access to feature data inn training and serving.</li> <li>Provide consistent and point-in-time correct access to feature data.</li> <li>Enable discovery, documentation, and insights into your features.</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#data-warehouse","title":"Data Warehouse","text":"<p>Data warehouse is a technology that aggregates data from one or more sources so that it can be processed and analyzed. It's optimized for long running batch jobs and read operations. They are not designed to placed in a production environment.</p>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#key-features-of-data-warehouse","title":"Key features of data warehouse","text":"<ul> <li>A data warehouse is subject oriented and information stored in it revolves around a topic.</li> <li>The data might be collected from various types of sources, like RDBMS, files etc.,</li> <li>Non-volatile, previous versions of data is not erased when new data is added</li> <li>Time variant, can let you go through timestamped data.</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#advantages","title":"Advantages","text":"<ul> <li>Enhanced ability to analyze the data, without worrying about serving performance degradation.</li> <li>Timely access to data</li> <li>Enhanced data quality and consistency</li> <li>High return on investment</li> <li>Increased query and system performance</li> </ul>"},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#comparison-with-databases","title":"Comparison with databases","text":""},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#data-lakes","title":"Data Lakes","text":"<p>A data lake is a system or repository of data stores in its natural and raw format.</p> <ul> <li>A data lake, like warehouse aggregates data from various sources of enterprise data</li> <li>Data can be structured or unstructured</li> <li>Doesn't involve any processing before writing data, don't have schema</li> </ul> <p></p> Week 3 References"},{"location":"ml/mle-for-production/ml-data-lifecycle/week3/#week-3-data-journey-and-data-storage","title":"Week 3: Data Journey and Data Storage","text":"<p>If you wish to dive more deeply into the topics covered this week, feel free to check out these optional references. You won\u2019t have to read these to complete this week\u2019s practice quizzes.</p> <p>Data Versioning:</p> <ol> <li>https://dvc.org/</li> <li>https://git-lfs.github.com/</li> </ol> <p>ML Metadata:</p> <ol> <li>https://www.tensorflow.org/tfx/guide/mlmd#data_model</li> <li>https://www.tensorflow.org/tfx/guide/understanding_custom_components</li> </ol> <p>Chicago taxi trips data set:</p> <ol> <li>https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew/data</li> <li>https://archive.ics.uci.edu/ml/datasets/covertype</li> </ol> <p>Feast:</p> <ol> <li>https://cloud.google.com/blog/products/ai-machine-learning/introducing-feast-an-open-source-feature-store-for-machine-learning</li> <li>https://github.com/feast-dev/feast</li> <li>https://www.gojek.io/blog/feast-bridging-ml-models-and-data</li> </ol>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/","title":"ML modeling Pipeline in Production","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/","title":"Week 1 \u2014 Hyperparameter tuning: searching for the best architecture","text":"<p>Overview</p> <p>This course is focuses on tools and techniques to effectively manage modeling resources and best serve batch and real-time inference requests.</p> <ul> <li>Effective search strategies for the best model that will scale for various serving needs</li> <li>Constraining model complexity and hardware requirements</li> <li>Optimize and manage compute storage and IO needs</li> <li>We'll be going through TensorFlow Model Analysis (TFMA)</li> </ul> <p></p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#neural-architecture-search","title":"Neural Architecture Search","text":"<ul> <li>Neural architecture search (NAS) is a technique for automating the design of artificial neural networks</li> <li>It helps finding the optimal architecture</li> <li>This is a search over a huge space</li> <li>AutoML is an algorithm to automate this search</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#types-of-parameters-in-ml-models","title":"Types of parameters in ML models","text":"<ul> <li>Trainable parameters<ul> <li>Learned by the algorithm during training</li> <li>e.g., weights of a neural network</li> </ul> </li> <li> <p>Hyperparameters</p> <ul> <li>Set before launching the learning process</li> <li>not updated in each training step</li> </ul> <p>Hyperparameters are of two types:</p> <ol> <li>Model hyperparameters\u00a0which influence model selection such as the number and width of hidden layers</li> <li>Algorithm hyperparameters\u00a0which influence the speed and quality of the learning algorithm such as the learning rate for Stochastic Gradient Descent (SGD) and the number of nearest neighbors for a k Nearest Neighbors (KNN) classifier.</li> <li>e.g., learning rate or the number of units in a dense layer</li> </ol> </li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#manual-hyperparameter-tuning-is-not-scalable","title":"Manual hyperparameter tuning is not scalable","text":"<p>The process of finding the optimal set of hyperparameters is called hyperparameter tuning or hypertuning.</p> <ul> <li>Hyperparameters can be numerous even for small models</li> <li>e.g., shallow DNN<ul> <li>Architecture choices</li> <li>activation functions</li> <li>Weight initialization strategy</li> <li>Optimization hyperparameters such as learning rate, stop condition</li> </ul> </li> <li>Tuning them manually can be a real brain teaser</li> <li>helps boost model performance.</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#automating-hyperparameter-tuning-with-keras-tuner","title":"Automating hyperparameter tuning with Keras Tuner","text":"<ul> <li>Automation is key.</li> <li>Keras Tunes:<ul> <li>Hyperparameter tuning with TensorFlow 2.0</li> <li>Many methods available&gt;</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#keras-autotuner-demo","title":"Keras Autotuner Demo","text":"<ul> <li>Do the model need more or less hidden units to perform well?</li> <li>How does model size affect the convergence spped?</li> <li>Is there any trade off between convergence speed, model size and accuracy?</li> <li>Search automation is the natural path to take</li> <li>keras tuner built in search functionality will help!</li> <li>Keras Tuner has four tuners available with built-in strategies:<ul> <li><code>RandomSearch</code></li> <li><code>Hyperband</code></li> <li><code>BayesianOptimization</code></li> <li><code>Sklearn</code></li> </ul> </li> </ul> <pre><code>import keras_tuner as kt\n\n\ndef model_builder(hp):\n  '''\n  Builds the model and sets up the hyperparameters to tune.\n  Args:\n    hp - Keras tuner object\n  Returns:\n    model with hyperparameters to tune\n  '''\n  # The model you set up for hypertuning is called a hypermodel.\n  model = keras.Sequential()\n  model.add(keras.layers.Flatten(input_shape=(28, 28)))\n\n  hp_units = hp.Int('units', min_value=16, max_value=512, step=16)\n  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n  model.add(tf.keras.layers.Dropout(0.2))\n  model.add(keras.layer.Dense(10))\n\n  model.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n  return model\n\n# Keras tuner support multiple strategies, one we're using is Hyperband strategy\n# Keras Tuner has four tuners available with built-in strategies - RandomSearch, Hyperband, BayesianOptimization, and Sklearn\ntuner = kt.Hyperband(\n  model_builder,\n  objective='val_accuracy',\n  max_epochs=10,\n  factor=3,\n  directory='my_dir',\n  project_name='intro_to_kt')\n\n# callback configuration\nstop_early = tf.keras.callbacks.EarlyStopping(\n  monior='val_loss', patience=5)\n\ntuner.search(\n  X_train, y_train,\n  epochs=50,\n  validation_split=0.2,\n  callbacks=[stop_early])\n</code></pre>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#search-output","title":"Search Output","text":"<p>Then, we can set the hidden units to be <code>48</code></p> <pre><code>best_hps=tuner.get_best_hyperparameters()[0]\nh_model = tuner.hypermodel.build(best_hps)\nh_model.fit(X_train, y_train, epochs=NUM_EPOCHS, validation_split=0.2)\n</code></pre>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#automl-intro-to-automl-automated-machine-learning","title":"AutoML \u2014 Intro to AutoML (Automated Machine Learning)","text":"<p>It is aimed at developers with very little experience in ML to make use of ML model and techniques by trying to automate entire workflow end-to-end.</p> <p></p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#neural-architecture-search-of-automl","title":"Neural Architecture Search (\u2764\ufe0f of AutoML)","text":"<p>The process of automating architecture engineering is strictly called NAS.</p> <p>Three main parts:</p> <ul> <li>Search space: Defines the range of architecture which can be represented.</li> <li>Search strategy: Defines how we explore the search space.</li> <li>Performance estimation strategy: Helps measure the comparing the performance of various architectures.</li> </ul> <p></p> <p></p> <p></p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#understanding-search-spaces","title":"Understanding Search Spaces","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#types-of-search-space","title":"Types of Search Space:","text":"<ul> <li>Macro</li> <li>Micro</li> </ul> <p>Node: A node is a layer in a neural network.</p> <p>An arrow from layer $\\text{L}_i$ to $\\text{L}_j$ indicates the layer $\\text{L}_j$receives the output of $\\text{L}_i$as input.</p> <p></p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#macro-architecture-search-space","title":"Macro Architecture Search Space","text":"<p>A macro search space contains the individual layers and connection types of neural network.</p> <ul> <li>A NAS searches within that space for the best model, building the model layer by layer.</li> </ul> <p></p> <p>In a chain-structured Neural Network Architecture (NNA), space is parametrized as:</p> <ul> <li>The operation every layer can execute</li> <li>Hyperparameters associated with the operation</li> <li>A number of n sequentially fully-connected layers</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#micro-architecture-search-space","title":"Micro Architecture Search Space","text":"<p>In a micro search space, NAS build a neural network from cells where each cell is a smaller network.</p> <ul> <li>Cells are stacked to produce the final network.</li> </ul> <p></p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#search-strategies","title":"Search Strategies","text":"<ul> <li>Find the architecture that produces the best performance</li> </ul> <p>A few search strategies</p> <ol> <li>Grid Search</li> <li>Random Search</li> <li>Bayesian Optimization</li> <li>Evolutionary Algorithms</li> <li>Reinforcement Learning</li> </ol>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#grid-search-random-search","title":"Grid Search &amp; Random Search","text":"<p>Grid Search</p> <ul> <li>Exhaustive search approach on fixed grid values</li> </ul> <p>Random Search:</p> <ul> <li>Select the next options randomly within the search space.</li> </ul> <p>Both Suitable for small search space.</p> <p>Both quickly fail with growing size of search space</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#bayesian-optimization","title":"Bayesian Optimization","text":"<ul> <li>Assumes that a specific probability distribution, is underlying the performance.</li> <li>Tested architectures constrain the probability distribution and guide the selection of the next option.</li> <li>This way, promising architectures can be stochastically determined and tested.</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#evolutionary-methods","title":"Evolutionary Methods","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#reinforcement-learning","title":"Reinforcement Learning","text":"<ul> <li>Agents goal is to maximise a reward.</li> <li>The available options are selected from the search space.</li> <li>The performance estimation strategy determines the reward</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#reinforcement-learning-for-nas","title":"Reinforcement Learning for NAS","text":"<ul> <li>A NN can also be specified by a variable length string.</li> <li>Hence RNN can be used generate that string, referred as controller.</li> <li>After training model on real data called, child network, we measure the accuracy on validation set.</li> <li>This accuracy then determines the reinforcement learning reward.</li> </ul> Neural Architecture Search"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#neural-architecture-search_1","title":"Neural Architecture Search","text":"<p>If you wish to dive more deeply into neural architecture search , feel free to check out these optional references. You won\u2019t have to read these to complete this week\u2019s practice quizzes.</p> <ul> <li>Neural Architecture Search</li> <li>Bayesian Optimization</li> <li>Neural Architecture Search with Reinforcement Learning</li> <li>Progressive Neural Architecture Search</li> <li>Network Morphism</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#measuring-automl-efficacy","title":"Measuring AutoML Efficacy","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#performance-estimation-strategy","title":"Performance Estimation Strategy","text":"<p>The search strategies in neural architecture search need to estimate the performance of generated architectures, so that they can in turn generate even better performing architectures.</p> <p>The simplest approach is to measure validation accuracy...</p> <p></p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#strategies-to-reduce-the-cost","title":"Strategies to Reduce the Cost","text":"<ol> <li>Lower fidelity estimates</li> <li>Learning Curve Extrapolation</li> <li>Weight Inheritance/ Network Morphisms</li> </ol>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#lower-fidelity-estiamtes","title":"Lower fidelity estiamtes","text":"<p>Lower fidelity or precision estimates try to reduce training time by reframing the problem make is easier to solve.</p> <p>This is done either by:</p> <ul> <li>Training on subset of data</li> <li>lower resolution images</li> <li>Fewer filters per layer and fewer cells</li> </ul> <p></p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#learning-curve-extrapolation","title":"Learning Curve Extrapolation","text":"<p>Based on the assumption that you have mechanisms to predict the learning curve reliably.</p> <ul> <li>Extrapolates based on initial learning</li> <li>Removes poor performers</li> <li>Progressive NAS, uses a similar approach by training a surrogate model and using it to predict the performance using architectural properties.</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#weight-inheritancenetwork-morphisms","title":"Weight Inheritance/Network Morphisms","text":"<ul> <li>Initialize weights of new architectures based on previously trained architectures.<ul> <li>Similar to transfer learning</li> </ul> </li> <li>Uses Network Morphism<ul> <li>Modifies the architecture without changing the underlying function.</li> <li>New network inherits knowledge from parent network</li> <li>Computational speed up: only a few days of GPU usage</li> <li>Network size not inherently bounded</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#automl-on-the-cloud","title":"AutoML on the Cloud","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#amazon-sagemaker-autopilot","title":"Amazon SageMaker Autopilot","text":"<p>Automatically trains and tunes the model for classification or regression based on your data.</p> <p></p> <p></p> <p></p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#microsoft-azure-automl","title":"Microsoft Azure AutoML","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#google-cloud-automl","title":"Google Cloud AutoML","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#how-do-these-cloud-offerings-perform-automl","title":"How do these Cloud offerings perform AutoML?","text":"<ul> <li>We don't know (or can't say) and they're not about to tell us.</li> <li>The underlying algorithms will be similar to what we've learned.</li> <li>The algorithms will evolve with the state of the art</li> </ul> AutoML"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week1/#automl","title":"AutoML","text":"<p>If you wish to dive more deeply into AutoMLs, feel free to check out these cloud-based tools. You won\u2019t have to read these to complete this week\u2019s practice quizzes.</p> <ul> <li>Amazon SageMaker Autopilot</li> <li>Microsoft Azure Automated Machine Learning\\</li> <li>Google Cloud AutoML</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/","title":"Week 2 \u2014 Dimensionality Reduction","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#dimensionality-effect-on-performance","title":"Dimensionality Effect on Performance","text":"<p>A note about neural networks</p> <ul> <li>Yes, neural networks will perform a kind of automatic feature selection</li> <li>However, that's not as efficient as a well-designed dataset and model<ul> <li>Much of the model can be largely \"shut off\" to ignore unwanted features</li> <li>Even unused parts of the model consume space and compute resources</li> <li>Unwanted features can still introduce unwanted noise</li> <li>Each feature requires infrastructure to collect, store, and manage.</li> </ul> </li> </ul> <p></p> <pre><code># WORD EMBEDDING EXAMPLE\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nfrom keras.datasets import reuters\nfrom keras.preprocessing import sequence\n\nNUM_WORDS = 1_000 # Least repeated words are considered unknown\n\n(reuters_train_x, reuters_train_y), (reuters_test_x, reuters_test_y) =\n  tf.keras.dataset.reuters.load_data(num_words=NUM_WORDS)\nn_laberls = np.unique(reuters_train_y).shape[0]\n\n# Further preprocessing\nfrom keras.utils import np_utils\nreuters_train_y = np_utils.to_categorical(reuters_train_y, 46)\nreuters_test_y = np_utils.to_categorical(reuters_test_y, 46)\n\nreuters_train_x = keras.preprocessing.sequence.pad_sequence(\n  reuters_train_x, maxlen=20)\nreuters_test_x = keras.preprocessing.sequence.pad_sequence(\n  reuters_test_x, maxlen=20)\n\n# Using all dimensions\nfrom tensorflow.keras import layers\nmodel = keras.Sequential([\n    # The embedding is projected to 1000 dimesions here (2nd parameter)\n    layers.Embedding(NUM_WORDS, 1000, input_length=20),\n    layers.Flatten(),\n    layers.Dense(256),\n    layers.Dropout(0.25),\n    layers.Actiation('relu'),\n    layers.Dense(46),\n    layers.Activation('softmax')\n])\n\n# Model compilation and training\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\nmodel_1 = model.fit(reuters_train_x, reuters_train_y,\n                    validation_data=(reuters_test_x\n                    reuters_test_y),\n  batch_size=128, epochs=20, verbose=0)\n</code></pre> <p>tion=\"This model is overfitting as the model clearly perform poorly on validation set, and a high loss for validation set compared to training loss.\" &gt;}}</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#word-embeddings-6-dimensions-instead-of-1000","title":"Word embeddings: 6 dimensions instead of 1000","text":"<pre><code> model = keras.Sequential([\n    # The embedding is projected to 1000 dimesions here (2nd parameter)\n    layers.Embedding(NUM_WORDS, 6, input_length=20),\n    layers.Flatten(),\n    layers.Dense(256),\n    layers.Dropout(0.25),\n    layers.Actiation('relu'),\n    layers.Dense(46),\n    layers.Activation('softmax')\n])\n</code></pre> <p>tion=\"Although there is some overfitting, The newer model seems to perform better\" &gt;}}</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#curse-of-dimensionality","title":"Curse of Dimensionality","text":"<p>Many ML methods use the distance measures like KNN, SVM and recommendation systems.</p> <p>Most common being Euclidean Distance.</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#why-is-high-dimensional-data-a-problem","title":"Why is high-dimensional data a problem?","text":"<ul> <li>More dimension $\\rightarrow$ more features</li> <li>Risk of overfitting our models</li> <li>Distances grow more and more alike, vectors might appear equidistant from all others.</li> <li>No clear distinction between clustered objects</li> <li>Concentration phenomenon for Euclidean distance<ul> <li>Distribution of norms (distance between vectors) in a given distribution of points tends to concentrate.</li> </ul> </li> <li>Adding dimensions increases feature space volume</li> <li>Solutions take longer to converge, might get stuck in local optima</li> <li>Runtime and system memory requirements</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#why-are-more-features-bad","title":"Why are more features bad?","text":"<ul> <li>Redundant/irrelevant features</li> <li>More noise added than signal</li> <li>Hard to interpret and visualize</li> <li>Hard to store and process data</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#curse-of-dimensionality-in-the-distance-function","title":"Curse of dimensionality in the distance function","text":"<p>$$ d_{i,j} = \\sqrt{\\sum_{k=1}^n(x_{ik} - x_{jk})^2} \\tag{Euclidean distance} $$</p> <ul> <li>New dimensions add non-negative terms to the sum</li> <li>Distance increases with the number of dimensions</li> <li>For a given number of examples, the feature space becomes increasingly sparse</li> </ul> <p></p> <ul> <li>The size of the feature space grows exponentially as the number of features increases making it much harder to generalize efficiently.</li> <li>The variance increases, features might even be correlated and thus there are higher chances of overfitting to noise.</li> <li>The challenge is to keep as much of the predictive information as possible using as few features as possible.</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#the-hughes-effect","title":"The Hughes effect","text":"<p>The more the features, the large the hypothesis space.</p> <p></p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#curse-of-dimensionality-an-example","title":"Curse of Dimensionality: an Example","text":"<ul> <li>More features aren't better if they don't add predictive information</li> <li>Number of training instances needed increases exponentially with each added feature</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#what-do-ml-models-need","title":"What do ML models need?","text":"<ul> <li>No hard and fast rule on how many features are required</li> <li>Number of features to be used vary depending on the amount of training data available, the variance in that data, the complexity of the decision surface, and the type of classifier that is used. It can also depend on which features actually contain predictive information.</li> <li>Prefer uncorrelated data, with containing predictive information to produce correct results</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#manual-dimensionality-reduction","title":"Manual Dimensionality Reduction","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#increasing-predictive-performance","title":"Increasing predictive performance","text":"<ul> <li>Features must have information to produce correct results</li> <li>Derive feature from inherent features</li> <li>Extract and recombine to create new feature</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#why-reduce-dimensionality","title":"Why reduce dimensionality?","text":"<p>Dimensionality reduction looks for patterns and data to re express the data in a lower dimensional form.</p> <ul> <li>Reduce multicollinearity by removing redundant features.</li> </ul> <p></p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#feature-engineering","title":"Feature Engineering","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#manual-dimensionality-reduction-case-study-with-taxi-fare-dataset","title":"Manual Dimensionality Reduction: Case Study with Taxi Fare dataset","text":"<pre><code>CSV_COLUMNS = [\n    'fare_amount',\n    'pickup_datetime', 'pickup_longitude', 'pickup_latitude',\n    'dropoff_longitude', 'dropoff_lattidude',\n    'passenger_count', 'key'\n]\n\nLABEL_COLUMN = 'fare_amount'\nSTRING_COLS = ['pickup_datetime']\nNUMERIC_COLS = ['pickup_longitude', 'pickup_latitude',\n    'dropoff_longitude', 'dropoff_lattidude',\n    'passenger_count']\n\nDEFAULTS = [[0.0], ['na'], [0.0], [0.0], [0.0], [0.0], [0.0], ['na']]\nDAYS = ['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']\n\n# Build a baseline model using raw features\nfrom tensorflow.leras import layers\nfrom tensorflow.keras.metrics import RootMeanSquared as RMSE\n\ndnn_inputs = layers.DenseFeatures(features_columns.values())(inputs)\nh1 = layers.Dense(32, activation='relu', name='h1')(dnn_inputs)\nh2 = layers.Dense(8, activation='relu', name='h2')(h1)\noutput = layers.Dense(1, actiation='linear', name='fare')(h2)\n\nmodel = keras.models.Model(inputs, output)\nmodel.compile(optimizer='adam', loss='mse',\n              metrics=[RMSE(name='rmse'), 'mse'])\n</code></pre>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#increasing-model-performance-with-feature-engineering","title":"Increasing model performance with Feature Engineering","text":"<ul> <li>Carefully craft features for the data types<ul> <li>Temporal (pickup date &amp; time)</li> <li>Geographical (latitude and longitude)</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#handling-temporal-features","title":"Handling temporal features","text":"<pre><code>def parse_datetime(s):\n  if type(s) is not str:\n    s = s.numpy().decode('utf-8')\n  return datetime.datetime.strptime(s, \"%Y-%m-%d %H:%M:%S %Z\")\n\ndef get_dayofweek(s):\n  ts = parse_datetime(s)\n  return DAYS[ts.weekday()]\n\n@tf.function\ndef dayofweek(ts_in):\n  return tf.map_fn(\n    lambda s: tf.py_function(get_dayofweek, inp=[s],\n              Tout=tf.string),\n    ts_in)\n</code></pre>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#geological-features","title":"Geological features","text":"<pre><code>def euclidean(params):\n  lon1, lat1, lon2, lat2 = params\n  lodiff = lon2 - lon1\n  latdiff = lat2 - lat1\n  return tf.sqrt(londiff * longdiff + latdiff * latdif)\n</code></pre>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#scaling-latitude-and-longitude","title":"Scaling latitude and longitude","text":"<pre><code>def sclae_longitude(lon_column):\n  return (lon_column + 78)/8.   # Min: -70 | Max: +78\n\ndef scale_latitude(lat_column):\n  return (lat_column - 37/8.    # Min:  37 | Max: 45\n</code></pre>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#preparing-the-transformations","title":"Preparing the transformations","text":"<pre><code>def transform(inputs, numeric_cols, string_cols, nbuckets):\n  ...\n  feature_columns = {\n    colname: tf.feature_column.numeric_column(colname)\n      for colname in numeric_cols\n  }\n\n  for lon_col in ['pickup_longitude', 'dropoff_longitude']:\n    transformed[lon_col] = layers.Lambda(scale_longitude,\n        ...)(inputs[long_col])\n\n  for lat_col in ['pickup_latitude', 'dropoff_latitude']:\n    transformed[lat_col] = layers.Lambda(scale_latitude,\n        ...)(inputs[lat_col])\n</code></pre>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#bucketizing-and-feature-crossing","title":"Bucketizing and feature crossing","text":"<p>Unless the specific geometry of the earth is relevant to your data, a bucketized version of the map is likely to be more useful than the raw inputs.</p> <pre><code>def transform(inputs, numeric_cols, string_cols, nbuckets):\n  ...\n  latbucksts = np.linspace(0, 1, nbuckets).tolist()\n  lonbuckets = ...\n  b_plat = fc.bucketized_column(\n    feature_columns['pickup_latitude'], latbuckets)\n  b_dlat = # Bucketize 'dropoff_latitidue'\n  b_plon = # Bucketize 'pickup_longitude'\n  b_dlon = # Bucketize 'dropoff_longitude'\n\nploc = fc.cross_column([b_plat, b_plon], nbuckets * nbuckets)\ndloc = # Feature corss 'b_dlat' and 'b_dlon'\npd_pair = fc.crossed_column([ploc, dloc], nbuckets ** 4)\n\nFeature_columns['pickup_and_dropoff'] = fc.embedding_column(pd_pair, 100)\n</code></pre> <p></p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#algorithmic-dimensionality-reduction","title":"Algorithmic Dimensionality Reduction","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#linear-dimensionality-reduction","title":"Linear dimensionality reduction","text":"<ul> <li>Linearly project $n$-dimensional data onto a $k$-dimensional subspace ($k &lt; n$, often $k &lt;&lt; n$)</li> <li>There are infinitely many $k$-dimensional subspace we can project the data onto</li> <li>Which one should we choose?</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#projecting-onto-a-line","title":"Projecting onto a line","text":"<ul> <li>Let's thing of features as vectors existing in a high-dimensional space.</li> <li>Vectors being high dimensional is difficult to visualize, but if we project onto a lower dimension, allows us to visualize the data more easily. This is referred to as embedding.</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#best-k-dimensional-subspace-for-projection","title":"Best k-dimensional subspace for projection","text":"<p>Classification: maximum separation among classes</p> <p>Example: Linear Discriminant analysis (LDA)</p> <p>Regression: maximize correlation between projected data and output variable</p> <p>Example: Partial Least Squares (PLS)</p> <p>Unsupervised: retain as much data variance as possible</p> <p>Example Principal component analysis (PCA)</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#principal-components-analysis","title":"Principal Components Analysis","text":"<ul> <li>Relies on eigen-decomposition (which can only be done for square matrices)</li> <li>PCA is a minimization of the orthogonal distance</li> <li>Widely used method for unsupervised &amp; linear dimensionality reduction</li> <li>Accounts for variance of data in as few dimensions as possible using linear projections</li> </ul> <p>PCA performs dimensionality reduction in two steps:</p> <ol> <li> <p>PCA rotates the samples so that they are aligned with the coordinate axis.</p> <p>PCA also shifts the samples so that they have a mean of zero</p> </li> </ol>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#principal-components-pcs","title":"Principal components (PCs)","text":"<ul> <li>PCs maximize the variance of projections</li> <li>PCs are orthogonal</li> <li>Gives the best axis to project</li> <li>Goal of PCA: Minimize total squared reconstruction error</li> </ul> <p> Repeat until we a have k orthogonal lines</p> <pre><code>from sklearn.decomposition import PCA\n\n# PCA that will retain 99% of the variance\npca = PCA(n_components=0.99, whiten=True)\npca.fit(X)\nX_pca = pca.transform(X)\n</code></pre>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#plot-the-explained-variance","title":"Plot the explained variance","text":"<pre><code>tot = sum(pca.e_vals_)\nvar_exp = [(i / tot) * 100 for i in sorted (pca.e_evals_, reverse=True)\ncum_var_exp = np.cumsum(var_exp)\n</code></pre>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#pca-factor-loadings","title":"PCA factor loadings","text":"<p>The factor loadings are the unstandardized values of the eigenvectors.</p> <p><pre><code>loadings = pca.e_vecs_ * np.sqrt(pca.e_vals_)\n</code></pre> </p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#when-to-use-pca","title":"When to use PCA?","text":"<p>Strengths:</p> <ul> <li>A versatile technique</li> <li>Fast and simple</li> <li>Offers several variations and extensions (e.g., kernel/sparse PCA)</li> </ul> <p>Weaknesses:</p> <ul> <li>Result is not interpretable</li> <li>Requires setting threshold for cumulative explained variance</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#other-techniques","title":"Other Techniques","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#unsupervised","title":"Unsupervised","text":"<ul> <li>Latent Semantic Indexing/ Analysis (LSI and LSA) (SVD)</li> <li>Independent Component Analysis (ICA)</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#matrix-factorization","title":"Matrix Factorization","text":"<ul> <li>Non-Negative Matrix Factorization (NMF)</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#latent-methods","title":"Latent Methods","text":"<ul> <li>Latent Dirichlet Allocation (LDA)</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#singular-value-decomposition-svd","title":"Singular Value Decomposition (SVD)","text":"<ul> <li>SVD decomposes non-square matrices</li> <li>Useful for sparse matrices and matrices that are not square matrices as produced by TF-IDF.</li> <li>Removes Redundant features from the dataset</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#independent-component-analysis","title":"Independent Component Analysis","text":"<ul> <li>PCA seeks directions in feature space that minimize reconstructions error, or for uncorrelated factors.</li> <li>ICA seeks directions that are most statistically independent</li> <li>ICA addresses higher order dependence</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#how-does-ica-work","title":"How does ICA work?","text":"<ul> <li>Assume there exists independent signals:<ul> <li>$S = [s_1(t), s_2(t), ..., s_N(t)]$</li> </ul> </li> <li>Linear combinations of signals: $Y(t) = A S(t)$<ul> <li>Both $A$ and $S$ are unknown</li> <li>$A$ - mixing matrix</li> </ul> </li> <li>Goal of ICA: recover original signal, $S(t)$ from $Y(t)$</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#comparing-pca-and-ica","title":"Comparing PCA and ICA","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#non-negative-matrix-factorization-nmf","title":"Non-negative Matrix Factorization (NMF)","text":"<ul> <li>NMF model are interpretable and easier to understand</li> <li>NMF requires the sample features to be non-negative</li> <li>NMF models are interpretable but it can't be applied to all datasets<ul> <li>It requires the sample features to non-negative.</li> </ul> </li> </ul> Dimensionality Reduction Techniques"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#dimensionality-reduction-techniques","title":"Dimensionality Reduction Techniques","text":"<p>If you wish to dive more deeply into dimensionality reduction techniques, feel free to check out these optional references. You won\u2019t have to read these to complete this week\u2019s practice quizzes.</p> <ul> <li>Principal Component Analysis (PCA)</li> <li>Independent Component Analysis (ICA)</li> <li>PCA extensions</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#quantization-and-pruning-mobile-iot-and-similar-use-cases","title":"Quantization and Pruning \u2014 Mobile, IoT, and Similar Use Cases","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#factors-driving-new-trend-of-edge-computing","title":"Factors driving new trend of edge computing","text":"<ul> <li>Demands move ML capability from cloud to on-device</li> <li>Cost-effectiveness</li> <li>Compliance with privacy regulations</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#online-ml-inference","title":"Online ML inference","text":"<ul> <li>To generate real-time predictions you can:<ul> <li>Host the model on a server</li> <li>Embed the model in the device</li> </ul> </li> <li>Is it faster on a server, or on-device?</li> <li>Mobile processing limitations?</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#mobile-inference","title":"Mobile inference","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#model-development","title":"Model development","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#benefits-and-process-of-quantization","title":"Benefits and Process of Quantization","text":"<p>Quantization involves transforming a model into an equivalent representation that uses parameters and computations at a lower precision.</p> <p>This improves model execution performance and efficiency but at the cost of slightly lower model accuracy.</p> <p> Source</p> <p>Think of a picture, which is a grid of pixels, each pixel has a certain number of bits. Now if we try reducing the continuous color spectrum of real life to discrete colors, we're quantizing or approximating the image.</p> <p>Beyond a certain point it may get harder to recognize what the data/image really is.</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#why-quantize-neural-netowrks","title":"Why quantize neural netowrks?","text":"<ul> <li>Neural networks have many parameters and take up space</li> <li>Shrinking model file size</li> <li>Reduce computational resources</li> <li>Make models run faster and use less power with low-precision</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#mobilenets-latency-vs-accuracy-trade-off","title":"MobileNets: Latency vs Accuracy trade-off","text":"<p>MobileNets are family of small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases.</p> <p></p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#benefits-of-quantization","title":"Benefits of quantization","text":"<ul> <li>Faster compute</li> <li>Low memory bandwidth</li> <li>Low power</li> <li>Integer operations supported across CPU/DSP/NPUs</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#the-quantization-process","title":"The quantization process","text":"<p>The weights and activations for a particular layer often tend to lie in a small range, which can be estimated beforehand. That's why we don't need to store the range in the same data type. Therefore we can concentrate the fewer bits within a smaller range.</p> <p>Find the maximum absolute weight value, $m$, then maps the floating point range $-m$ to $+m$ to the fixed-point range $-127$ to $+127$.</p> <p></p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#what-parts-of-the-mode-are-affected","title":"What parts of the mode are affected?","text":"<ul> <li>Static parameters (like weights of the layers)</li> <li>Dynamix parameters (like activations inside networks)</li> <li>Computation (transformations)</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#trade-offs","title":"Trade-offs","text":"<ul> <li>Optimizations impact model accuracy<ul> <li>Difficult to predict ahead of time</li> </ul> </li> <li>In rare cases, models may actually gain some accuracy</li> <li>Undefined effects on ML interpretability.</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#choose-the-best-model-for-the-task","title":"Choose the best model for the task","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#post-training-quantization","title":"Post Training Quantization","text":"<p>In this technique an already trained TensorFlow model size is reduced by using TensorFlow Lite converter to save into TensorFlow Lite format.</p> <ul> <li>Reduced precision representation</li> <li>Incur small loss in model accuracy</li> <li>Joint optimization for mode and latency</li> </ul> <p></p> <ul> <li>In dynamic range quantization, during inference the weights are converted back from eight bits to floating point and activations are computed using floating point kernels.</li> <li>This conversion is done once, cached to reduce latency</li> </ul> <pre><code>import tensorflow as tf\n\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\ncoverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n\ntflite_quant_model = converter.convert()\n</code></pre>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#model-accuracy","title":"Model accuracy","text":"<ul> <li>Small accuracy loss incurred (mostly for smaller network)</li> <li>Use the benchmarking tools to evaluate model accuracy</li> <li>If the loss of accuracy drop is not within acceptable limits, consider using quantization-aware training</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#quantization-aware-training","title":"Quantization Aware Training","text":"<p>Quantization  aware training adds fake quantization operations to the mode so it can learn to ignore the quantization noise during training.</p> <ul> <li>Inserts fake quantization (FQ) nodes in the forward pass</li> <li>Rewrites the graph to emulate quantized inference</li> <li>Reduces the loss of accuracy due to quantization</li> <li>Resulting model contains all data to be quantized according to spec</li> </ul> <p></p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#qat-on-entire-model","title":"QAT on entire model","text":"<pre><code>import tensorflow_model_optimization as tfmot\n\nmodel = tf.keras.Sequential([\n    ...\n])\n\n# Qunatize the entire model.\nquntized_model = tfmot.quantization.keras.quantize_model(model)\n\n# Continue with training as usual\nquantized_model.compile(...)\nquantized_model.fit(...)\n</code></pre>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#quantize-partss-of-a-model","title":"Quantize parts(s) of a model","text":"<pre><code>import tensorflow_model_optimization as tfmot\nquanitze_annotate_layer = tfmot.quanitzation.keras.quanitze_annotate_kayer\n\nmodel = tf.keras.Sequential([\n    ....\n    # Only anotated layers will be quantized\n    quantize_annotate_layer(Conv2D()),\n    quantize_annotate_layer(ReLU()),\n    Dense(),\n    ...\n])\n\n# Quantize the model\nquantized_model = tfmot.quantization.keras.quantize_apply(model)\n</code></pre>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#quantize-custom-keras-layer","title":"Quantize custom Keras layer","text":"<pre><code>quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\nquantize_annotate_model = tfmot.quantization.leras.quantize_annotate_model\nquantize_scope = tfmot.quatization.keras.quantize_scope\n\nmodel = quantize_annotate_model(tf.keras.Sequential([\n    quantize_annotate_layer(CustomLayer(20, input_shape=(20,)),\n                                                    DefaultDenseQuantizeConfig()),\n    tf.keras.layers.Flatten()\n]))\n\n# `quantize_apply` requires mentioning `DefaultDenseQuantizeConfig` with\n# `quantize_scope`\nwith quantize_scope(\n    {'DefaultDenseQuantizeConfig': DefaultDenseQuantizeConfig,\n     'CustomLayer': CustomLayer}):\n    # Use 'quantize_apply` to actually make the model quantization aware\n    quant_aware_model = tfmot.quantization.keras.quantize_apply(model)\n</code></pre> Quantization"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#quantization","title":"Quantization","text":"<p>If you wish to dive more deeply into quantization, feel free to check out these optional references. You won\u2019t have to read these to complete this week\u2019s practice quizzes.</p> <ul> <li>Quantization</li> <li>Post-training quantization</li> <li>Quantization aware training</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#pruning","title":"Pruning","text":"<p>Pruning increases the efficiency of model by removing parts (connections) of model that do not contribute to substantially to producing accurate results.</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#model-sparsity","title":"Model Sparsity","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#origins-of-weight-pruning","title":"Origins of weight pruning","text":"<p>The first major paper advocating sparsity and neural networks dates back from 1990, in \"Optimal of Brain Damage\" written by Yann LeCun, John S.Denker, and Sara A. Solla.</p> <p>At that time post pruning NN was already a trendy choice to reduce the size of models. runing was mainly done by using magnitude as an approximation for saliency to determine less useful connections.</p> <ul> <li>Intuition being smaller the weight smaller the effect was on the output.</li> <li>The saliency of each weight was estimated, defined by change in the loss function upon applying a perturbation to the nodes in the network. Finally retraining the model again.</li> <li>But retraining became a lot harder.</li> <li>The answer came with \"lottery ticket hypothesis\"</li> </ul> <p></p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#lottery-ticket-hypothesis","title":"Lottery ticket Hypothesis","text":"<p>As the network size increases, the number of possible subnetworks and the probability of finding the \u2018lucky subnetwork\u2019 also increase. As per the lottery ticket hypothesis, if we find this lucky subnetwork, we can train small and sparsified networks to give higher performance even when 90 percent of the full network\u2019s parameters are removed.</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#finding-sparse-neural-networks","title":"Finding Sparse Neural Networks","text":"<p>\"A randomly-initialized, dense neural network contains a subnetwork that is initialized such that \u2014 when trained in isolation \u2014 it can match the test accuracy of the original network after training for at most the same number of iterations\"                                           - Jonathan Frankle and Michael Carbin </p> <p>Basically instead of fine-tuning weights, just reset the weight to original value and retain.</p> <p>This lead to the acceptance of the idea that over parameterized dense networks containing several sparse subnetworks with varying performances, and one of these subnetworks is the winning ticket, which perform all others.</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#pruning-research-is-evolving","title":"Pruning research is evolving","text":"<ul> <li>The new method didn't perform well at large sacle</li> <li>The new method failed to identify the randomly initialized winners</li> <li>Active area of research</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#eliminate-connections-based-on-their-magnitude","title":"Eliminate connections based on their magnitude","text":"<p>TensorFlow includes a weight pruning API, which can iteratively remove connections based on their magnitude during training.</p> <p> </p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#sparsity-increases-with-training","title":"Sparsity increases with training","text":"<p>tion=\"Image source: TensorFlow Model Optimization Toolkit\u200a\u2014\u200aPruning API (morioh.com) Black cells indicate where the non-zero weights exist as pruning is applied to a tensor.\" &gt;}}</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#whats-special-about-pruning","title":"What's special about pruning?","text":"<ul> <li>Better storage and/or transmission</li> <li>Gain speedups in CPU and some ML accelerators</li> <li>Can be used in tandem with quantization to get additional benefits</li> <li>Unlock performance improvements</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#pruning-with-tf-model-optimization-toolkit-iwth-keras","title":"Pruning with TF Model Optimization Toolkit iwth Keras","text":"<p> <pre><code>import tensorflow_model_optimization as tfmot\n\nmodel = build_your_model()\npruning_schedule = tfmot.sparsity.keras.PolynomialDecay(\n                                            initial_sparsity=0.5, final_sparsity=0.8,\n                                            begin_Step=2000, end_step=4000)\n\nmodel_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(\n                                            model,\n                                            pruning_scehdule=pruning_schedule)\n\n...\nmodel_for_pruning.fit()\n</code></pre></p> <p></p> Pruning"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week2/#pruning_1","title":"Pruning","text":"<p>If you wish to dive more deeply into pruning, feel free to check out these optional references. You won\u2019t have to read these to complete this week\u2019s practice quizzes.</p> <ul> <li>Pruning</li> <li>The Lottery Ticket Hypothesis</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/","title":"Week 3 \u2014 High Performance Modeling\"","text":"<p>Knowledge Distillation, Data Parallelism, ETL Process</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#distributed-training","title":"Distributed Training","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#rise-in-coputational-requirements","title":"Rise in coputational requirements","text":"<ul> <li>At first, training models is quick and easy</li> <li>Training models becomes more time-consuming<ul> <li>With more data</li> <li>With larger models</li> </ul> </li> <li>Longer training \u2192 More epochs \u2192 Less efficient</li> <li>Use distributed training approaches</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#types-of-distrubued-training","title":"Types of distrubued training","text":"<ul> <li>Data parallelism: In data parallelism, models are replicated onto different accelerators (GPU/TPU) and data is split between them.<ul> <li>Model agnostic i.e., can be applied to any neural architecture</li> </ul> </li> <li>Model parallelism: When models are too large to fit on a singe device then they can be divided into partitions, assigning different partitions to different accelerators.</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#data-parallelism","title":"Data parallelism","text":"<p>The model is copied to workers, and data is split between them. That means workers should have enough memory to store the model and perform the forward and backward pass. After backprop the changes are shared among the workers and weights are updated.</p> <p></p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#distributed-training-using-data-parallelism","title":"Distributed training using data parallelism","text":"<ol> <li>Synchronous training<ul> <li>All workers train and complete updated in sync</li> <li>Supported via all-reduce architecture</li> </ul> </li> <li>Asynchronous training<ul> <li>Each worker trains and completes updates separately</li> <li>Supported via parameter server architecture</li> <li>More efficient, but can result in lower accuracy and slower convergence.</li> </ul> </li> </ol>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#making-your-model-distribute-aware","title":"Making your model distribute-aware","text":"<ul> <li>If you want to distribute a model:<ul> <li>Supported in high-level APIs such as Keras/Esstimators</li> <li>For more control, you can use custom training loops</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#tfdistributestrategy","title":"<code>tf.distribute.Strategy</code>","text":"<ul> <li>Library in TensorFlow for running a computation in multiple devices</li> <li>Supports distribution strategies for high-level APIs like Keras and custom training loops</li> <li>Convenient to use with little or no code changes.</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#distribution-strategies-supported-by-tfdistributestrategy","title":"Distribution Strategies supported by <code>tf.distribute.Strategy</code>","text":"<ul> <li>One Device Strategy</li> <li>Mirrored Strategy</li> <li>Parameter Server Strategy</li> <li>Multi-Worker Mirrored Strategy</li> <li>Central Storage Strategy</li> <li>TPU Strategy</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#one-device-strategy","title":"One Device Strategy","text":"<ul> <li>Single device - no distribution</li> <li>Typical usage of this strategy is testing your code before switching to other strategies that actually distribute your code.</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#mirrored-strategy","title":"Mirrored Strategy","text":"<p>This strategy is typically used on one machine with multiple GPUs.</p> <ul> <li>Supported synchronous distributed training.</li> <li>Creates a replica per GPU &lt;&gt; Variables are mirrored</li> <li>Weight updating is done using efficient cross-device communication algorithms (all-reduce algorithms)</li> </ul> <p>Fault tolerance</p> <ul> <li>If a worker gets interrupted, the whole cluster will pause, wait for interrupted worker to restart and then other worker will also restart. After that then can resume from their former state</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#parameter-server-strategy","title":"Parameter Server Strategy","text":"<ul> <li>Some machines are designed as workers and others as parameter servers.<ul> <li>Parameter servers store variables so that workers can perform computations on them.</li> </ul> </li> <li>Implements asynchronous data parallelism by default.</li> </ul> <p>Fault tolerance</p> <ul> <li>Catastrophic failures in one worker would cause failure of distribution strategies.</li> <li>How to enable fault tolerance in case a worker dies?<ul> <li>By restoring training state upon restart from job failure.</li> <li>Keras implementation: <code>BackupAndRestore</code> callback.</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#high-performance-ingestion","title":"High Performance Ingestion","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#why-input-pipelines","title":"Why input pipelines?","text":"<p>Data at times can't fit into memory and sometimes, CPUs are under-utilized in compute intensive tasks like training a complex model.</p> <p>You should avoid these inefficiencies so that you can make the most of the hardware available $\\rightarrow$ Use input pipelines.</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#tfdata-tensorflow-input-pipeline","title":"tf.data: TensorFlow Input Pipeline","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#an-improved-etl-process","title":"An improved ETL process","text":"<p>Overlap different parts of ETL by using a technique known as software pipelining. </p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#pipelining","title":"Pipelining","text":"<p>Through pipelining we can overcome CPU bottlenecks by overlapping CPU pre-processing and model execution by accelerators.</p> <p>With pipeline while GPU is training, the CPU starts preparing data for next training step, thus decreasing the idle time.</p> <p></p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#how-to-optimize-pipeline-performance","title":"How to optimize pipeline performance?","text":"<ul> <li>Prefetching</li> <li>Parallelize data extraction and transformation</li> <li>Caching</li> <li>Reduce Memory</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#optimize-with-prefetching","title":"Optimize with prefetching","text":"<p>Overlap the work of a producer with a work of a consumer. </p> <p>This transformation uses a background thread and an internal buffer to prefetch elements from the input dataset ahead of time before they are requested.</p> <pre><code>benchmark(\n    ArtificialDataset()\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n</code></pre> <p>Ideally the number of elements to prefetch should be equal to or greater than the number of batches consumed by a single training step.</p> <p>We can manually tune this value, or set it to <code>tf.data.experimental.AUTOTUNE</code></p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#parallelize-data-extraction","title":"Parallelize data extraction","text":"<p>A pipeline that run locally might become bottleneck when the data is stored remotely like in cloud or in HDFS due to:</p> <ul> <li>Time-to-first-byte: Prefer local storage as it takes significantly longer to read data from remote storage.</li> <li>Read throughput: Maximize the aggregate bandwidth of remote storage by reading more files instead of single one.</li> </ul> <p></p> <p>Parallelize the data loading step including interleaving contents of other datasets.</p> <pre><code>benchmark(\n    tf.data.Dataset.range(2)\n    .interleave(\n        ArtificialDataset,\n        num_parallel_calls=tf.data.experimental.AUTOTUNE # Level of parallelis\n    )\n)\n</code></pre>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#parallelize-data-transformation","title":"Parallelize data transformation","text":"<p>Often times, the input elements may need to be preprocessed.</p> <ul> <li>Post data loading, the inputs may need preprocessing</li> <li>Element-wise preprocessing can be parallelized across CPU cores</li> <li>The optimal value for the level of parallelism depends on:<ul> <li>Size and shape of training data</li> <li>Cost of the mapping transformation</li> <li>Load the PCU is experiencing currently</li> </ul> </li> <li>With <code>tf.data</code> you can use <code>AUTOTUNE</code> to set parallelism automatically</li> </ul> <p></p> <pre><code>benchmark(\n    ArtificialDataset()\n    .map(\n        mapped_function,\n        num_paralllel_calls=tf.data.AUTOTUNE\n    )\n)\n</code></pre>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#improve-training-time-with-caching","title":"Improve training time with caching","text":"<ul> <li>In-memory: <code>tf.data.Dataset.cache()</code></li> <li>Dish: <code>tf.data.Dataset.cache(filename=...)</code></li> </ul> <p>If user defined mapping transformation function is expensive it makes sense to cache the dataset as long as it fits in memory.</p> <p>If the user-defined function increases the space required to store the dataset beyond the cache capacity either applied it after cache transformation or consider pre-processing the data before your training job to reduce the resource requirement.</p> <pre><code>benchmark(\n    ArtificialDatset().map(mapped_function).cache(), 5\n)\n</code></pre>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#training-large-models-the-rise-of-giant-neural-nets-and-parallelism","title":"Training Large Models - The Rise of Giant Neural Nets and Parallelism","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#rise-of-giant-neural-networks","title":"Rise of giant neural networks","text":"<ul> <li>In 2014, the ImageNet winner was GoogleNet with 4 million parameters and scoring a 74.8% top-1 accuracy.</li> <li>In 2017, Squeeze-and-excitation networks achieved 82.7% top-1 accuracy with 145.8 million parameters.</li> </ul> <p>36 fold increase in the number of parameters in just 3 years!</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#issues-with-training-larger-networks","title":"Issues with training larger networks","text":"<ul> <li>GPU memory only increased by factor ~3</li> <li>Saturated the amount of memory available in Cloud TPUs</li> <li>Need for large-scale training of giant neural networks</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#overcoming-memory-constraints","title":"Overcoming memory constraints","text":"<ol> <li> <p>Strategy #1 - Gradient Accumulation</p> <ul> <li>Split batches into mini-batches and only perform backprop after whole batch.</li> </ul> <p>During backprop, the model isn't updated with each mini-batch, and instead, the gradients are accumulated. When the batch completes, all the accumulated gradients of the mini-batches are used for the backprop to update the model.</p> </li> <li> <p>Strategy #2 - Memory swap</p> <ul> <li>Copy activations between CPU and memory, back and forth.</li> </ul> <p>Since there isn't enough storage on the accelerator, you copy activations back to memory and then back to the accelerator. But this is slow.</p> </li> </ol>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#parallelism-revisited","title":"Parallelism revisited","text":"<p>Data parallelism: In data parallelism, models are replicated onto different accelerators (GPU/TPU) and data is split between them.</p> <p>Model parallelism: When models are too large to fit on a single device then they can be divided into partitions, assigning different partitions to different accelerators.</p> <p>Sophisticated methods for model parallelism make sure that each worker is similarly busy by analyzing the computational complexity of each layer.</p> <p>This enables training of larger networks but suffers from a large hit in performance since workers are constantly waiting for each other and only one can perform updates at a given time.</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#challenges-in-data-parallelism","title":"Challenges in data parallelism","text":"<p>Two factors contribute to the communication overhead across all models:</p> <ol> <li>An increase in data parallel workers</li> <li>An increase in GPU compute capacity</li> </ol> <p>The frequency of parameter synchronization affects both statistical and hardware efficiency:</p> <ul> <li> <p>Synchronizing at the end of every mini-batch reduces the staleness of weights used to compute gradients ensuring good statistical efficeincy.</p> <p>But this requires each GPUs to wait for gradients from other GPUs, i.e., impacting hardware efficiency.</p> </li> </ul> <p>Communication stalls also become inevitable in data parallelism making the communication can often dominate total execution time.</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#challenges-keeping-accelerators-busy","title":"Challenges keeping accelerators busy","text":"<ul> <li>Accelerators have limited memory</li> <li>Model parallelism: large networks can be trained<ul> <li>But, accelerator compute capacity is underutilized (Other accelerators might have to wait until previous one finishes)</li> </ul> </li> <li>Data parallelism: train same model with different input data<ul> <li>But, the maximum model size an accelrator can support is limited.</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#pipeline-parallelism-using-gpipe","title":"Pipeline parallelism using GPipe","text":"In this diagram, F refers to the forward pass while B refers to the backward pass distributed among the accelerators <p>Google's GPipe is an open source library for efficiently training large-scale models using pipeline parallelism.</p> <p>In the diagram at the bottom, the GPipe divides the input mini-batch into smaller micro-batches, enabling different accelerators to work on separate micro-batches at the same time.</p> <p>It inserts communication primitives at the partition boundaries</p> <p>Automatic parallelism to reduce consumption</p> <p>Synchronous stochastic Gradient accumulation across micro-batches, sot that model quality is preserved.</p> <ul> <li>Integrates both data and model parallelism<ul> <li>Divide mini-batch data into micro-batches</li> <li>Different workers work on different micro-batches in parallel</li> <li>Allow ML models to have significantly more parameters</li> </ul> </li> </ul> High-Performance Modeling <p>If you wish to dive more deeply into  high-performance modeling, feel free to check out these optional references. You won\u2019t have to read these to complete this week\u2019s practice quizzes.</p> <ul> <li>Distributed training</li> <li>Data parallelism</li> <li>Pipeline parallelism</li> <li>GPipe</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#teacher-and-student-networks","title":"Teacher and Student Networks","text":"<p>We can also try to capture or distill the knowledge that has been learned by a model in a more compact model by a style of learning known as Knowledge Distillation.</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#sophisticated-models-and-their-problems","title":"Sophisticated models and their problems","text":"<ul> <li>Larger sophisticated models become complex</li> <li>Complex models learn complex tasks</li> <li>Can we express this learning more efficiently?</li> </ul> <p>Is it possible to 'distill' or concentrate this complexity into smaller networks?</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#knowledge-distillation","title":"Knowledge distillation","text":"<p>Knowledge distillation is a way to train a small model to mimic a larger model or even ensemble of models by first training a complex model achieving a high level of accuracy and then using that model as teacher model for a simpler student model.</p> <ul> <li>Duplicate the performance of a complex model in a simpler model</li> <li>Idea: Create a simple 'student' model that learns from a complex 'teacher' model</li> </ul> <p> Source: [Giang Nguyen](https://github.com/giangnguyen2412/paper-review-continual-learning/blob/master/distillation.md</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#knowledge-distillation-techniques","title":"Knowledge Distillation Techniques","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#teacher-and-student","title":"Teacher and student","text":"<ul> <li>Training objectives of the models vary</li> <li>Teacher (normal training)<ul> <li>maximises the actual metric</li> </ul> </li> <li>Student (knowledge transfer)<ul> <li>matches p-distribution of the teacher's predictions to form 'soft targets' (The probability of the predictions)</li> <li>'Soft targets' tell us about the knowledge learned by the teacher</li> </ul> </li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#transferring-dark-knowledge-to-the-student","title":"Transferring \"dark knowledge\" to the student","text":"<p>The way knowledge distillation works is that you transfer knowledge form the teacher to the student by minimizing the loss function in which the target is the distribution of class probabilities predicted by the teacher model</p> <p>The teacher model's logits from the input to the final softmax layer</p> <ul> <li>Improve softness of the teacher's distribution with 'softmax temperature' ($T$)</li> <li>By raising the temperature in the objective functions of the student and teacher, we can improve the softness of the teacher's distribution</li> </ul> <p>As $T$ grows, you get more insight about  which classes the teacher finds similar to the predicted one.</p> <p>As $T$ decreases, the soft target defined by the teacher network become less informative.</p> <p>As $T$ =1, It's a normal softmax function.</p> <p>$$ p_i = \\frac{\\exp(\\frac{z_i}{T})}{\\sum_j\\exp(\\frac{z_j}{T})} $$</p> <p>The probability $p$ of class $i$ is calculated from the logits $z$ as shown. The $T$ refers to the temperature parameter.</p> <ul> <li>When T is 1, it acts like standard softmax function</li> <li> <p>As $T$ starts growing the probability distribution generated by the softmax function becomes softer, providing more information as to which classes the teacher found more similar to the predicted class.</p> <p>Authors of the paper referred it to as \"dark knowledge\" embedded in the teacher model.</p> </li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#techniques","title":"Techniques","text":"<p>Various techniques can be employed to train students to match the teacher's soft targets.</p> <ul> <li> <p>Approach #1: Weight objectives (student and teacher) and combine during backprop</p> <p>Train using both teacher's logits and training labels using normal objective functions, the two objective functions are weighted and combined in backpropagatation.</p> </li> <li> <p>Approach #2: Compare distributions of the predictions (Student and teacher) using KL divergence.</p> <p>The distributions of the teacher's predictions and teacher's predictions are compared using metric such as KL divergence.</p> </li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#kl-divergence","title":"KL divergence","text":"<p>$$ L = (1-\\alpha ) L_H + \\alpha L_{KL} $$</p> <p>The Kullback-Leibler divergence here is a metric of the difference between two probability distributions.</p> <p>Generally knowledge is done by blending two loss functions, choosing a value for $\\alpha$ between zero and one and we want the probability distributions of predictions of teacher and student model to be close as possible.</p> <p>Here :</p> <ul> <li>$L$ is the cross-entropy loss from the hard labels</li> <li>$L_{KL}$ is the Kullabck-Leibler divergence loss from the divergence logits of teacher</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#how-knowledge-transfer-takes-place","title":"How knowledge transfer takes place","text":"<p>When computing the \"standard\" loss between student's prdicted class probabilities and the ground truth \"hard\" labels, we use a value of the softmax temperature $T$ equal to 1.</p> <p>In case of heavy data augmentations after training the teacher network, the alpha hyperparmeter should be high in the student network loss function otherwise low alpha parameter would increase the influence of the hard labels that went through aggressive perturbations due to data augmentation.</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#case-study-how-to-distill-knowledge-for-a-qa-task","title":"Case Study - How to Distill Knowledge for a Q&amp;A Task","text":"<p>Traditional model compression suffer from information loss leading to inferior models. To tackle this researchers at Microsoft proposed a two stage multi teacher knowledge distillation method (TMKD) for a web question answering system.</p> <p></p> <ul> <li>They first developed a general Q&amp;A distillation task for student model pretraining (right side of diagram), and further fine tune this pre trained student model with a multi-teacher knowledge distillation model (left side of diagram).</li> <li>Although the process can effectively reduce the number of parameters and time inference, due to the information loss during knowledge distillation, the performance of the student model is sometimes not on par with that of the teacher.</li> <li>This lead of different approach called M on M or many on many ensemble model. Combining both ensemble and knowledge distillation.</li> <li>This involves first training multiple teacher models. Then a student model for each teacher model is then trained. Finally, the student models trained from different teachers are ensembled to create the final result.</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#make-efficientnets-robust-to-noise-with-distillation","title":"Make EfficientNets robust to noise with distillation","text":"<p>Carnegie Mellon University trained models with a semi supervised learning method called noisy student:</p> <ul> <li>In this approach, the knowledge distillation process is iterative.</li> <li>The student is purposely kept larger in terms of the number of parameters than the teacher, this enables the model to attain robustness to noisy labels as opposed to traditional knowledge distillation approach.</li> </ul> <p></p> <p>The noising is what pushes the student model to learn harder from pseudo labels.</p> <p>The teacher model is not noised during the generation of pseudo labels to ensure it's accuracy isn't altered in any way.</p> Knowledge Distillation"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week3/#knowledge-distillation_1","title":"Knowledge Distillation","text":"<p>If you wish to dive more deeply into knowledge distillation, feel free to check out these optional references. You won\u2019t have to read these to complete this week\u2019s practice quizzes.</p> <ul> <li>Knowledge distillation</li> <li>Q&amp;A case study</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/","title":"Week 4 \u2014 Model Performance Analysis","text":"<p>TF Model Analysis, model debugging, benchmarking, fairness, residual analysis, continuous evaluation and Monitoring\"</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#model-performance-anlysis","title":"Model Performance Anlysis","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#whats-next-after-model-trainingdeployment","title":"What's next after model training/deployment?","text":"<ul> <li>Is model performaing well?</li> <li>Is there scope for improvement?</li> <li>Can the data change in future?</li> <li>Has the data changed since you created your training dataset?</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#black-box-evaluation-vs-model-introspection","title":"Black box evaluation vs model introspection","text":"<p>Black box evaluation</p> <ul> <li>Don't consider the internal structure of the model</li> <li>Models can be tested for metrics like accuracy and losses like test error without knowing internal details.</li> <li>For finer evaluation, models can be inspected part by part.</li> <li>TensorBoard is a tool for black-box evaluation.</li> </ul> <p>Model introspection</p> <ul> <li>On left side, the maximally activated patches of various filters in Convolutional layer of the model are shown. Using these patterns we can inspect that at which layer the model is learning a particular structure of your data.</li> <li> <p>On the right, an example of a class activation map is shown.</p> <p>You might be interested in knowing which parts of the image are primarily responsible for making the desired prediction for a particular class.</p> </li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#performance-metrics-vs-optimization-objectives","title":"Performance metrics vs optimization objectives","text":"<p> tion=\"Source: https://cs231n.github.io/neural-networks-3/\"&gt;}}</p> <p>The above gif shows different optimization function finding to optima value for loss function, on loss surface.</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#top-level-aggregate-metrics-vs-slicing","title":"Top level aggregate metrics vs slicing","text":"<ul> <li>Most of the time, metrics are calculated on the entire dataset</li> <li>Slicing deals with understanding how the model is performing on each subset of data.</li> </ul> TensorBoard"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#tensorboard","title":"TensorBoard","text":"<p>If you wish to dive more deeply into TensorBoard usage for model analysis feel free to check out this optional reference. You won\u2019t have to read these to complete this week\u2019s practice quizzes.</p> <p>TensorBoard</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#advanced-model-analysis-and-debugging-introduction-to-tensorflow-model-analysis-tfma","title":"Advanced Model Analysis and Debugging \u2014 Introduction to TensorFlow model Analysis (TFMA)","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#why-should-you-slice-your-data","title":"Why should you slice your data?","text":"<p>Your top-level metrics may hide problems</p> <ul> <li>Your model may not perform well for particular [customers | products | stores | days of the week | etc.]</li> </ul> <p>Each prediction request is an individual event, maybe an individual customer</p> <ul> <li>For example, customers may have a bad experience</li> <li>For example, some stores may perform badly</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#tensorflow-model-analysis-tfma","title":"TensorFlow Model Analysis (TFMA)","text":"<p>TFMA is a open source, scalable, and a versatile tool for doing deep analysis of your model's performance.</p> <ul> <li>Ensure models meet required quality thresholds</li> <li>Used to compute and visualize evaluation metrics</li> <li>Inspect model's performance against different slices of data</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#architecture","title":"Architecture","text":"<ul> <li>The first stage Read inputs is made up of transform that takes raw input and converts into a dictionary format , <code>tfma.Extracts</code> to be understandable by next stage 'Extraction'. Across all stages the output is kept in this dictionary format.</li> <li>The next stage Extraction performs distributed processing using Apache Beam. Input extractor and slice key extractor forms slices of the original dataset, which will then be utilized by predict extractor to run predictions on each slice.</li> <li>The next stage Evaluation also performs distributed processing using Apache Beam. We can even create custom evaluator.</li> <li>The final stage Write Results writes the result to disk.</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#one-model-vs-multiple-models-over-time","title":"One model vs multiple models over time","text":"<p>TensorBoard visualizes streaming metrics of multiple models over global training set.</p> <p>TFMA visualizes the metrics computes across different saved model versions</p> <p></p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#aggregate-vs-sliced-metrics","title":"Aggregate vs sliced metrics","text":"<p>Determining which slices are important to analyze requires domain knowledge about the data and application.</p> <p></p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#streaming-vs-full-pass-metrics","title":"Streaming vs full-pass metrics","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#tfma-in-practice","title":"TFMA in Practice","text":"<ul> <li>Analyze impact of different slices of data over various metrics</li> <li>How to track metrics over time?</li> <li>When you use TFMA in a TFX pipeline, the evaluator component already contains many of the steps described here.</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#step-1-export-evalsavedmodel-for-tfma","title":"Step 1: Export EvalSavedModel for TFMA","text":"<pre><code>import tensorflow as tf\nimport tensorflow_transform as tft\nimport tensorflow_model_analysis as tfma\n\ndef get_serve_tf_examples_fn(model, tf_transform_output):\n    # Return a function that parses a serialized tf.Example and applies TFT\n    tf_transform_output = tft.TFTransformOutput(transform_output_dir)\n\nsignatures = {\n    'serving_default': get_serve_tf_examples_fn(model, tf_transform_output)\n                                                                                            .get_concreate_function(tf.TensroSpec(...)),\n}\nmodel.save(serving_model_dir_path, sava_format='tf', signatures=signatures)\n</code></pre>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#step-2-create-evalconfig","title":"Step 2: Create EvalConfig","text":"<p>Create eval_config object that encapsulates the requirements for TFMA.</p> <pre><code># Specify slicing spec\nslice_spec = [slicer.SingleSliceSpec(columns=['column_name']), ...]\n\n# Define metrics\nmetrics = [tf.keras.metrics.Accuracy(name='accuracy'),\n                     tfma.metrics.MeanPrediction(name='mean_prediction'),\n           ...]\nmetrics_specs = tfma.metrics.specs_from_metrics(metrics)\n\neval_config = tfma.EvalConfig(\n                                     model_specs=[tfma.ModelSpec(label_key=features.LABEL_KEY)],\n                                     slicing_specs=slice_spec,\n                                     metrics_specs=metrics_specs,\n                   ...)\n</code></pre>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#step-3-analyze-model","title":"Step 3: Analyze model","text":"<pre><code># Specify the path to the eval graph and to where the result should be written\neval_model_dir = ...\nresult_path = ...\n\neval_shared_model = tfma.default_eval_shared_model(\n                                                        eval_saved_model_path=eval_model_dir,\n                                                        eval_config=eval_config)\n\n# RUN TensorFlow Model Analysis\neval_result = tfma.run_model_analysis(eval_shared_model=eval_shared_model,\n</code></pre>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#step-4-visualizing-metrics","title":"Step 4: Visualizing metrics","text":"<pre><code># rendder results\ntfma.viewer.render_slicing_metrics(result)\n</code></pre> TensorFlow Model Analysis"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#tensorflow-model-analysis","title":"TensorFlow Model Analysis","text":"<p>If you wish to dive more deeply into TensorFlow Model Analysis (TFMA) capabilities, feel free to check out these resources. You won\u2019t have to read these to complete this week\u2019s practice quizzes.</p> <ul> <li>TFMA</li> <li>TFMA architecture</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#model-debugging-overview","title":"Model Debugging Overview","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#model-robustness","title":"Model robustness","text":"<ul> <li>Robustness is much more than generalization</li> <li>Is the model accurate even for slightly corrupted input data?</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#robustness-metrics","title":"Robustness metrics","text":"<ul> <li>Robustness measurement shouldn't take place during training and not using training data</li> <li>Split data into train/val/test sets</li> <li>Specific metrics for regression and classification problems</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#model-debugging","title":"Model Debugging","text":"<ul> <li>Deals with detecting and dealing with problems in ML systems</li> <li>Applies mainstream software engineering practices to ML models</li> </ul> <p>Objectives:</p> <ul> <li> <p>Opaqueness</p> <p>Model debugging tries to improve the transparency of models by highlighting how data is flowing inside.</p> </li> <li> <p>Social discrimination</p> <p>Model shouldn't work poorly for certain groups of people.</p> </li> <li> <p>Security vulnerabilities</p> <p>Model debugging also aims to reduce the vulnerability of your model to attacks.</p> </li> <li> <p>Privacy harms</p> <p>The data should be anonamized before training</p> </li> <li> <p>Model decay</p> <p>Model performance decays with time as the distribution of incoming data changes.</p> </li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#model-debugging-techniques","title":"Model Debugging Techniques","text":"<ol> <li>Benchmark models</li> <li>Sensitivity analysis</li> <li>Residual analysis</li> </ol>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#benchmark-models","title":"Benchmark Models","text":"<p>Benchmarking model are simple but consistence performance giving models that are used before you start development for baselining your problem.</p> <p>Compare your model against these models to see if its performing better than the simpler benchmarking model as a sanity test.</p> <p>Once the model passes the benchmark test, the benchmark model become a solid debugging tool and a starting point of ML development.</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#sensitivity-analysis-and-adversarial-attacks","title":"Sensitivity Analysis and Adversarial Attacks","text":"<p>Sensitivity analysis helps us understand the model by examining the impact each feature has on the model's prediction.</p> <p>In sensitivity analysis we experiment by changing a single feature's value while holding the other features constant and observe the model results.</p> <p>If changing the features values causes the models result to be drastically different. It means that that feature has a big impact on the prediction.</p> <ul> <li>See how model reacts to data which has never been used before</li> <li>What-If tool developed by TensorFlow team allows to perform sensitivity analysis to understand and debug you model performance.</li> </ul> <p>Common approaches for doing sensitivity analysis:</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#1-random-attacks","title":"1. Random Attacks","text":"<ul> <li>Expose models to high volumes of random input data.</li> <li>Exploits the unexpected software and math bugs</li> <li>Great way to start debugging</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#2-partial-dependence-plots","title":"2. Partial dependence plots","text":"<ul> <li>Partial dependence plots show the marginal effect of one or two features and the effect they have on the model results.</li> <li>These plots can describe the relationship between outcome and a particular feature is linear, monotonic or more complex.</li> <li>PDPbox and PyCEbox are open source packages for creating such plots.</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#how-vulnerable-to-attacks-is-your-model","title":"How vulnerable to attacks is your model?","text":"<p>Sensitivity can mean vulnerability</p> <ul> <li>Attacks are aimed at fooling your model by creating examples which are formed by making small but carefully designed changes to the data.</li> <li>Successful attacks could be catastrophic</li> <li>Test adversarial examples</li> <li>Harden your model</li> </ul> <p></p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#informational-and-behavioral-harms","title":"Informational and Behavioral Harms","text":"<ul> <li>Informational Harm: unanticipated leakage of information</li> <li>Behavioral Harm: manipulating the behavior of the model</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#informational-harms","title":"Informational Harms","text":"<ul> <li> <p>Membership Inference attacks: was this person's data used for training?</p> <p>These attacks are aimed at inferring whether or not an individual's data was used to train the model based on a sample of the model's output</p> </li> <li> <p>Model Inversion attacks: recreate the training data</p> </li> <li>Model Extraction: recreate the model</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#behavioral-harms","title":"Behavioral Harms","text":"<ul> <li>Poisoning attacks: insert malicious data into training data in order to change the behavior of the model.</li> <li>Evasion attacks: input data that causes the mode to intentionally misclassify that data</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#measuring-your-vulnerability-to-attack","title":"Measuring your vulnerability to attack","text":"<p>Cleverhans: an open-source Python library to benchmark machine learning systems's vulnerability to adversarial examples.</p> <ul> <li>To harden your model to adversarial attacks, one approach is to include adversarial examples into the training set, this is referred to as adversarial training.</li> </ul> <p>Foolbox: an open-source Python library that lets you easily run adversarial attacks against machine learning models.</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#adversarial-example-searches","title":"Adversarial example searches","text":"<p>Attempted defenses against adversarial examples:</p> <ul> <li>Defensive distillation: the main difference from the original distillation is that we use the same architecture to train both the original network as well as the distilled network.</li> </ul> <p></p> Sensitivity Analysis and Residual attacks <p>If you wish to dive more deeply into adversarial attacks, feel free to check out this paper:</p> <p>Explaining and Harnessing Adversarial Examples</p> <p>To access the Partial Dependence Plots libraries please check these resources:</p> <ul> <li>PDPbox</li> <li>PyCEbox</li> </ul> <p>The demo shown in the previous video is based on this Google Colab notebook.</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#residual-analysis","title":"Residual Analysis","text":"<ul> <li>Measures the differences between model's performance and ground truth</li> <li>Randomly distributed errors are good</li> <li>Correlated or systematic errors show that a model can be improved</li> </ul> <ul> <li>Residuals should not be correlated with another feature</li> <li> <p>Adjacent residuals should not be correlated with each other (autocorrelation)</p> <p>Performing a Durbin Watson test is also useful for detecting autocorrelation.</p> </li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#model-remediation-how-to-improve-robustness","title":"Model Remediation \u2014 How to improve robustness?","text":"<p>Data augmentation:</p> <ul> <li>Adding synthetic data into training set</li> <li>Helps correct for unbalanced training data</li> </ul> <p>Interpretable and explainable ML:</p> <ul> <li>Understand how data is getting transformed</li> <li>Overcome myth of neural networks as black box</li> </ul> <p>Model editing:</p> <ul> <li>Applies to decision trees</li> <li>Manual tweaks to adapt your use case</li> </ul> <p>Model assertions:</p> <ul> <li>Implement business rules that override model predictions</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#reducing-or-eliminating-model-biaas","title":"Reducing or eliminating Model Biaas","text":"<p>Discrimination remediation</p> <ul> <li>Include people with varied backgrounds for collecting training data</li> <li>Conduct feature selection on training data</li> <li>Use fairness metrics to select hyperparameters and decision cut-off thresholds</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#remediation-in-production","title":"Remediation in production","text":"<p>Model monitoring</p> <ul> <li>Conduct model debugging at regular intervals</li> <li>Inspect accuracy, fairness, security problems, etc.</li> </ul> <p>Anomaly detection</p> <ul> <li>Anomalies can be a warning of an attack</li> <li>Enforce data integrity constraints on incoming data</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#fairness","title":"Fairness","text":"<p>Make sure model does not cause harm to specific group of people</p> <ul> <li>Fairness indicators is an open source library to compute fairness metrics</li> <li>Easily scales across dataset of any size</li> <li>Built on top of TFMA</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#what-does-fairness-indicators-do","title":"What does fairness indicators do?","text":"<ul> <li>Compute commonly-identified fairness metrics for classification models</li> <li>Compare model performance across subgroups to other models</li> <li>No remediation tools provided</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#evaluate-at-individual-slices","title":"Evaluate at individual slices","text":"<ul> <li>Overall metrics can hide poor performance for certain parts of data</li> <li>Some metrics may fare well over others</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#aspects-to-consider","title":"Aspects to consider","text":"<ul> <li>Establish context and different user types</li> <li>Seek domain experts help</li> <li>Use data slicing widely and wisely</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#general-guidelines","title":"General guidelines","text":"<ul> <li>Compute performance metrics at all slices of data</li> <li>Evaluate your metrics across multiple thresholds</li> <li>If decision margin is small, report in more detail</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#measuring-fairness","title":"Measuring Fairness","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#positive-rate-negative-rate","title":"Positive rate / Negative rate","text":"<ul> <li>Percentage data points classified as positive/negative</li> <li>Independent of ground truth</li> <li>Use case: having equal final percentages of groups is important</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#true-positive-rate-tpr-false-negative-rate-fnr","title":"True positive rate (TPR) / False negative rate (FNR)","text":"<p>TPR: percentage of positive data points that are correctly labeled positive</p> <p>FNR: percentage of positive data points that are incorrectly labeled negative</p> <ul> <li>Measures equality of opportunity, when the positive class should be equal across subgroups</li> <li>Use case: where it is important that same percent of qualified candidates are rated positive in each group"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#false-positive-rate-fpr-true-negative-rate-tnr","title":"False Positive rate (FPR) / True negative rate (TNR)","text":"<p>FPR: percentage of negative data points that are incorrectly labeled negative.</p> <p>TNR: percentage of negative data points that are correctly labeled negative</p> <ul> <li>Measures equality of opportunity, when the negative class should be equal across subgroups</li> <li>Use case: where misclassifying something as positive are more concerning than classifying the positives.</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#accuracy-area-under-the-curve-auc","title":"Accuracy &amp; Area under the curve (AUC)","text":"<p>Accuracy: percentage of data points that are correctly labeled</p> <p>AUC: percentage of data points that are correctly labeled when each class is given equal weight independent of number of samples</p> <ul> <li>Metrics related to predictive parity</li> <li>Use case: when precision is critical</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#some-tips","title":"Some Tips","text":"<ul> <li>Unfair skews if there is a gap in a metric between two groups</li> <li>Good fairness indicators doesn't always mean the model is fair</li> <li>Continuous evaluation throughout development and deployment</li> <li>Conduct adversarial testing for rare, malicious examples</li> <li>Model Remediation and Fairness</li> </ul> Model Remediation and Fairness"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#model-remediation-and-fairness","title":"Model Remediation and Fairness","text":"<p>If you wish to dive more deeply into model remediation and fairness, feel free to check out these optional resources and tools. You won\u2019t have to read these to complete this week\u2019s practice quizzes.</p> <ul> <li>Fairness</li> <li>Learning fair representations</li> <li>Fairness-aware Machine Learning library</li> <li>AI 360 open source model fairness library</li> <li>Model remediation</li> <li>Model cards</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#continuous-evaluation-and-monitoring","title":"Continuous Evaluation and Monitoring","text":""},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#why-do-models-need-to-be-monitored","title":"Why do models need to be monitored?","text":"<ul> <li>Training data is a snapshot of the world at a point in time</li> <li>many types of data change over times, some quickly</li> <li>ML Models do not get better with age</li> <li>As model performance degrades, you want an early warning</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#data-drift-and-shift","title":"Data drift and shift","text":"<p>Concept drift: change in the relationship between inputs and labels.</p> <p>Concept Emergence: new type of data distribution</p> <p>Types of dataset shift:</p> <ul> <li>Covariate shift: The distribution of your input data changes but the conditional probability of your output over the input remains the same. Distribution of label doesn't change.</li> <li> <p>Prior probability shift: The distribution of your labels changes but the input data remains same.</p> <p>Concept drift can be taught of as a type of prior probability shift.</p> </li> </ul> <p>How do you find the shift and drift. There are both supervised and unsupervised techniques. Let's discuss them:</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#statistical-process-control-supervised","title":"Statistical process control \u2014 supervised","text":"<p>Method used: drift detection method</p> <ul> <li>It assumes that the stream of data will be stationary and then models the number of errors as binomial random variable</li> </ul> <p>$$ \\mu = np_t \\qquad \\sigma = \\sqrt{\\frac{p_t(1-p_t)}{n}} $$</p> <ul> <li> <p>Alert rule</p> <p>$$ p_t + \\sigma_t \\geq p_{min} + 3\\sigma_{min} \\implies \\text{ALERT!} $$</p> <p>Analyses the rate of errors and since it's a supervised method it requires us to have labels for incoming stream of data. It triggers a drift alert if the parameters of the distributions go beyond a certain thresholds.</p> </li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#sequential-analysis-supervised","title":"Sequential analysis \u2014 supervised","text":"<p>Method used: Linear four rates</p> <ul> <li>The idea is if data is stationary, continency table should remain constant</li> <li>The contingency table here corresponds to the truth table for a classifier</li> </ul> <p>We calculate four rates:</p>  $$ P_{npv} = \\frac{TN}{TN+FN} \\quad P_{precision} =\\frac{TP}{TP+ FP} \\\\[10pt] P_{recall / sensitivity / TPR} = \\frac{TP}{TP+FN}\\quad P_{specificity/ FPR} = \\frac{TN}{TN+FP} \\\\[10pt] P_*^t \\leftarrow \\eta_*P_*^{t-1} + (1-\\eta_*) I_{y_t=\\hat{y}_t} $$  <p>If model is predicting correctly, these four values remain constant.</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#error-distribution-monitoring-unsupervised","title":"Error distribution monitoring \u2014 unsupervised","text":"<p>Method used: Adaptive Windowing (ADWIN)</p> <ul> <li>In this method you divide the incoming data into windows. Calculate mean error rate at every window of data.</li> <li>Size of window adapts, becoming shorter when data is not stationary</li> </ul> <p>Calculate absolute of mean error rate at every successive window and compare it with threshold based on the Hoeffding bound.</p> <p>$$ |\\mu_0 -\\mu_1| &gt; \\Theta_\\text{Hoeffding} $$</p> <p>The Hoeffding bound is used for testing the difference between the means of two populations.</p>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#clusteringnovelty-detection-unsupervised","title":"Clustering/novelty detection \u2014 unsupervised","text":"<p>Assign data to known cluster or detect emerging concept</p> <ul> <li>Multiple algorithms available: OLINDDA, MINAS, ECSMiner, and GC3, etc.</li> <li>Susceptible to curse of dimensionality</li> <li>Can only detect cluster based drift not detect population based changes</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#feature-distribution-monitoring-unsupervised","title":"Feature distribution monitoring \u2014 unsupervised","text":"<p>Monitors individual feature separately at every window of data, comparing individual features against each window of data.</p> <p>Algorithms to compare:</p> <ul> <li>Pearson correlation in Change of Concept</li> <li>Hellinger Distance in HDDDM (Hellinger Distance Drift Detection Method)</li> </ul> <p>Use PCA to reduce number of features</p> <ul> <li>Cannot detect population drift since it looks only at individual features</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#model-dependent-monitoring-unsupervised","title":"Model-dependent monitoring \u2014 unsupervised","text":"<p>This method monitors the space near the decision boundaries or margins in the latent feature space of your model.</p> <ul> <li>one algorithm is Margin Density Drift Detection (MD3)</li> <li>Area in latent space where classifier have low confidence matter more<ul> <li>A change in the number of samples in the margin, margin density indicates drift</li> </ul> </li> <li>Reduce false alarm rate</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#google-cloud-ai-continuous-evaluation","title":"Google Cloud AI Continuous Evaluation","text":"<ul> <li>Leverages AI Platform Prediction and Data Labeling services</li> <li>Deploy your model to AI Platform Prediction with model version</li> <li>Create evaluation job</li> <li>Input and output are saved in BigQuery table</li> <li>Run evaluation job on a few of these samples</li> <li>View the evaluation metrics on Google Cloud console</li> </ul>"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#how-often-should-you-retrain","title":"How often should you retrain?","text":"<ul> <li>Depends on the rate of change</li> <li>If possible, automate the management of detecting model drift and triggering model retraining.</li> </ul> Continuous Evaluation and Monitoring"},{"location":"ml/mle-for-production/ml-modeling-pipeline/week4/#continuous-evaluation-and-monitoring_1","title":"Continuous Evaluation and Monitoring","text":"<p>If you wish to dive more deeply into continuous evaluation and model monitoring, feel free to check out these optional resources and tools. You won\u2019t have to read these to complete this week\u2019s practice quizzes.</p> <ul> <li>Instrumentation, Observability &amp; Monitoring of Machine Learning Models</li> <li>Monitoring Machine Learning Models in Production - A Comprehensive Guide</li> <li>Concept Drift detection for Unsupervised Learning</li> <li>Google Cloud</li> <li>Amazon SageMaker</li> <li>Microsoft Azure</li> </ul>"}]}